<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">RESTEasy 3.15.2.Final is now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/D3x9fgUiqVY/" /><author><name /></author><id>https://resteasy.github.io/2021/09/23/resteasy-3.15.2.Final/</id><updated>2021-09-23T18:11:11Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/D3x9fgUiqVY" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/09/23/resteasy-3.15.2.Final/</feedburner:origLink></entry><entry><title type="html">Introducing Kogito API Incubation</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/U1sAAZ0dDJI/kogito-api-incubation.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2021/09/kogito-api-incubation.html</id><updated>2021-09-23T15:00:00Z</updated><content type="html">Would you like to be able to evaluate a process in Kogito with a simple Java API like this? var id = appRoot.get(ProcessIds.class).get("hello-world"); // wrap the Map in a data context var ctx = MapDataContext.create(); ctx.set("name", "Paul"); // evaluate the process and get the result as a map-like var res = processSvc.evaluate(id, ctx).as(MapDataContext.class); System.out.println(res.get("message")); // "hello, Paul!" What about invoking a specific subcomponent of a DMN, such as a decision service ? MapDataContext ctx = MapDataContext.create(); // construct the path to a DMN decision service var id = appRoot .get(DecisionIds.class) .get("https://github.com/evacchi/my-namespace", "Traffic Violation") .services() .get("my-service-id"); // evaluate the decision and return the result var res = dmnSvc.evaluate(id, ctx); Or maybe you do not want the flexibility of a Map. Maybe you just want to use a simple data object. Is a Java record concise enough for you? public record LinearRegParams( float fld1, float fld2, String fld3) implements DataContext, DefaultCastable {} public record LinearRegResult( float fld4) implements DataContext, DefaultCastable {} // construct the path to a PMML predictive model var id = appRoot .get(PredictionIds.class) .get("LinReg"); // create the context object var ctx = new LinearRegParams(...); // evaluate var res = svc.evaluate(id, ctx); // map the result to the record var rec = pmmlSvc.as(LinearRegResult.class); What if you could just use that record in your REST endpoint? @POST @Consumes(MediaType.APPLICATION_JSON) // take the custom data context as a parameter, // get JSON conversion automatically public Float hello(LinearRegParams payload) { var id = appRoot .get(PredictionIds.class) .get("LinReg"); var res = pmmlSvc.evaluate(id, payload); // extract the float result and return it var rec = res.as(LinearRegResult.class); return rec.fld4(); } Or maybe evaluate a Drools query and get a stream of typed results? public record MyQueryResult( String contents) implements DataContext, DefaultCastable {} @POST @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public Stream&lt;MyQueryResult&gt; hello(MapDataContext ctx) { var queryId = appRoot.get(RuleUnitIds.class) .get(Hello.class) .queries() .get("hello"); // evaluate the query return svc.evaluate(queryId, ctx) map(ctx -&gt; ctx.as(MyQueryResult.class)); // map each result to a typed value } You have learned to love the powerful Kogito codegen capabilities. You just drop your asset to your source directory (or you feed it to the powerful operator) and you get a fully-functional REST endpoint for free. However, from day one we understood that automated code-generation would bring you only to a certain point. That is why we wanted to provide a great programmatic API. Yet, so far we were not confident enough to open up the internal API that we were using as our code generation target. The reason is that we wanted to get it right first. We are now happy to release a first version of our new public API in incubation state starting from version 1.13! WHAT DOES INCUBATION MEAN? Incubation means that this API is a first approximation of what we will deliver in the end. It is made available early in the spirit of release early, release often, with no guarantees that it will not change. However, you are welcome to try it and give us feedback on it, because once released we will not be able to make any more changes to it. WHEN CAN I GET MY HANDS ON THIS? We plan to roll out a first version of this for the 1.13 release. However, until this is incubating then we will not commit to long-term stability. Breakage may occur; you have been warned: use it at your risk! WHEN ARE YOU DELIVERING THE FINAL VERSION OF THIS API ? We plan to roll this out in stages. As you know our platform contains multiple components (DMN, PMML, Processes, Severless Workflows, Drools rules), and it is possible that each one of those will reach maturity at a different moment in time. Moreover, every component has a different set of capabilities. So we may decide to roll out features instead of full components. For instance, we may declare that the API for evaluating a DMN model has reached maturity stage, but keep the listener API still in incubation. SHALL I WAIT FOR IT TO BE FINAL? Well that depends on you, really. Do you like living on the bleeding edge? If you picked Kogito we may assume that you do! Even if we make some changes, we don’t think they will be revolutionary. Indeed, you will certainly have to rename your imports from org.kie.kogito.api.incubation. to org.kie.kogito.api. at some point. Maybe update your dependencies. But the structure will probably be pretty similar to what we will deliver now. So don’t be too scared. OK, SURE BUT THIS LOOKS PRETTY VANILLA. IS THERE SOMETHING ELSE TO IT? Indeed. What if I told you that every identifier is really translated into path? // /predictions/LinReg appRoot.get(PredictionIds.class).get("LinReg"); // /rule-units/org.kie.kogito.examples.Hello/queries/hello appRoot.get(RuleUnitIds.class) .get(Hello.class) .queries() .get("hello"); // /decisions/https%3A%2F%2Fgithub.com%2Fevacchi%2Fmy-namespace%23Traffic%20Violation var id = appRoot .get(DecisionIds.class) .get("https://github.com/evacchi/my-namespace", "Traffic Violation") // /processes/hello-world var id = appRoot.get(ProcessIds.class).get("hello-world"); What if I told you that paths could be mounted onto URIs ? kogito://my-app-1@my.host1.name:54321/predictions/LinReg kogito://my-app-2@my.host2.name:54321/rule-units/org.kie.kogito.examples.Hello/queries/hello kogito://my-app-3@my.host3.name:54321/decisions/https%3A%2F%2Fgithub.com%2Fevacchi%2Fmy-namespace%23Traffic%20Violation kogito://my-app-4@my.host4.name:54321/rules/processes/hello-world What if you could use one unified way to send commands across the network between Kogito applications? What if those same IDs and commands were easily mapped onto HTTP commands? POST https://my.host1.name:54321/my-app-1/predictions/LinReg { "json" : "data" } or would you rather prefer Cloud Events ? POST https://my.host1.name:54321 { "specversion" : "1.0", "type" : "org.kie.kogito.predictions.evaluate", "source" : "/some/sender/id", "org.kie.kogito.target" : "/predictions/LinReg", "id" : "...", "time" : "2021-04-05T17:31:00Z", "datacontenttype" : "text/json", "data" : { "fld1" : 3.0, "fld2" : 2.0, "fld3" : "y" } } WAIT, ARE YOU SAYING THIS IS ALREADY AVAILABLE? THAT’S AWES– No, it’s not, sorry to burst that bubble. We are still working on it. But really, that is where we would like to end up. For now, just enjoy this new way to develop your own REST endpoints! But stay tuned: easy serialization and distribution are the main drivers of this redesign. ALRIGHT, ALRIGHT. THAT STILL LOOKS PRETTY COOL. WHERE DO I GET IT? Check out our examples at … or try these simple steps… [ TODO ] OK COOL Wait, wait wouldn’t you like to learn more about how this was designed? NOT REALLY, BUT IF YOU INSIST Well I do. In the next post I’ll explain the design rationale for this API. You may also join us on October 2nd at our KIE Live; yours truly will deliver a presentation and will give a live coding demo. Ain’t that cool? The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/U1sAAZ0dDJI" height="1" width="1" alt=""/&gt;</content><dc:creator>Edoardo Vacchi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/09/kogito-api-incubation.html</feedburner:origLink></entry><entry><title>Leader election in Kubernetes using Apache Camel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ptZfmNY_LdI/leader-election-kubernetes-using-apache-camel" /><author><name>Stephen Nimmo</name></author><id>82cd25ad-54ed-445f-a5b5-85e2b1f9259f</id><updated>2021-09-23T07:00:00Z</updated><published>2021-09-23T07:00:00Z</published><summary type="html">&lt;p&gt;When deploying applications on &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, certain platform characteristics strongly influence the application's architecture. In a greenfield setting, it's all about harnessing the ephemeral nature of stateless applications. Applications are built to run in scenarios where there is an expectation of high availability via horizontal scaling. Not only can the application scale out, but Kubernetes' orchestration characteristics emphasize that no individual pod is safe from destruction. Kubernetes is the epitome of the old U.S. Navy Seal saying: "Two is one, and one is none."&lt;/p&gt; &lt;p&gt;Workloads on Kubernetes don't always fit this model, however. Some workloads are singular in nature, and parallelization isn't an option. For example, suppose an application connects out to an external service and receives information asynchronously via a TCP socket or websocket. As part of this process, the application receives data, transforms the structure, and publishes that data into an &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; topic. In this case, only a single connection can be active at one time because of the possibility of publishing duplicate data (see Figure 1).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="There are two processes, but only one is active at a time." data-entity-type="file" data-entity-uuid="64e0dd2b-3eb5-43b9-92cb-31c66f8ab706" src="https://developers.redhat.com/sites/default/files/inline-images/pic1.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Only one process can be active at a time.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The quick solution to this problem is actually a fundamental characteristic of Kubernetes. If the deployment is created with the &lt;a href="https://docs.openshift.com/container-platform/4.8/applications/deployments/what-deployments-are.html#deployments-kube-deployments_what-deployments-are"&gt;replicas set to 1&lt;/a&gt;, then when the controller detects the pod is no longer running, it will attempt to create a new one. However, real-world situations can be more complicated. Some applications require a long startup time due to cache warming needs. When you combine slow startup times (minutes) for the pod with business requirements to minimize the loss of downtime, the default solution becomes unsuitable. In this situation, we will want to have multiple instances up and ready to take over the consumption as quickly as possible.&lt;/p&gt; &lt;p&gt;This article shows you how to implement leader election in Kubernetes using Apache Camel. To follow along with the examples, see the demo code available on &lt;a href="https://github.com/stephennimmo/quarkus-camel-master-demo"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Hot-warm with leader election&lt;/h2&gt; &lt;p&gt;To run an application as &lt;em&gt;hot-warm&lt;/em&gt; means to have multiple instances of the application running and ready to serve requests, but only one instance actually doing the work. Within Kubernetes, this means having multiple pods ready at all times, but only one pod active for a particular process. In this scenario, the pods negotiate among themselves which one is active.&lt;/p&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt; has a component (called &lt;a href="https://camel.apache.org/components/latest/master-component.html"&gt;master&lt;/a&gt;) that is built exactly for this scenario. As the docs explain, the Camel-Master endpoint lets us ensure only a single consumer in a cluster consumes from a given endpoint, with automatic failover if that Java virtual machine (JVM) dies. To achieve this goal, the endpoint requires a shared resource and locking. The component has multiple implementations for the locking mechanism, including the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;camel-atomix&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-consul&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-file&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-infinispan&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-jgroups-raft&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-jgroups&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-kubernetes&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-zookeeper&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Within the component's configuration, the developer provides a namespace to designate the shared resource. All processes that use the same namespace for the locking will ensure that only one process at a time obtains the lock. When a process has the lock, it is the leader, and the process will run. If it loses the lock for any reason, the component will stop the process, as well.&lt;/p&gt; &lt;h2&gt;Leader election with a Camel route&lt;/h2&gt; &lt;p&gt;The first example uses the traditional Camel route domain-specific language (DSL), where we incorporate the &lt;a href="https://camel.apache.org/components/latest/master-component.html"&gt;master component&lt;/a&gt; along with the &lt;a href="https://camel.apache.org/components/latest/timer-component.html"&gt;timer component&lt;/a&gt;. For this example, we are using &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; and &lt;a href="https://quarkus.io/guides/camel"&gt;Quarkus Camel extensions&lt;/a&gt; to implement the functionality:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java" lang="java" xml:lang="java"&gt;import org.apache.camel.builder.RouteBuilder; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class TimerLoggerClusteredRoute extends RouteBuilder { @Override public void configure() throws Exception { from("master:timer-logger-ns:timer://current-leader-check-timer?fixedRate=true&amp;period=500") .log("Current Leader"); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing you should notice is that the master DSL is prepended to the timer DSL. The DSL designates a namespace of &lt;code&gt;timer-logger-ns&lt;/code&gt;, so all instances of this application will check the locks at regular intervals to see if it's available. However, all the implementation details are obscured from the application as it relates to how the lock is created or managed. It's simply a logical construct around locking a namespace.&lt;/p&gt; &lt;h2&gt;Testing the example locally&lt;/h2&gt; &lt;p&gt;Although we will ultimately deploy this application out to a Kubernetes cluster, we want to demonstrate and test this functionality in our local environment. When doing so, we won't have access to the Kubernetes leases, so we will implement the locking using the &lt;code&gt;camel-file&lt;/code&gt; component. Because of the environmental differences, we will leverage Quarkus &lt;a href="https://quarkus.io/guides/config-reference#profiles"&gt;profiles&lt;/a&gt; to produce the correct &lt;code&gt;CamelClusterService&lt;/code&gt; implementation for our environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java" lang="java" xml:lang="java"&gt;@ApplicationScoped public class ClusterLockProducer { @ConfigProperty(name = "namespace") Optional&lt;String&gt; namespace; @Produces @UnlessBuildProfile("prod") public CamelClusterService fileLockClusterService(CamelContext camelContext) throws Exception { FileLockClusterService service = new FileLockClusterService(); service.setRoot("cluster"); service.setAcquireLockDelay(1, TimeUnit.SECONDS); service.setAcquireLockInterval(1, TimeUnit.SECONDS); return service; } @Produces @IfBuildProfile("prod") public CamelClusterService kubernetesClusterService(CamelContext camelContext) { KubernetesClusterService service = new KubernetesClusterService(); if (namespace.isPresent()){ service.setKubernetesNamespace(namespace.get()); } return service; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For the local environment, we set up the &lt;code&gt;FileLockClusterService&lt;/code&gt;. The &lt;code&gt;setRoot&lt;/code&gt; allows us to designate the location of the files used for the lock, relative or absolute. After the process starts, the files are created and used as locks to designate the current leader, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="When the process is started with the current configuration in our local development environment, it places the locks into a “cluster” folder setup in the application's working directory." data-entity-type="file" data-entity-uuid="56fb86dd-adde-40d4-a8b9-8a3789c486f6" src="https://developers.redhat.com/sites/default/files/inline-images/pic2.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: The target folder that contains the files for locking.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If a new instance of the application is started locally, then the newly started application will not be able to obtain the locks and therefore will not run the timer component. If you kill the leader, the other application will check the lock, see that it's not locked, and subsequently obtain the lock and start processing. Additionally, we can apply settings to designate how often we want the service to check the locks and acquire the lock.&lt;/p&gt; &lt;h2&gt;Leader election without Camel routes&lt;/h2&gt; &lt;p&gt;The Camel route example in the previous section shows how easy it is to add the additional DSL to create and manage singleton processes. However, there are times when the processes to be managed do not fit well into a Camel route, or the processes might already be implemented and we don't necessarily want to have to rewrite the code. Luckily the mechanisms supplied by the master component are available outside of the DSL.&lt;/p&gt; &lt;p&gt;Let's say I have a regular service that can be started and stopped. In this example, the application-scoped bean will instantiate a &lt;a href="https://www.baeldung.com/java-executor-service-tutorial"&gt;single-threaded scheduled executor&lt;/a&gt; that logs messages at a certain fixed rate.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java" lang="java" xml:lang="java"&gt;@ApplicationScoped public class BeanLoggerService { private static final Logger LOGGER = LoggerFactory.getLogger(BeanLoggerService.class); private ScheduledExecutorService scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(); private ScheduledFuture scheduledFuture; public void start() { if (scheduledFuture == null) { scheduledFuture = scheduledExecutorService.scheduleAtFixedRate(() -&gt; { LOGGER.info("{}", System.currentTimeMillis()); }, 1, 1, TimeUnit.SECONDS); } } public void stop() { if (scheduledFuture != null) { this.scheduledFuture.cancel(true); this.scheduledFuture = null; } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want to ensure that this service is always running as a singleton among the JVMs. I can accomplish this by obtaining the &lt;code&gt;CamelClusterService&lt;/code&gt; directly and adding an event listener to start and stop the service when leadership changes are detected:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java" lang="java" xml:lang="java"&gt;@Startup public class BeanLoggerLeadershipListener { private static final Logger LOGGER = LoggerFactory.getLogger(BeanLoggerLeadershipListener.class); private CamelContext camelContext; private BeanLoggerService beanLoggerService; public BeanLoggerLeadershipListener(CamelContext camelContext, BeanLoggerService beanLoggerService) { this.camelContext = camelContext; this.beanLoggerService = beanLoggerService; } void onStart(@Observes StartupEvent ev) throws Exception { CamelClusterService camelClusterService = ClusterServiceHelper.lookupService(camelContext).orElseThrow(() -&gt; new RuntimeException("Unable to lookupService for CamelClusterService")); if (camelClusterService instanceof KubernetesClusterService) { KubernetesClusterService kcs = (KubernetesClusterService) camelClusterService; LOGGER.info("KubernetesClusterService: LeaseResourceType={}, KubernetesNamespace={}, KubernetesResourceName={}, MasterUrl={}", kcs.getLeaseResourceType().name(), kcs.getKubernetesNamespace(), kcs.getKubernetesResourceName(), kcs.getMasterUrl()); } camelClusterService.getView("bean-logger-ns").addEventListener((CamelClusterEventListener.Leadership) (view, leader) -&gt; { LOGGER.info("LeadershipEvent[bean-logger-ns]: {}", leader); boolean weAreLeader = leader.isPresent() &amp;&amp; leader.get().isLocal(); if (weAreLeader) { beanLoggerService.start(); } else { beanLoggerService.stop(); } }); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this example, we are using Quarkus's lifecycle hooks to run code to register the leadership event listener needed to start and stop the &lt;code&gt;BeanLoggerService&lt;/code&gt;. Note that the additional logging code in there will be used to better demonstrate the scenario running in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Running Camel-Master on OpenShift&lt;/h2&gt; &lt;p&gt;In a local environment, we used the &lt;code&gt;FileLockClusterService&lt;/code&gt;. Now that we are ready to deploy this application on OpenShift, we will switch the implementation from using files to using &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/"&gt;Kubernetes leases&lt;/a&gt;. To start, let’s take a look at the deployment manifest for the application.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: I used Kustomize to manage all the manifests, so you will notice the absence of the namespace. That's managed in my main &lt;code&gt;kustomization.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code lang="yaml" xml:lang="yaml"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: quarkus-camel-master-demo-deployment labels: app: quarkus-camel-master-demo spec: replicas: 2 selector: matchLabels: app: quarkus-camel-master-demo template: metadata: labels: app: quarkus-camel-master-demo spec: serviceAccountName: "camel-leader-election" containers: - name: quarkus-camel-master-demo-container image: quay.io/stephennimmo/quarkus-camel-master-demo:0.0.1-SNAPSHOT imagePullPolicy: Always env: - name: QUARKUS_LOG_LEVEL value: "DEBUG" - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - containerPort: 8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this deployment, we have two replicas, but we want only one pod to run the processes. The configuration contains two items of note:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Notice the &lt;code&gt;serviceAccountName&lt;/code&gt; in the deployment config. For this deployment, we need to set up a service account that specifically has access to the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/"&gt;Kubernetes leases&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;We are passing in the namespace as a configuration parameter to be used to set up the &lt;code&gt;KubernetesClusterService&lt;/code&gt; to point the same namespace as our application. This will tell the application to attempt to acquire the lease objects in the same namespace as our application.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Before we deploy, we need to set up the service account and give it permissions to read and write to the lease objects:&lt;/p&gt; &lt;pre&gt; &lt;code lang="yaml" xml:lang="yaml"&gt;apiVersion: v1 kind: ServiceAccount metadata: name: camel-leader-election --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: camel-leader-election rules: - apiGroups: -"" - "coordination.k8s.io" resources: - configmaps - secrets - pods - leases verbs: - create - delete - deletecollection - get - list - patch - update - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: camel-leader-election subjects: - kind: ServiceAccount name: camel-leader-election roleRef: kind: Role name: camel-leader-election apiGroup: rbac.authorization.k8s.io&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Leadership elections in OpenShift&lt;/h2&gt; &lt;p&gt;Once we deploy the application into OpenShift, the application will use the &lt;code&gt;KubernetesClusterService&lt;/code&gt; implementation of the &lt;code&gt;CamelClusterService&lt;/code&gt; to perform the leadership elections. To do this, the service will periodically query the lease information and attempt to update the information if the last update has not been performed in the designated lease time. The configuration for the timing of the leader election activity is more detailed, which should be expected; we are no longer simply checking a file lock, but rather working in more of a heartbeat monitoring pattern. Let's take a look at the running pods, the leases, and the associated YAML file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" lang="bash" xml:lang="bash"&gt;% oc get pods NAME READY STATUS RESTARTS AGE quarkus-camel-master-demo-deployment-894569d67-fcjhc 1/1 Running 0 4d1h quarkus-camel-master-demo-deployment-894569d67-lt5jv 1/1 Running 0 4d1h % oc get leases NAME HOLDER AGE leaders-bean-logger-ns quarkus-camel-master-demo-deployment-894569d67-lt5jv 15d leaders-timer-logger-ns quarkus-camel-master-demo-deployment-894569d67-fcjhc 18d&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash" lang="bash" xml:lang="bash"&gt;% oc get lease leaders-timer-logger-ns -o yaml apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: "2021-08-08T15:07:29Z" labels: provider: camel managedFields: - apiVersion: coordination.k8s.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:renewTime: {} manager: okhttp operation: Update time: "2021-08-26T15:52:43Z" name: leaders-timer-logger-ns namespace: quarkus-camel-master-demo resourceVersion: "26043451" selfLink: /apis/coordination.k8s.io/v1/namespaces/quarkus-camel-master-demo/leases/leaders-timer-logger-ns uid: 4a79df33-d9d7-4ae1-a16b-d964254f46c6 spec: acquireTime: "2021-08-22T14:23:28.586420Z" holderIdentity: quarkus-camel-master-demo-deployment-894569d67-fcjhc leaseDurationSeconds: 15 leaseTransitions: 20 renewTime: "2021-08-26T15:52:43.467366Z" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;holderIdentity&lt;/code&gt; shows that the pod named &lt;code&gt;quarkus-camel-master-demo-deployment-894569d67-fcjhc&lt;/code&gt; is currently the leader and is running the timer-logger process. If we look at the logs (Figure 3), we will see that this is the case. We turned on the debugging for the leadership detection so we can actually see the interactions with the leases being updated.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Screenshot of the OpenShift logging view of the pod with the leader election." data-entity-type="file" data-entity-uuid="3199a652-8f5f-4371-9140-e248a87780f8" src="https://developers.redhat.com/sites/default/files/inline-images/pic3.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: The OpenShift logging view of the pod with the leader election.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;One of the more interesting pieces to note: This pod is currently only the leader as it relates to the &lt;code&gt;timer-logger-ns&lt;/code&gt;. Remember, we actually have an additional master namespace for the &lt;code&gt;bean-logger-ns&lt;/code&gt;. Let’s take a look at that lease:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" lang="bash" xml:lang="bash"&gt;% oc get lease leaders-bean-logger-ns -o yaml apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: "2021-08-10T15:47:54Z" labels: provider: camel managedFields: - apiVersion: coordination.k8s.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:renewTime: {} manager: okhttp operation: Update time: "2021-08-26T16:01:12Z" name: leaders-bean-logger-ns namespace: quarkus-camel-master-demo resourceVersion: "26053368" selfLink: /apis/coordination.k8s.io/v1/namespaces/quarkus-camel-master-demo/leases/leaders-bean-logger-ns uid: faae3687-ea77-45f5-9bad-910c11a21c2b spec: acquireTime: "2021-08-22T14:23:28.424619Z" holderIdentity: quarkus-camel-master-demo-deployment-894569d67-lt5jv leaseDurationSeconds: 15 leaseTransitions: 4 renewTime: "2021-08-26T16:01:12.382697Z" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For the bean-logging process, the leader is actually the other pod: &lt;code&gt;quarkus-camel-master-demo-deployment-894569d67-lt5jv&lt;/code&gt;. The leadership election for each master namespace is completely independent. This behavior might or might not be suitable for the application's needs, so keep that in mind if you are building multiple dependent processes that need to always run in a single pod.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;To learn more about how Red Hat can support your integration needs, including support for Apache Camel, check out the &lt;a href="https://developers.redhat.com/products/fuse/overview"&gt;Red Hat Fuse&lt;/a&gt; website for more information.&lt;/p&gt; &lt;p&gt;The demo code for this article is available on &lt;a href="https://github.com/stephennimmo/quarkus-camel-master-demo"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Follow me on Twitter &lt;a href="https://twitter.com/stephennimmo"&gt;@stephennimmo&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/23/leader-election-kubernetes-using-apache-camel" title="Leader election in Kubernetes using Apache Camel"&gt;Leader election in Kubernetes using Apache Camel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ptZfmNY_LdI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Stephen Nimmo</dc:creator><dc:date>2021-09-23T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/23/leader-election-kubernetes-using-apache-camel</feedburner:origLink></entry><entry><title type="html">Visualize, Edit, and Share your BPMN, DMN and PMML with github.dev</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9Vq3fcx0JNE/visualize-edit-and-share-your-bpmn-dmn-and-pmml-with-github-dev.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/09/visualize-edit-and-share-your-bpmn-dmn-and-pmml-with-github-dev.html</id><updated>2021-09-23T05:00:00Z</updated><content type="html">Some weeks ago, GitHub released which allows you to open any repository in VS Code directly from your browser just pressing . (dot key) on it. On Kogito Tooling 0.13.0 release, we updated our VS Code BPMN, DMN and PMML extension to also work on this innovative environment. Check it out: Note: There is another option to launch github.dev, just replace “.com” on your github URL to “.dev”, as example: HOW TO START TO USE IT? It’s super simple. As soon as you open your “.dev” environment, click on the Extensions menu and search for the BPMN, DMN and PMML extension on VS Code marketplace: COLLABORATE In my opinion, the real power of the github.dev environment is to quickly visualize and collaborate with a project. For instance, you can quickly visually see the differences between your edited model and the version of the current branch: If you are happy with your changes, you can even send a Pull Request directly from github.dev: NEXT STEPS The github.dev is still fresh and new, but I can already see a lot of value for the BPMN and DMN users. As with any experimental feature, there are some issues that we plan to fix on our Editors in the next releases, including: * Resource Content API Support on github.dev, enabling access in a model of other files; * PR visualization doesn’t load all editors side by side on github.dev Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9Vq3fcx0JNE" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/09/visualize-edit-and-share-your-bpmn-dmn-and-pmml-with-github-dev.html</feedburner:origLink></entry><entry><title type="html">WildFly 25 Beta1 S2I images have been released on quay.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ScBMUVj3qC4/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/09/23/WildFly-s2i-25Beta1-Released/</id><updated>2021-09-23T00:00:00Z</updated><content type="html">WILDFLY 25 BETA1 S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 25 Beta1, have been released on . For complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . IMPORTANT CHANGES TO MENTION IN THIS BETA RELEASE We have been evolving the s2i builder image to reflect part of the main changes that occurred in . In particular the s2i image content is impacted by the removal of legacy security: * Changes in the default server configuration: * Now secured with elytron. * Security configuration based on legacy security-realms has been removed. * security subsystem and extension have been removed. * Impact on SSL configuration based on environment variables: * elytron is now used by default to configure SSL. The env variable CONFIGURE_ELYTRON_SSL=true is no more needed. * Impact on Keycloak integration: * By default when configuring Keycloak OIDC and SAML adapters elytron was already in use. Nothing changes there. * If you were using the env variable SSO_FORCE_LEGACY_SECURITY=true to rely on the legacy security subsystem, the server will fail to start, you will need to remove this env variable and rely on elytron integration. ANTICIPATING A FUTURE SUPPORT FOR OPENID CONNECT In this new release we are deprecating the usage of the keycloak Galleon layer and automatic configuration based on We are planning in a future release to rely on the that is providing a native support for OpenID Connect allowing to interact with Keycloak server but with also other servers compatible with the OIDC protocol. Stay tuned!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ScBMUVj3qC4" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/09/23/WildFly-s2i-25Beta1-Released/</feedburner:origLink></entry><entry><title>Thoth prescriptions for resolving Python dependencies</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Xf62gjoGMro/thoth-prescriptions-resolving-python-dependencies" /><author><name>Fridolin Pokorny</name></author><id>46067c5b-cc11-4655-aa62-baf2d3e13038</id><updated>2021-09-22T07:00:00Z</updated><published>2021-09-22T07:00:00Z</published><summary type="html">&lt;p&gt;Python offers a wealth of programming libraries, which often invoke functions from other libraries in complex hierarchies. While these libraries make it possible to develop powerful applications quickly, the ever-changing library versions often introduce conflicts that cause runtime or build-time issues. &lt;a href="http://thoth-station.ninja/"&gt;Thoth&lt;/a&gt;, an open source project developed within the &lt;a href="https://github.com/aicoe/"&gt;Artificial Intelligence Center of Excellence (AICoE)&lt;/a&gt;, is dedicated to alleviating this problem in &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; programs. This article looks at Thoth &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;prescriptions&lt;/a&gt;, a mechanism that you can use to avoid clashing library versions in your Python applications.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For more about resolving Python project dependencies with Thoth, see our recent introduction to &lt;a href="https://developers.redhat.com/articles/2021/09/17/resolve-python-dependencies-thoth-dependency-monkey"&gt;Thoth Dependency Monkey&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Curated knowledge about Python libraries&lt;/h2&gt; &lt;p&gt;One of Thoth's major offerings is a &lt;a href="https://discuss.python.org/t/thoth-an-enhanced-server-side-resolution-offered-to-the-python-community/"&gt;cloud-based resolver&lt;/a&gt;, which examines the packages and libraries requested by an application and determines the best way to resolve them so that they work together in the target environment. Thoth's &lt;a href="https://github.com/thoth-station/prescriptions"&gt;prescription repository&lt;/a&gt; is comparable to the more familiar security project from the &lt;a href="https://github.com/pypa"&gt;Python Packaging Authority (PyPA)&lt;/a&gt;, a &lt;a href="https://discuss.python.org/t/proposing-a-community-maintained-database-of-pypi-package-vulnerabilities/"&gt;curated database of known vulnerabilities&lt;/a&gt; in the Python ecosystem. The &lt;a href="https://github.com/pypa/advisory-db"&gt;Python Advisory DB&lt;/a&gt; resulting from that project is now available on GitHub. The repository contains YAML files describing known vulnerabilities in machine-readable form.&lt;/p&gt; &lt;p&gt;Whereas the Python Advisory DB focuses only on security flaws, Thoth prescriptions are more generic and are directly consumed by the resolver. The database curates a broad range of knowledge about Python libraries and packages: Their communities, known build-time issues, runtime issues, compatibility with native dependencies, suggestions for which runtime to use, or other suggestions of interest to Python package consumers.&lt;/p&gt; &lt;p&gt;Prescription information is stored as YAML files and used automatically by the &lt;a href="https://github.com/thoth-station/adviser"&gt;Thoth resolver&lt;/a&gt; to guide Python application developers. Anyone can contribute to this database. The Thoth resolver automatically loads prescriptions and consults them during the resolution process, so that resolved dependencies are in a healthy state. As a result, developers can focus on application development rather than on fixing library issues. Unlike other resolvers, such as &lt;a href="https://pip.pypa.io"&gt;pip&lt;/a&gt;, &lt;a href="https://pypi.org/project/pipenv"&gt;Pipenv&lt;/a&gt;, or &lt;a href="https://pypi.org/project/poetry"&gt;Poetry&lt;/a&gt;, which tend to resolve the latest libraries, Thoth’s resolver &lt;a href="https://thoth-station.ninja/docs/developers/adviser/introduction.html"&gt;chooses the libraries that best fit the application's needs&lt;/a&gt;, and takes the prescriptions into consideration.&lt;/p&gt; &lt;h2&gt;Examples of Thoth prescriptions at work&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow"&gt;In a previous article&lt;/a&gt;, the Project Thoth team showed an issue in the TensorFlow software stack that occurred when &lt;a href="https://thoth-station.ninja/j/tf_21_urllib3.html"&gt;urrlib3 was installed with package six&lt;/a&gt;. The problem is recorded in a &lt;a href="https://github.com/thoth-station/prescriptions/blob/d216a7e8d093ef12e716b0205eb4dcb526aa0269/prescriptions/te_/tensorflow/tf_21_urllib.yaml"&gt;prescription&lt;/a&gt; that helps the Thoth resolver avoid trying to combine problematic versions of these packages. Applications using the Thoth resolver do not suffer from this recognized runtime problem.&lt;/p&gt; &lt;p&gt;Another example is a &lt;a href="https://github.com/python-pillow/Pillow/issues/5571"&gt;Pillow issue that will not work with NumPy&lt;/a&gt;. A &lt;a href="https://github.com/thoth-station/prescriptions/blob/f500613f72b197e20743a82273f4f34752daeac1/prescriptions/pi_/pillow/pillow830_typeerror.yaml"&gt;prescription&lt;/a&gt; is provided to protect Python application stacks from this issue.&lt;/p&gt; &lt;p&gt;Yet another example warns users about the archived &lt;a href="https://github.com/ThomasWaldmann/argparse"&gt;argparse project&lt;/a&gt;. Using archived projects on GitHub that suffer from the problem might drive users away, so &lt;a href="https://github.com/thoth-station/prescriptions/blob/f500613f72b197e20743a82273f4f34752daeac1/prescriptions/ar_/argparse/gh_archived.yaml"&gt;this prescription&lt;/a&gt; notifies users about the project state.&lt;/p&gt; &lt;p&gt;Browse the &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;Thoth prescriptions repository&lt;/a&gt; or &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;documentation&lt;/a&gt; for more examples.&lt;/p&gt; &lt;h2&gt;Watch a video about Thoth prescriptions&lt;/h2&gt; &lt;p&gt;Are you curious about how "greatest" stacks are resolved? This video introduces a community-curated database that Thoth users can benefit from:&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: see &lt;a href="https://www.youtube.com/watch?v=dg6_WhUK5Ew"&gt;Healing Python applications with prescriptions&lt;/a&gt; for a video overview of the prescriptions concept. Visit the &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;Thoth Station&lt;/a&gt; YouTube channel for more videos like these.&lt;/p&gt; &lt;h2&gt;Calling all Python developers and package maintainers!&lt;/h2&gt; &lt;p&gt;If you are a Python developer or Python package maintainer, we encourage you to get involved in building the prescriptions database. You can report issues to be turned into prescriptions, which are used to help create healthy Python applications.&lt;/p&gt; &lt;p&gt;If you would like to report a library issue, reach out to us at the &lt;a href="https://github.com/thoth-station/support"&gt;Thoth Station support repository&lt;/a&gt;. You can also write prescriptions directly by following the &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;online documentation&lt;/a&gt;. If you would like to be notified when a new prescription is created for a library, add yourself to the repository's &lt;a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-code-owners"&gt;CODEOWNERS file&lt;/a&gt; to follow per-project prescription updates.&lt;/p&gt; &lt;h2&gt;Helping the Python community create healthy applications&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow updates in project Thoth, &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;subscribe to our YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation Twitter handle&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies" title="Thoth prescriptions for resolving Python dependencies"&gt;Thoth prescriptions for resolving Python dependencies&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Xf62gjoGMro" height="1" width="1" alt=""/&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2021-09-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies</feedburner:origLink></entry><entry><title type="html">Kogito Process Eventing Add-ons</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1HvZpazSnDc/kogito-process-eventing-add-ons.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2021/09/kogito-process-eventing-add-ons.html</id><updated>2021-09-21T16:47:01Z</updated><content type="html">This post  illustrates the usage of Kogito add-ons which, during process execution, in an asynchronous way, are either publishing information to an external destination or waiting for a particular event to be triggered by an external source. This external actor will be in all cases configurable and typically, although not necessarily, will consist of  an event broker like Kafka, AMQ or similar middle-ware.   An add-on in Kogito is an optional library that users might include in their classpath to enable certain functionality. With a few exceptions, every add-on in Kogito has two versions depending on the target platform: Quarkus and Spring Boot. Although both are always equivalent in terms of functionality, configuration details might vary between them, as we will see later.   The list of add-ons included in Kogito release 1.11, which might be used by a process to interact with an event source or destination are: * Messaging. It allows integration between a and an event source or listener, depending on the Message type. * Process Events. It publishes internal events generated as result of the execution of the workflow into an external event listener. * Decision management events. Publish events generated by the DMN engine.  * . Enables integration with Knative platform (only available for Quarkus)  * In this post we are going to focus on the two first ones: Messaging and Process Events.  KOGITO MESSAGING ADD-ON When this add-on is active, a BPMN process that defines a Start Message or Catching Intermediate Message element will register a listener. When the  expected event  is triggered by an external source, the process will be accordingly started or resumed. The payload of that event will be mapped to process properties using Message data output association.  Symmetrically, when a BPMN process reaches an End Message or a Throwing Intermediate Message element, it will publish an event that can be consumed by an external listener. The payload of that event will be built using the Message object, as indicated by Message data input association.  Let’s take a look at the following BPMN This process will be started when it receives a message named travellers. The payload of the message is mapped to the traveller process property of type , using Start Message data output assignment.   When the process ends, it will send a Message containing traveler details. The name of the end message depends on the result of the execution of the ProcessTraveler , which determines what nationalities should be allowed to travel. If allowed,  processedtraveller message will be issued; otherwise, no travel message will be sent.  For both End Messages, the Message object is built from the traveller process property, using data input assignment.   The expected payload of the event received by travellers should be convertible to a java object. By default Kogito expects a JSON message, so the payload of the event should resemble the following snippet, where you can see that  the Traveller object is included inside the data field and type field matches the message name. { "specversion": "0.3", "id": "21627e26-31eb-43e7-8343-92a696fd96b1", "source": "", "type": "travellers", "time": "2019-10-01T12:02:23.812262+02:00[Europe/Warsaw]", "data": { "firstName" : "Rafael", "lastName" : "Gordillo", "email" : "vivaerbetis@manquepierda.com", "nationality" : "Spanish" } } Till now, what he has seen is pure BPMN and  target independent, meaning that there is no difference between Quarkus and Springboot. Now we are going to see how to configure both platforms to send Messages over a Kafka broker by enabling specific add-on and properly configuring it.  QUARKUS Messaging Quarkus add-on artifact id is , so, if your project uses Maven,  in order to enable it you need to include in pom.xml &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-quarkus-messaging&lt;/artifactId&gt; &lt;/dependency&gt; This addon implementation  is internally based on library. Smallrye provides a set of connectors for most popular event brokers: JMS, AMQP, Kafka, etc. This implies that the add-on is not specifically tied to any broker, but also requires further steps to indicate which Smallrye connector should be used.  Since we are going to use Kafka, we need to include the connector library in our dependencies set. For maven, we need to add the following dependency in pom.xml.  &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt; &lt;/dependency&gt; This last step just makes the Kaka Connector available in our classpath. In order to allow multi- broker support, Smallrye defines an abstraction called channel. For every channel defined in our application, you need to specify which connector will be used for that channel. This is done through a property of the form: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.connector = &lt;connector name&gt; Optionally, if you choose to use Kafka connector (name is smallrye-kafka), you can define which should be used for that channel, with a property of the form: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.topic = &lt;topic name&gt; If no such property is found, the topic name is assumed to be the same as the Kafka name. In general, we can set up any channel property by using: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.&lt;property name&gt;= &lt;property value&gt; You can find the whole list of properties supported for Kafka   When Smallrye finds a property starting with mp.messaging.[incoming|outgoing].&lt;channel name&gt;, it internally creates a channel. Question is, how many channels should the user define?. There are basically three approaches: * Define one incoming (named kogito_incoming_stream) and outgoing channel (named kogito_outgoing_stream)  for the whole application, so all incoming messages are received in the same channel and all outgoing message are published to the same channel  * Define a channel per each different message name. So every message type (as identified by its name) has a dedicated channel.  * A mix of both. If there is a channel with the same name as the message, that channel will be used for that message. If not, the default channel, if defined, will be used.  MULTIPLE CHANNELS In the previously described BPMN example, there is one incoming message (traveller) and two outgoing messages (processedtraveller and no travel), so it make senses to have one channel per each message. Let’s assume you want travellers messages to be received in travellers topic, processedtravellers messages to be published on processedtraveller topic and no travel messages to be published on cancelledTravellers topic.  In order to achieve that,  in , you must add properties defining channels named travellers, processedtravellers and no travel, specifying kafka as the connector (also you have to establish string as serializer and deserializer). Additionally, in the case of no travel channel, you need to specify that the topic should be cancelledTravellers (or it will publish to topic no travel)  mp.messaging.incoming.travellers.connector=smallrye-kafka mp.messaging.incoming.travellers.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer mp.messaging.outgoing.processedtravellers.connector=smallrye-kafka mp.messaging.outgoing.processedtravellers.value.serializer=org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.no\u0020travel.connector=smallrye-kafka mp.messaging.outgoing.no\u0020travel.topic=cancelledtravellers mp.messaging.outgoing.no\u0020travel.value.serializer=org.apache.kafka.common.serialization.StringSerializer DEFAULT CHANNELS Now, let’s imagine our BPMN example is slightly different and the no travel end message is replaced by an end node that does nothing in case the traveler is skipped. In such case, we will have just one incoming message and just one outgoing message, so the default channel approach, which just defines channels kogito_incoming_stream and kogito_outgoing_stream, suits it perfectly.  Assuming that we want to consume from travellers topic and publish to processtravelers topic, we need to add following configuration to mp.messaging.incoming.kogito_incoming_stream.connector=smallrye-kafka mp.messaging.incoming.kogito_incoming_stream.topic=travellers mp.messaging.incoming.kogito_incoming_stream.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer mp.messaging.outgoing.kogito_outgoing_stream.connector=smallrye-kafka mp.messaging.outgoing.kogito_outgoing_stream.topic=processedtravellers mp.messaging.outgoing.kogito_outgoing_stream.value.serializer=org.apache.kafka.common.serialization.StringSerializer SPRING BOOT Messaging Spring Boot add-on artifact id is , so, if your project uses Maven,  in order to enable it you need to include in your pom.xml &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-springboot-messaging&lt;/artifactId&gt; &lt;/dependency&gt; Contrary to Quarkus add-on, which is vendor independent, Springboot add-on is Kafka specific. So you do not need to add any extra dependency to your pom like in the Quarkus case.  Internally, Spring Boot messaging add-on is implemented using KafkaTemplate and accepts all the properties described in . This allows you to set the list of kafka servers by setting property spring.kafka.bootstrap-servers. Spring Boot add-on Kogito specific configuration is functionally equivalent to Quarkus add-on. You can map topics to messages using the message name as discriminator or/and you might define default topics to be used for all unmapped message names.  In order to map your message name to a particular topic, you need to define a property of the form  kogito.addon.cloudevents.kafka.[kogito_incoming_stream|kogito_outgoing_stream].&lt;message name&gt; = &lt;topic name&gt;.  If there is no such property, the default topic name is used.  Default incoming topic name can be changed by establishing property  kogito.addon.cloudevents.kafka.kogito_incoming_stream=&lt;default topic name&gt; Default outgoing topic name can be changed by establishing property  kogito.addon.cloudevents.kafka.kogito_outgoing_stream=&lt;default topic name&gt; For BPMN example which contains travellers, processedtravellers and  no travel message names, in order each message to use a different topic, you need to add to kogito.addon.cloudevents.kafka.kogito_incoming_stream.travellers=travellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream.processedtravellers=processedtravellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream.no\u0020travel=cancelledtravellers Or, you can also use default topic names for travellers and processedtravellers (note that since there is no explicit mapping for travellers and processedtravellers message names, modified default topic names will be used with equivalent effects than in  the previous configuration)  kogito.addon.cloudevents.kafka.kogito_incoming_stream=travellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream=processedtravellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream.no\u0020travel=cancelledtravellers For the BPMN example which just contains travellers and processedtravellers message names, you just need to define the default topic names, by adding these two properties to   kogito.addon.cloudevents.kafka.kogito_incoming_stream=travellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream=processedtravellers Or you can also use message name to topic mapping  kogito.addon.cloudevents.kafka.kogito_incoming_stream.travellers=travellers kogito.addon.cloudevents.kafka.kogito_outgoing_stream.processedtravellers=processedtravellers KOGITO PROCESS EVENT ADD-ON When this add-on is active,  process, task and variable events, generated as result of the execution of an operation that alters process state, are sent to an external event listener. These events are commonly known as runtime events. Every modifying operation in Kogito is executed within an abstraction called Unit of Work.  The publishing of runtime events occurs once the Unit of Work is completed. Examples of such operations are: creation of a process instance, task transition, variable modification, etc.  One of the main usages of this add-on is to build a historical representation of all process instance executions, like the is doing. It might also be used, together with process REST API, to build a custom Graphical User Interface for human task handling.  Events published by add-on follow specification. The data field contains a Json representation of one these types: * , published when a process instance is created, modified or completed.  * , published when any change occurs on a human task. * , published when a variable is created, modified or removed. Lets now check how we can activate the add-on functionality for Quarkus and Springboot using Kafka broker.  QUARKUS Process event Quarkus add-on artifact id is , so, if your project uses Maven,  in order to enable it you need to include in pom.xml &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-quarkus-events-process&lt;/artifactId&gt; &lt;/dependency&gt; This addon implementation  is internally based on library. Smallrye provides a set of connectors for most popular event brokers: JMS, AMQP, Kafka, etc. This implies that the add-on is not specifically tied to any broker, but also requires further steps to indicate which Smallrye connector should be used.  Since we are going to use Kafka, we need to include the connector library in our dependencies set. For maven, we need to add the following dependency in pom.xml.  &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt; &lt;/dependency&gt; This last step just makes the Kafka Connector available in our classpath. In order to allow multi- broker support, Smallrye defines an abstraction called channel. For every channel defined in our application, you need to specify which connector will be used for that channel. This is done through a property of the form: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.connector = &lt;connector name&gt; Optionally, if you choose to use Kafka connector (name is smallrye-kafka), you can define which should be used for that channel, with a property of the form: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.topic = &lt;topic name&gt; If no such property is found, the topic name is assumed to be the same as the Kafka name. In general, we can set up any channel property by using: mp.messaging.[incoming|outgoing].&lt;channel name&gt;.&lt;property name&gt;= &lt;property value&gt; The process event addon define one channel per each event type: kogito-processinstances-events, kogito-usertaskinstances-events and kogito-variables-events So, when the addon is enabled, assuming you want to use topic names kogito-processinstances-events, kogito-usertaskinstances-events and kogito-variables-events, you should add to you application.properties:  mp.messaging.outgoing.kogito-processinstances-events.connector=smallrye-kafka mp.messaging.outgoing.kogito-processinstances-events.topic=kogito-processinstances-events mp.messaging.outgoing.kogito-processinstances-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.kogito-usertaskinstances-events.connector=smallrye-kafka mp.messaging.outgoing.kogito-usertaskinstances-events.topic=kogito-usertaskinstances-events mp.messaging.outgoing.kogito-usertaskinstances-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.kogito-variables-events.connector=smallrye-kafka mp.messaging.outgoing.kogito-variables-events.topic=kogito-variables-events mp.messaging.outgoing.kogito-variables-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer Kogito also allows you to disable publishing on any of these three channels by setting one or more of these properties to false: kogito.events.processinstances.enabled, kogito.events.usertasks.enabled and kogito.events.variables.enabled.  For example, in case you want to publish process events, but are not interested in variables or task events, you will add to application.properties: kogito.events.usertasks.enabled=false kogito.events.variables.enabled=false SPRING BOOT Process event Spring Boot addon artifact id is , so, if your project uses Maven,  in order to enable it you need to include in your pom.xml: &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-springboot-events-process-kafka&lt;/artifactId&gt; &lt;/dependency&gt; Contrary to Quarkus add-on, which is vendor independent, as its name indicated Spring Boot add-on is Kafka specific. So you do not need to add any extra dependency to your pom like in the Quarkus case.  Internally, Spring Boot messaging add-on is implemented using KafkaTemplate and accepts all the properties described in . This allows you to set the list of kafka servers by setting property spring.kafka.bootstrap-servers. Process event Spring Boot add-on uses three topics: kogito-processinstances-events, kogito-usertaskinstances-events and kogito-variables-events. Kogito also allows you to disable publishing on any of these three channels by setting one or more of these properties to false: kogito.events.processinstances.enabled, kogito.events.usertasks.enabled and kogito.events.variables.enabled.  For example, in case you want to publish task events, but are not interested in variables or process events, you will add to application.properties kogito.events.processinstances.enabled=false kogito.events.variables.enabled=false CONCLUSION We have seen how to configure messaging and process event add-ons to publish and consume Kafka records for Quarkus and Spring Boot. In upcoming post we will discuss how to change the payload format of Kafka record value from default JSON Cloud Event to Apache Avro. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1HvZpazSnDc" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>https://blog.kie.org/2021/09/kogito-process-eventing-add-ons.html</feedburner:origLink></entry><entry><title>Distributed transaction patterns for microservices compared</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/23CiXwiLV1Y/distributed-transaction-patterns-microservices-compared" /><author><name>Bilgin Ibryam</name></author><id>af253bbc-ee33-41e4-a016-53ef3153b9ea</id><updated>2021-09-21T07:00:00Z</updated><published>2021-09-21T07:00:00Z</published><summary type="html">&lt;p&gt;As a consulting architect at Red Hat, I've had the privilege of working on legions of customer projects. Every customer brings their own challenges but I've found some commonalities. One thing most customers want to know is how to coordinate writes to more than one system of record. Answering this question typically involves a long explanation of dual writes, distributed transactions, modern alternatives, and the possible failure scenarios and drawbacks of each approach. Typically, this is the moment when a customer realizes that &lt;a href="https://developers.redhat.com/articles/2021/06/14/application-modernization-patterns-apache-kafka-debezium-and-kubernetes#"&gt;splitting a monolithic application into microservices&lt;/a&gt; is a long and complicated journey, and usually requires tradeoffs.&lt;/p&gt; &lt;p&gt;Rather than go down the rabbit hole of discussing transactions in-depth, this article summarizes the main approaches and patterns for coordinating writes to multiple resources. I’m aware that you might have good or bad past experiences with one or more of these approaches. But in practice, in the right context and with the right constraints, all of these methods work fine. Tech leads are responsible for choosing the best approach for their context.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are interested in dual writes, watch my Red Hat Summit 2021 session, where I covered &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog/session/1607126048915001oL4X"&gt;dual write challenges&lt;/a&gt; in depth. You can also &lt;a href="https://www.slideshare.net/bibryam/dual-write-strategies-for-microservices"&gt;skim through the slides&lt;/a&gt; from my presentation. Currently, I am involved with Red Hat OpenShift Streams for Apache Kafka, a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;fully managed Apache Kafka service&lt;/a&gt;. It takes less than a minute to start and is completely free during the trial period. &lt;a href="https://console.redhat.com/beta/application-services/streams/kafkas"&gt;Give it a try&lt;/a&gt; and help us shape it with your early feedback. If you have questions or comments about this article, hit me on Twitter &lt;a href="https://twitter.com/bibryam"&gt;@bibryam&lt;/a&gt; and let’s get started.&lt;/p&gt; &lt;h2&gt;The dual write problem&lt;/h2&gt; &lt;p&gt;The single indicator that you may have a dual write problem is the need to write to more than one system of record predictably. This requirement might not be obvious and it can express itself in different ways in the distributed systems design process. For example:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You have chosen the best tools for each job and now you have to update a NoSQL database, a search index, and a cache as part of a single business transaction.&lt;/li&gt; &lt;li&gt;The service you have designed has to update its database and also send a notification to another service about the change.&lt;/li&gt; &lt;li&gt;You have business transactions that span multiple services boundaries.&lt;/li&gt; &lt;li&gt;You may have to implement service operations as idempotent because consumers have to retry failed invocations.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For this article, we'll use a single example scenario to evaluate the various approaches to handling dual writes in distributed transactions. Our scenario is a client application that invokes a microservice on a mutating operation. Service A has to update its database, but it also has to call Service B on a write operation, as illustrated in Figure 1. The actual type of the database, the protocol of the service-to-service interactions, is irrelevant for our discussion as the problem remains the same.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1.png?itok=KT3Ab21g" width="600" height="492" alt="The dual write problem in microservices." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The dual write problem in microservices. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;A small but critical clarification explains why there are no simple solutions to this problem. If Service A writes to its database and then sends a notification to a queue for Service B (let’s call it a &lt;em&gt;local-commit-then-publish approach&lt;/em&gt;), there is still a chance the application won't work reliably. While Service A writes to its database and then sends the message to a queue, there is a small probability of the application crashing after the commit to the database and before the second operation, which would leave the system in an inconsistent state. If the message is sent before writing to the database (let’s call this approach &lt;em&gt;publish-then-local-commit&lt;/em&gt;), there is a possibility of database write failing or timing issues where Service B receives the event before Service A has committed the change to its database. In either case, this scenario involves dual writes to a database and a queue, which is the core problem we are going to explore. In the next sections, I will discuss the various implementation approaches available today for this always-present challenge.&lt;/p&gt; &lt;h2&gt;The modular monolith&lt;/h2&gt; &lt;p&gt;Developing your application as a modular monolith might seem like a hack or going backward in architectural evolution, but I have seen it work fine in practice. It is not a microservices pattern but an exception to the microservices rule that can be combined cautiously with microservices. When strong write consistency is the driving requirement, more important even than the ability to deploy and scale microservices independently, then you could go with the modular monolith architecture.&lt;/p&gt; &lt;p&gt;Having a monolithic architecture does not imply that the system is poorly designed or bad. It does not say anything about quality. As the name suggests, it is a system designed in a modular way with exactly one deployment unit. Note that this is a purposefully designed and implemented &lt;a href="https://developers.redhat.com/blog/2016/10/27/the-fast-moving-monolith-how-we-sped-up-delivery-from-every-three-months-to-every-week"&gt;modular monolith&lt;/a&gt;, which is different from an accidentally created monolith that grows over time. In a purposeful modular monolith architecture, every module follows the microservices principles. Each module encapsulates all the access to its data, but the operations are exposed and consumed as in-memory method calls.&lt;/p&gt; &lt;h3&gt;The architecture of a modular monolith&lt;/h3&gt; &lt;p&gt;With this approach, you have to convert both microservices (Service A and Service B) into library modules that can be deployed into a shared runtime. You then make both microservices share the same database instance. Because the services are written and deployed as libraries in a common runtime, they can participate in the same transactions. Because the modules share a database instance, you can use a local transaction to commit or rollback all changes at once. There are also differences around the deployment method because we want the modules to be deployed as libraries within a bigger deployment, and to participate in existing transactions.&lt;/p&gt; &lt;p&gt;Even in a monolithic architecture, there are ways to isolate the code and data. For example, you can segregate the modules into separate packages, build modules, and source code repositories, which can be owned by different teams. You can do partial data isolation by grouping tables by naming convention, schemas, database instances, or even by database servers. The diagram in Figure 2, inspired by Axel Fontaine's talk on &lt;a href="https://www.youtube.com/watch?v=BOvxJaklcr0"&gt;majestic modular monoliths&lt;/a&gt;, illustrates the different code- and data-isolation levels in applications.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%202021-07-03%20at%2010.39.22.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%202021-07-03%20at%2010.39.22.png?itok=UzQyCA0I" width="600" height="464" alt=" Levels of code and data isolation for applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Levels of code and data isolation for applications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The last piece of the puzzle is to use a runtime and a wrapper service capable of consuming other modules and including them in the context of an existing transaction. All of these constraints make the modules more tightly coupled than typical microservices, but the benefit is that the wrapper service can start a transaction, invoke the library modules to update their databases, and commit or roll back the transaction as one operation, without concerns about partial failure or eventual consistency.&lt;/p&gt; &lt;p&gt;In our example, illustrated in Figure 3, we have converted Service A and Service B into libraries and deployed them into a shared runtime, or one of the services could act as the shared runtime. The tables from the databases also share a single database instance, but it is separated as a group of tables managed by the respective library services.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2.png?itok=iNSJIAfJ" width="600" height="732" alt="Modular monolith with a shared database." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Modular monolith with a shared database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Benefits and drawbacks of the modular monolith&lt;/h3&gt; &lt;p&gt;In some industries, it turns out the benefits of this architecture are far more important than the faster delivery and pace of change that are so highly valued at other places. Table 1 summarizes the benefits and drawbacks of the modular monolith architecture.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1: Benefits and drawbacks of the modular monolith architecture.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Simple transaction semantics with local transactions ensuring data consistency, read-your-writes, rollbacks, and so on.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;A shared runtime prevents us from independently deploying and scaling modules, and prevents failure isolation.&lt;/li&gt; &lt;li&gt;The logical separation of tables in a single database is not strong. With time, it can turn into a shared integration layer.&lt;/li&gt; &lt;li&gt;Module coupling and sharing transaction context requires coordination during the development stage and increases the coupling between services.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Runtimes such as Apache Karaf and WildFly that allow modular and dynamic deployment of services.&lt;/li&gt; &lt;li&gt;Apache Camel’s &lt;code&gt;direct&lt;/code&gt; and &lt;code&gt;direct-vm&lt;/code&gt; components allow exposing operations for in-memory invocations and preserve transaction contexts within a JVM process.&lt;/li&gt; &lt;li&gt;Apache Isis is one of the best examples of the modular monolith architecture. It enables domain-driven application development by automatically generating a UI and REST APIs for your Spring Boot applications.&lt;/li&gt; &lt;li&gt;Apache OFBiz is another example of a modular monolith and service-oriented architecture (SOA). It is a comprehensive enterprise resource planning system with hundreds of tables and services that can automate enterprise business processes. Despite its size, its modular architecture allows developers to quickly understand and customize it.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Distributed transactions are typically the last resort, used in a variety of instances:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;When writes to disparate resources cannot be eventually consistent.&lt;/li&gt; &lt;li&gt;When we have to write to heterogeneous data sources.&lt;/li&gt; &lt;li&gt;When exactly-once message processing is required and we cannot refactor a system and make its operations idempotent.&lt;/li&gt; &lt;li&gt;When integrating with third-party black-box systems or legacy systems that implement the two-phase commit specification.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In all of these situations, when scalability is not a concern, we might consider distributed transactions an option.&lt;/p&gt; &lt;h3&gt;Implementing the two-phase commit architecture&lt;/h3&gt; &lt;p&gt;The technical requirements for two-phase commit are that you need a distributed transaction manager such as &lt;a href="https://narayana.io/"&gt;Narayana&lt;/a&gt; and a reliable storage layer for the transaction logs. You also need &lt;a href="https://publications.opengroup.org/standards/dist-computing/c193"&gt;DTP XA&lt;/a&gt;-compatible data sources with associated XA drivers that are capable of participating in distributed transactions, such as RDBMS, message brokers, and caches. If you are lucky to have the right data sources but run in a dynamic environment, such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, you also need an operator-like mechanism to ensure there is only a single instance of the distributed transaction manager. The transaction manager must be highly available and must always have access to the transaction log.&lt;/p&gt; &lt;p&gt;For implementation, you could explore a &lt;a href="https://github.com/snowdrop/narayana-spring-boot/tree/master/openshift/recovery-controller"&gt;Snowdrop Recovery Controller&lt;/a&gt; that uses the &lt;a href="https://developers.redhat.com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns"&gt;Kubernetes StatefulSet pattern&lt;/a&gt; for singleton purposes and persistent volumes to store transaction logs. In this category, I also include specifications such as &lt;a href="http://docs.oasis-open.org/ws-tx/wstx-wsat-1.2-spec.html"&gt;Web Services Atomic Transaction&lt;/a&gt; (WS-AtomicTransaction) for SOAP web services. What all of these technologies have in common is that they implement the XA specification and have a central transaction coordinator.&lt;/p&gt; &lt;p&gt;In our example, shown in Figure 4, Service A is using distributed transactions to commit all changes to its database and a message to a queue without leaving any chance for duplicates or lost messages. Similarly, Service B can use distributed transactions to consume the messages and commit to Database B in a single transaction without any duplicates. Or, Service B can choose not to use distributed transactions, but use local transactions and implement the idempotent consumer pattern. For the record, a more appropriate example for this section would be using WS-AtomicTransaction to coordinate the writes to Database A and Database A in a single transaction and avoid eventual consistency altogether. But that approach is even less common, these days, than what I've described.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3.png?itok=jH8uWG3a" width="600" height="413" alt="Two-phase commit spanning between a database and a message broker." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Two-phase commit spanning between a database and a message broker. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Benefits and drawbacks of the two-phase commit architecture&lt;/h3&gt; &lt;p&gt;The two-phase commit protocol offers similar guarantees to local transactions in the modular monolith approach, but with a few exceptions. Because there are two or more separate data sources involved in an atomic update, they may fail in a different manner and block the transaction. But thanks to its central coordinator, it is still easy to discover the state of the distributed system compared to the other approaches I will discuss.&lt;/p&gt; &lt;p&gt;Table 2 summarizes the benefits and drawbacks of this approach.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 2: Benefits and drawbacks of two-phase commit.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Standard-based approach with out-of-the-box transaction managers and supporting data sources.&lt;/li&gt; &lt;li&gt;Strong data consistency for the happy scenarios.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Scalability constraints.&lt;/li&gt; &lt;li&gt;Possible recovery failures when the transaction manager fails.&lt;/li&gt; &lt;li&gt;Limited data source support.&lt;/li&gt; &lt;li&gt;Storage and singleton requirements in dynamic environments.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;The &lt;a href="https://en.wikipedia.org/wiki/Java_Transaction_API"&gt;Jakarta Transactions API&lt;/a&gt; (formerly Java Transaction API)&lt;/li&gt; &lt;li&gt;WS-AtomicTransaction&lt;/li&gt; &lt;li&gt;JTS/IIOP&lt;/li&gt; &lt;li&gt;eBay’s &lt;a href="https://tech.ebayinc.com/engineering/grit-a-protocol-for-distributed-transactions-across-microservices/"&gt;GRIT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Atomikos&lt;/li&gt; &lt;li&gt;Narayana&lt;/li&gt; &lt;li&gt;Message brokers such as Apache ActiveMQ&lt;/li&gt; &lt;li&gt;Relational data sources that implement the XA spec, in-memory data stores such as &lt;a href="https://infinispan.org/about/"&gt;Infinispan&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Orchestration&lt;/h2&gt; &lt;p&gt;With a modular monolith, we use local transactions and we always know the state of the system. With distributed transactions based on the two-phase commit protocol, we also guarantee a consistent state. The only exception would be an unrecoverable failure that involved the transaction coordinator. But what if we wanted to ease the consistency requirements while still knowing the state of the overall distributed system and coordinating from a single place? In this case, we might consider an &lt;em&gt;orchestration&lt;/em&gt; approach, where one of the services acts as the coordinator and orchestrator of the overall distributed state change. The orchestrator service has the responsibility to call other services until they reach the desired state or take corrective actions if they fail. The orchestrator uses its local database to keep track of state changes, and it is responsible for recovering any failures related to state changes.&lt;/p&gt; &lt;h3&gt;Implementing an orchestration architecture&lt;/h3&gt; &lt;p&gt;The most popular implementations of the orchestration technique are BPMN specification implementations such as the &lt;a href="https://www.jbpm.org/"&gt;jBPM&lt;/a&gt; and &lt;a href="https://github.com/camunda/camunda-bpm-platform"&gt;Camunda&lt;/a&gt; projects. The need for such systems doesn’t disappear with overly distributed architectures such as microservices or serverless; on the contrary, it increases. For proof, we can look to newer stateful orchestration engines that do not follow a specification but provide similar stateful behavior, such as Netflix’s &lt;a href="https://netflix.github.io/conductor/"&gt;Conductor&lt;/a&gt;, Uber’s &lt;a href="https://github.com/uber/cadence"&gt;Cadence&lt;/a&gt;, and Apache's &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt;. Serverless stateful functions such as Amazon StepFunctions, Azure Durable Functions, and Azure Logic Apps are in this category, as well. There are also open source libraries that allow you to implement stateful coordination and rollback behavior such as Apache Camel’s &lt;a href="https://camel.apache.org/components/latest/eips/saga-eip.html"&gt;Saga&lt;/a&gt; pattern implementation and the NServiceBus &lt;a href="https://docs.particular.net/nservicebus/sagas/"&gt;Saga&lt;/a&gt; capability. The many homegrown systems implementing the Saga pattern are also in this category.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4.png?itok=VRJKpDNh" width="600" height="483" alt="Orchestrating distributed transactions between two services." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Orchestrating distributed transactions between two services. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;In our example diagram, shown in Figure 5, we have Service A acting as the stateful orchestrator responsible to call Service B and recover from failures through a compensating operation if needed. The crucial characteristic of this approach is that Service A and Service B have local transaction boundaries, but Service A has the knowledge and the responsibility to orchestrate the overall interaction flow. That is why its transaction boundary touches Service B endpoints. In terms of implementation, we could set this up with synchronous interactions, as shown in the diagram, or using a message queue in between the services (in which case you could use a two-phase commit, too).&lt;/p&gt; &lt;h3&gt;Benefits and drawbacks of orchestration&lt;/h3&gt; &lt;p&gt;Orchestration is an &lt;em&gt;eventually consistent&lt;/em&gt; approach that may involve retries and rollbacks to get the distribution into a consistent state. While it avoids the need for distributed transactions, orchestration requires the participating services to offer idempotent operations in case the coordinator has to retry an operation. Participating services also must offer recovery endpoints in case the coordinator decides to roll back and fix the global state. The big advantage of this approach is the ability to drive heterogeneous services that might not support distributed transactions into a consistent state by using only local transactions. The coordinator and the participating services need only local transactions, and it is always possible to discover the state of the system by asking the coordinator, even if it is in a partially consistent state. Doing that is not possible with the other approaches I will describe.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 3: Benefits and drawbacks of orchestration.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Coordinates state among heterogeneous distributed components.&lt;/li&gt; &lt;li&gt;No need for XA transactions.&lt;/li&gt; &lt;li&gt;Known distributed state at the coordinator level.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Complex distributed programming model.&lt;/li&gt; &lt;li&gt;May require idempotency and compensating operations from the participating services.&lt;/li&gt; &lt;li&gt;Eventual consistency.&lt;/li&gt; &lt;li&gt;Possibly unrecoverable failures during compensations.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;jBPM&lt;/li&gt; &lt;li&gt;Camunda&lt;/li&gt; &lt;li&gt;MicroProfile &lt;a href="https://github.com/eclipse/microprofile-lra"&gt;Long Running Actions&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Conductor&lt;/li&gt; &lt;li&gt;Cadence&lt;/li&gt; &lt;li&gt;Step Functions&lt;/li&gt; &lt;li&gt;Durable Functions&lt;/li&gt; &lt;li&gt;Apache Camel Saga pattern implementation&lt;/li&gt; &lt;li&gt;NServiceBus Saga pattern implementation&lt;/li&gt; &lt;li&gt;The CNCF &lt;a href="https://serverlessworkflow.io/"&gt;Serverless Workflow&lt;/a&gt; specification&lt;/li&gt; &lt;li&gt;Homegrown implementations&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Choreography&lt;/h2&gt; &lt;p&gt;As you've seen in the discussion so far, a single business operation can result in multiple calls among services, and it can take an indeterminate amount of time before a business transaction is processed end-to-end. To manage this, the orchestration pattern uses a centralized controller service that tells the participants what to do.&lt;/p&gt; &lt;p&gt;An alternative to orchestration is &lt;em&gt;choreography&lt;/em&gt;, which is a style of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. Historically, the most common implementation for the choreography approach was using an asynchronous messaging layer for the service interactions. Figure 6 illustrates the basic architecture of the choreography pattern.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5.png?itok=t7MXveuI" width="600" height="417" alt="Service choreography through a messaging layer." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Service choreography through a messaging layer. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Choreography with a dual write&lt;/h3&gt; &lt;p&gt;For message-based choreography to work, we need each participating service to execute a local transaction and trigger the next service by publishing a command or event to a messaging infrastructure. Similarly, other participating services have to consume a message and perform a local transaction. That in itself is a dual-write problem within a higher-level dual-write problem. When we develop a messaging layer with a dual write to implement the choreography approach, we could design it as a two-phase commit that spans a local database and a message broker. I covered that approach earlier. Alternatively, we might use a &lt;em&gt;publish-then-local-commit&lt;/em&gt; or &lt;em&gt;local-commit-then-publish&lt;/em&gt; pattern:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Publish-then-local-commit&lt;/strong&gt;: We could try to publish a message first and then commit a local transaction. While this option might sound fine, it has practical challenges. For example, very often you need to publish an ID that is generated from the local transaction commit, which won’t be available to publish. Also, the local transaction might fail, but we cannot rollback the published message. This approach lacks read-your-write semantics and it is an impractical solution for most use cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-commit-then-publish&lt;/strong&gt;: A slightly better approach would be to commit the local transaction first and then publish the message. This has a small probability of failure occurring after a local transaction has been committed and before publishing the message. But even in that case, you could design your services to be idempotent and retry the operation. That would mean committing the local transaction again and then publishing the message. This approach can work if you control the downstream consumers and can make them idempotent, too. It's also a pretty good implementation option overall.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Choreography without a dual write&lt;/h3&gt; &lt;p&gt;The various ways of implementing a choreography architecture constrain every service to write only to a single data source with a local transaction, and nowhere else. Let’s see how that could work without a dual write.&lt;/p&gt; &lt;p&gt;Let’s say Service A receives a request and writes it to Database A, and nowhere else. Service B periodically polls Service A and detects new changes. When it reads the change, Service B updates its own database with the change and also the index or timestamp up to which it picked up the changes. The critical part here is the fact that both services only write to their own database and commit with a local transaction. This approach, illustrated in Figure 7, can be described as &lt;em&gt;service choreography&lt;/em&gt;, or we could describe it using the good old data pipeline terminology. The possible implementation options are more interesting.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5.1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5.1.png?itok=Pn0g_ir5" width="600" height="549" alt="Service choreography through polling." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Service choreography through polling. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The simplest scenario is for Service B to connect to the Service A database and read the tables owned by Service A. The industry tries to avoid that level of coupling with shared tables, however, and for a good reason: Any change in Service A's implementation and data model could break Service B. We can make a few gradual improvements to this scenario, for example by using the &lt;a href="https://microservices.io/patterns/data/transactional-outbox.html"&gt;Outbox pattern&lt;/a&gt; and giving Service A a table that acts as a public interface. This table could only contain the data Service B requires, and it could be designed to be easy to query and track for changes. If that is not good enough, a further improvement would be for Service B to ask Service A for any changes through an &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;API management layer&lt;/a&gt; rather than connecting directly to Database A.&lt;/p&gt; &lt;p&gt;Fundamentally, all of these variations suffer from the same drawback: Service B has to poll Service A continuously. Doing this can lead to unnecessary continuous load on the system or unnecessary delay in picking up the changes. Polling a microservice for changes is a hard sell, so let’s see what we can do to further improve this architecture.&lt;/p&gt; &lt;h3&gt;Choreography with Debezium&lt;/h3&gt; &lt;p&gt;One way to improve a choreography architecture and make it more attractive is to introduce a tool like &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;Debezium&lt;/a&gt;, which we can use to perform change data capture (CDC) using Database A’s transaction log. Figure 8 illustrates this approach.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/6.png?itok=d46hUqWE" width="600" height="393" alt="Service choreography with change data capture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Service choreography with change data capture. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Debezium can monitor a database's transaction log, perform any necessary filtering and transformation, and deliver relevant changes into an Apache Kafka topic. This way, Service B can listen to generic events in a topic rather than polling Service A's database or APIs. Swapping database polling for streaming changes and introducing a queue between the services makes the distributed system more reliable, scalable, and opens up the possibility of introducing other consumers for new use cases. Using Debezium offers an elegant way to implement the &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;Outbox pattern&lt;/a&gt; for orchestration- or choreography-based &lt;a href="https://www.infoq.com/articles/saga-orchestration-outbox/"&gt;Saga pattern implementations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A side-effect of this approach is that it introduces the possibility of Service B receiving duplicate messages. This can be addressed by implementing the service as idempotent, either at the business logic level or with a technical deduplicator (with something like Apache ActiveMQ Artemis’s &lt;a href="https://activemq.apache.org/components/artemis/documentation/1.1.0/duplicate-detection.html"&gt;duplicate message detection&lt;/a&gt; or Apache Camel's idempotent consumer pattern).&lt;/p&gt; &lt;h3&gt;Choreography with event sourcing&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Event sourcing&lt;/em&gt; is another implementation of the service choreography approach. With this pattern, the state of an entity is stored as a sequence of state-changing events. When there is a new update, rather than updating the entity's state, a new event is appended to the list of events. Appending new events to an event store is an atomic operation done in a local transaction. The beauty of this approach, shown in Figure 9, is that the event store also behaves like a message queue for other services to consume updates.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/7.png?itok=4NCBG5KL" width="600" height="524" alt="Service choreography through event sourcing." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Service choreography through event sourcing. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Our example, when converted to use event sourcing, would store client requests in an append-only event store. Service A can reconstruct its current state by replaying the events. The event store also needs to allow Service B to subscribe to the same update events. With this mechanism, Service A uses its storage layer also as the communication layer with other services. While this mechanism is very neat and solves the problem of reliably publishing events whenever the state change occurs, it introduces a new programming style unfamiliar to many developers and additional complexity around state reconstruction and message compaction, which require specialized data stores.&lt;/p&gt; &lt;h3&gt;Benefits and drawbacks of choreography&lt;/h3&gt; &lt;p&gt;Regardless of the mechanism used to retrieve data changes, the choreography approach decouples writes, allows independent service scalability, and improves overall system resiliency. The downside of this approach is that the flow of decision-making is decentralized and it is hard to discover the globally distributed state. Discovering the state of a request requires querying multiple data sources which can be challenging with a large number of services. Table 4 summarizes the benefits and drawbacks of this approach.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 4: Benefits and drawbacks of choreography.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Decouples implementation and interaction.&lt;/li&gt; &lt;li&gt;No central transaction coordinator.&lt;/li&gt; &lt;li&gt;Improved scalability and resilience characteristics.&lt;/li&gt; &lt;li&gt;Near real-time interactions.&lt;/li&gt; &lt;li&gt;Less overhead on the system with Debezium and similar tools.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;The global system state and coordination logic is scattered across all participants.&lt;/li&gt; &lt;li&gt;Eventual consistency.&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Homegrown database or API polling implementations.&lt;/li&gt; &lt;li&gt;The outbox pattern&lt;/li&gt; &lt;li&gt;Choreography based on the Saga pattern&lt;/li&gt; &lt;li&gt;&lt;a href="https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/"&gt;event sourcing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://eventuate.io/"&gt;Eventuate&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/debezium/debezium"&gt;Debezium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/zendesk/maxwell"&gt;Zendesk's Maxwell&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/canal"&gt;Alibaba's Canal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/linkedin/Brooklin/"&gt;Linkedin's Brooklin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://axoniq.io/"&gt;Axon Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.eventstore.com/eventstoredb"&gt;EventStoreDB&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Parallel pipelines&lt;/h2&gt; &lt;p&gt;With the choreography pattern, there is no central place to query the state of the system, but there is a sequence of services that propagates the state through the distributed system. Choreography creates a sequential pipeline of processing services, so we know that when a message reaches a certain step of the overall process, it has passed all the previous steps. What if we could loosen this constraint and process all the steps independently? In this scenario, Service B could process a request regardless of whether Service A had processed it or not.&lt;/p&gt; &lt;p&gt;With parallel pipelines, we add a router service that accepts requests and forwards them to Service A and Service B through a message broker in a single local transaction. From this step onward, as shown in Figure 10, both services can process the requests independently and in parallel.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/8.png?itok=8d5GcUrE" width="600" height="840" alt="Processing through parallel pipelines." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Processing through parallel pipelines. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;While this pattern is very simple to implement, it is only applicable to situations where there is no temporal binding between the services. For example, Service B should be able to process the request regardless of whether Service A has processed the same request. Also, this approach requires an additional router service or the client being aware of both Service A and B for targeting the messages.&lt;/p&gt; &lt;h3&gt;Listen to yourself&lt;/h3&gt; &lt;p&gt;There is a lighter alternative to this approach, known as the &lt;a href="https://medium.com/@odedia/listen-to-yourself-design-pattern-for-event-driven-microservices-16f97e3ed066"&gt;Listen to yourself&lt;/a&gt; pattern, where one of the services also acts as the router. With this alternative approach, when Service A receives a request, it would not write to its database but would instead publish the request into the messaging system, where it is targeted to Service B, and to itself. Figure 11 illustrates this pattern.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/9_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/9_0.png?itok=vnpBhA9Q" width="600" height="455" alt="The Listen to yourself pattern." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: The Listen to yourself pattern. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The reason for not writing to the database is to avoid dual writes. Once a message is in the messaging system, the message goes to Service B, and also it goes to back Service A in a completely separate transaction context. With that twist of the processing flow, Service A, and Service B can independently process the request and write to their respective databases.&lt;/p&gt; &lt;h3&gt;Benefits and drawbacks of parallel pipelines&lt;/h3&gt; &lt;p&gt;Table 5 summarizes the benefits and drawbacks of using parallel pipelines.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 5: Benefits and drawbacks of parallel pipelines.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Simple, scalable architecture for parallel processing.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Requires temporal dismantling; hard to reason about the global system state.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Apache Camel’s multicast and splitter with parallel processing.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;How to choose a distributed transactions strategy&lt;/h2&gt; &lt;p&gt;As you might have already guessed from this article, there is no right or wrong pattern for handling distributed transactions in a microservices architecture. Every pattern has its pros and cons. Each pattern solves some problems while generating others in turn. The chart in Figure 12 offers a short summary of the main characteristics of the dual write patterns I've discussed.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/10.png?itok=AX6B1LRd" width="1440" height="630" alt="Characteristics of dual write patterns." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: Characteristics of dual write patterns. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Whatever approach you choose, you will need to explain and document the motivation behind the decision and the long-lasting architectural consequences of your choice. You will also need to get support from the teams that will implement and maintain the system in the long term. I like to organize and evaluate the approaches described in this article based on their data consistency and scalability attributes, as shown in Figure 13.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/11.png?itok=k9nxg-Zu" width="1432" height="730" alt="Relative data consistency and scalability characteristics of dual write patterns." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: Relative data consistency and scalability characteristics of dual write patterns. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;As a good starting point, we could evaluate the various approaches from the most scalable and highly available to the least scalable and available ones.&lt;/p&gt; &lt;h3&gt;High: Parallel pipelines and choreography&lt;/h3&gt; &lt;p&gt;If your steps are temporarily decoupled, then it could make sense to run them in a parallel pipelines method. The chances are you can apply this pattern for certain parts of the system, but not for all of them. Next, assuming there is a temporal coupling between the processing steps, and certain operations and services have to happen before others, you might consider the choreography approach. Using service choreography, it is possible to create a scalable, &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt; where messages flow from service to service through a decentralized orchestration process. In this case, Outbox pattern implementations with Debezium and Apache Kafka (such as &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;) are particularly interesting and gaining traction.&lt;/p&gt; &lt;h3&gt;Medium: Orchestration and two-phase commit&lt;/h3&gt; &lt;p&gt;If choreography is not a good fit, and you need a central point that is responsible for coordination and decision making, then you would consider orchestration. This is a popular architecture, with standard-based and custom open source implementations available. While a standard-based implementation may force you to use certain transaction semantics, a custom orchestration implementation allows you to make a trade-off between the desired data consistency and scalability.&lt;/p&gt; &lt;h3&gt;Low: Modular monolith&lt;/h3&gt; &lt;p&gt;If you are going further left in the spectrum, most likely you have a very strong need for data consistency and you are ready to pay for it with significant tradeoffs. In this case, distributed transactions through two-phase commits will work with certain data sources, but they are difficult to implement reliably on dynamic cloud environments designed for scalability and high availability. In that case, you can go all the way to the good old modular monolith approach, accompanied by practices learned from the microservices movement. This approach ensures the highest data consistency but at the price of runtime and data source coupling.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In a sizable distributed system with tens of services, there won’t be a single approach that works for all, but a few of these combined and applied for different contexts. You might have a few services deployed on a shared runtime for exceptional requirements around data consistency. You might choose a two-phase commit for integration with a legacy system that supports JTA. You might orchestrate a complex business process, and also use choreography and parallel processing for the rest of the services. In the end, it doesn't matter what strategy you pick; what matters is choosing a strategy deliberately for the right reasons, and executing it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/21/distributed-transaction-patterns-microservices-compared" title="Distributed transaction patterns for microservices compared"&gt;Distributed transaction patterns for microservices compared&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/23CiXwiLV1Y" height="1" width="1" alt=""/&gt;</summary><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2021-09-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/21/distributed-transaction-patterns-microservices-compared</feedburner:origLink></entry><entry><title>Quarkus for Spring developers: Getting started</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3i0sklVc1Bw/quarkus-spring-developers-getting-started" /><author><name>Eric Deandrea</name></author><id>d4f91d87-1ed8-4cb9-a48d-51c9dd13d5f7</id><updated>2021-09-20T07:00:00Z</updated><published>2021-09-20T07:00:00Z</published><summary type="html">&lt;p class="Indent1"&gt;Want to learn more about developing applications with Quarkus? Download our free e-book &lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt;, which helps Java developers familiar with Spring make a quick and easy transition.&lt;/p&gt; &lt;p&gt;The tools available in the &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt; ecosystem make it easy to get started with building applications. However, the same is true for &lt;a href="https://developers.redhat.com/topics/quarkus/"&gt;Quarkus&lt;/a&gt;, which has many additional features and capabilities aimed at improving the developer experience. A Spring developer can quickly get started working with a Quarkus project and immediately become more productive, as we'll see in this article. Plug-ins and tooling are available for most major IDEs, including &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-quarkus"&gt;VSCode&lt;/a&gt;, &lt;a href="https://www.jetbrains.com/help/idea/quarkus.html"&gt;IntelliJ&lt;/a&gt;, and &lt;a href="https://tools.jboss.org/features/quarkus.html"&gt;Eclipse&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Creating a new project&lt;/h2&gt; &lt;p&gt;A Spring developer’s starting point is the &lt;a href="https://start.spring.io/"&gt;Spring Initializr&lt;/a&gt;, which can be used directly from the website, as shown in Figure 1, or via plug-ins for most major IDEs.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Figure 1 - Spring Initializr" data-entity-type="file" data-entity-uuid="d7579de7-735a-4016-8760-0b4c00866657" src="https://developers.redhat.com/sites/default/files/inline-images/Figure%201.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Spring Initializr.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Quarkus projects start at &lt;a href="https://code.quarkus.io"&gt;Code Quarkus&lt;/a&gt;. The project generator can be used directly from the website, as shown in Figure 2, or via plug-ins for all of the IDEs mentioned previously.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Figure 2 - Code Quarkus" data-entity-type="file" data-entity-uuid="304f42ab-6f6b-46a4-adb7-0a34a7c002bb" src="https://developers.redhat.com/sites/default/files/inline-images/Figure%202.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Code Quarkus.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Using Code Quarkus, you can describe the project's Maven coordinates, build tool of choice, and the technologies needed by the application, known as Quarkus extensions. Highlighted in Figure 2 are the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;An extension that provides example code.&lt;/li&gt; &lt;li&gt;Whether or not to include an extension's example code during project generation.&lt;/li&gt; &lt;li&gt;Extensions newer to the community are marked &lt;em&gt;PREVIEW&lt;/em&gt;. Such an extension may have API or configuration changes as it matures.&lt;/li&gt; &lt;li&gt;Extensions brand new to the community are marked &lt;em&gt;EXPERIMENTAL&lt;/em&gt;. Such an extension provides no guarantees of stability or long-term presence until it matures.&lt;/li&gt; &lt;li&gt;Additional details about each extension, including a link to the guide for the extension and how to add the extension to an existing project.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Project structure&lt;/h2&gt; &lt;p&gt;A Quarkus project’s structure is similar to a Spring Boot project’s structure. Table 1 details some files and directories of a Quarkus project.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1"&gt;&lt;caption&gt;Table 1: Quarkus project structure.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;File / Directory&lt;/th&gt; &lt;th scope="col"&gt;Description&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;README.md&lt;/td&gt; &lt;td&gt;Contains instructions on building and running the application, including native image.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;pom.xml&lt;br /&gt; build.gradle&lt;/p&gt; &lt;/td&gt; &lt;td&gt;Project build file.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;src/main/docker&lt;/td&gt; &lt;td&gt;Sample Dockerfiles for use in different modes (JVM, native image).&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;src/main/java&lt;/td&gt; &lt;td&gt;All application source code.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;src/main/resources&lt;/td&gt; &lt;td&gt;All application resources.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;src/main/resources/application.properties&lt;br /&gt; src/main/resources/application.yml&lt;br /&gt; src/main/resources/application.yaml&lt;/p&gt; &lt;/td&gt; &lt;td&gt;All application configurations, unified in a single file.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;src/main/resources/META-INF/resources&lt;/td&gt; &lt;td&gt;All application static content.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;src/test/java&lt;/td&gt; &lt;td&gt;All application test code.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Quarkus Dev Mode and live coding&lt;/h2&gt; &lt;p&gt;The Quarkus developer experience begins with a simple command: &lt;code&gt;./mvnw quarkus:dev&lt;/code&gt; (for Maven) or &lt;code&gt;./gradlew quarkusDev&lt;/code&gt; (for Gradle). This command will start Quarkus’s &lt;a href="https://quarkus.io/guides/maven-tooling#dev-mode"&gt;Dev Mode&lt;/a&gt;, which automatically detects changes made to a project. Changes could involve source files, configuration, static resources, or classpath dependencies. Quarkus transparently recompiles and redeploys detected changes typically in under a second. This feature allows developers to continuously write code and have the underlying platform seamlessly incorporate changes into the running application.&lt;/p&gt; &lt;p&gt;In some instances, it may not be possible or feasible to provision necessary dependent services onto a local developer workstation. In these instances, the only option is to continually deploy the application into an environment where these services are available. Without Quarkus, a developer needs to rebuild and redeploy the application each time a change is made. In some instances, this redeployment could take a minute or more. Running &lt;code&gt;./mvnw quarkus:remote-dev&lt;/code&gt; (for Maven) or &lt;code&gt;./gradlew quarkusRemoteDev&lt;/code&gt; (for Gradle) will enable &lt;a href="https://developers.redhat.com/blog/2021/02/11/enhancing-the-development-loop-with-quarkus-remote-development#step_2__live_coding_in_your_local_environment"&gt;Remote Dev Mode&lt;/a&gt;, extending automatic reloading to applications deployed on remote hosts, typically redeploying and reloading within a second or two. This capability greatly reduces the developer’s inner feedback loop.&lt;/p&gt; &lt;h2&gt;Dev Services&lt;/h2&gt; &lt;p&gt;Does your application require a running &lt;a href="https://quarkus.io/guides/datasource#dev-services-configuration-free-databases"&gt;database&lt;/a&gt;? &lt;a href="https://quarkus.io/guides/kafka-dev-services"&gt;Apache Kafka broker&lt;/a&gt;? &lt;a href="https://quarkus.io/guides/redis-dev-services"&gt;Redis server&lt;/a&gt;? &lt;a href="https://quarkus.io/guides/amqp-dev-services"&gt;AMQP broker&lt;/a&gt;? &lt;a href="https://quarkus.io/guides/apicurio-registry-dev-services"&gt;OpenID Connect Authorization Server&lt;/a&gt;? &lt;a href="https://quarkus.io/guides/apicurio-registry-dev-services"&gt;API/Schema registry&lt;/a&gt;? When running Quarkus Dev Mode, Quarkus Dev Services automatically bootstraps infrastructure needed by an application based on what extensions are present. For example, Quarkus will automatically bootstrap a database container image and set all the required configuration properties if it sees a database extension present and no configuration has been supplied.&lt;/p&gt; &lt;p&gt;Quarkus Dev Services speed up inner-loop development by providing necessary infrastructure without you having to think about it. New services are continually added to the platform.&lt;/p&gt; &lt;h2&gt;Continuous testing&lt;/h2&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Test-driven_development"&gt;Test-driven development&lt;/a&gt; is an adopted practice within many development teams. &lt;a href="https://youtu.be/0JiE-bRt-GU"&gt;Continuous testing&lt;/a&gt; support in Quarkus enables developers to take advantage of this practice. When running Quarkus Dev Mode, you can enable continuous testing with the press of a key, enabling Quarkus to automatically rerun tests affected by a code change in the background.&lt;/p&gt; &lt;p&gt;Quarkus understands which tests are affected by classes and methods within the application. As you make code changes, you get immediate feedback if the change passes your existing test suite. This capability is integrated directly into Quarkus—no IDE or special tooling is required.&lt;/p&gt; &lt;h2&gt;Natively native&lt;/h2&gt; &lt;p&gt;&lt;code&gt;./mvnw package -Pnative&lt;/code&gt; (for Maven) or &lt;code&gt;./gradlew build -Dquarkus.package.type=native&lt;/code&gt; (for Gradle) is all that’s needed to build a Quarkus application into a native image. All the heavy lifting and integration to consume GraalVM is done for you by the Quarkus build tools. Developers or CI/CD systems simply need to run a build, just like any other &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; build, to produce a native executable. Tests can even be run against the built artifact.&lt;/p&gt; &lt;p&gt;Quarkus’s design accounted for native compilation from the onset. A Quarkus native executable starts much faster and uses far less memory than a traditional JVM. Similar native image capabilities in Spring are still considered experimental or beta.&lt;/p&gt; &lt;h2&gt;Kubernetes native&lt;/h2&gt; &lt;p&gt;Quarkus was designed with low memory optimization and fast startup times in mind, making it a perfect fit for containers and container orchestration platforms such as &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Quarkus performs as much processing as possible at build time, reducing the size and footprint of the application running on the JVM. Many more Quarkus applications can be deployed than other Java or Spring applications.&lt;/p&gt; &lt;p&gt;Simply adding the &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes"&gt;Quarkus Kubernetes extension&lt;/a&gt; (or the &lt;a href="https://quarkus.io/guides/deploying-to-openshift"&gt;Quarkus OpenShift extension&lt;/a&gt;, if using &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;) will automatically generate Kubernetes (or OpenShift) resources based on the project. Want to run your application serverless? The Kubernetes (or OpenShift) extension can also &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes#knative"&gt;generate Knative resources&lt;/a&gt;, enabling you to deploy your application in a serverless fashion, whether it's on the JVM or a native image. Combined with previously-mentioned capabilities like &lt;em&gt;Remote Dev Mode&lt;/em&gt; or continuous testing, Quarkus gives you a Kubernetes-native development experience.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;There are many free resources available for learning about and getting started with Quarkus. Why wait for the future? Since its inception in 2019 and continuing today and into the future, Quarkus has provided a familiar and innovative framework for Java developers, supporting capabilities developers need and want today.&lt;/p&gt; &lt;p&gt;Check out these other available resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Read &lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; to learn about what challenges led to Quarkus and see side-by-side examples of familiar Spring concepts, constructs, and conventions.&lt;/li&gt; &lt;li&gt;Learn &lt;a href="https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices"&gt;why you should choose Quarkus over Spring&lt;/a&gt; for microservices development.&lt;/li&gt; &lt;li&gt;Explore &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Quarkus quick starts&lt;/a&gt; in the &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for experimenting with containerized applications.&lt;/li&gt; &lt;li&gt;Try free &lt;a href="https://learn.openshift.com/developing-with-quarkus/"&gt;15-minute interactive learning scenarios&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/get-started/"&gt;Get started with Quarkus&lt;/a&gt; on your own.&lt;/li&gt; &lt;li&gt;Learn about Quarkus's &lt;a href="https://quarkus.io/guides/spring-di#more-spring-guides"&gt;Spring compatibility features&lt;/a&gt;. &lt;ul&gt;&lt;li&gt;In some instances, there might not be any code changes needed to &lt;a href="https://developers.redhat.com/blog/2021/02/09/spring-boot-on-quarkus-magic-or-madness/"&gt;run a Spring application on Quarkus&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get &lt;a href="https://github.com/RedHat-Middleware-Workshops/spring-to-quarkus-todo"&gt;hands-on converting a Spring Boot application to Quarkus&lt;/a&gt; with little-to-no code changes.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Attend a &lt;a href="https://quarkus.io/worldtour/"&gt;Quarkus World Tour&lt;/a&gt; event when it comes to a city near you. You can even request a private stop.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/20/quarkus-spring-developers-getting-started" title="Quarkus for Spring developers: Getting started"&gt;Quarkus for Spring developers: Getting started&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3i0sklVc1Bw" height="1" width="1" alt=""/&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2021-09-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/20/quarkus-spring-developers-getting-started</feedburner:origLink></entry><entry><title type="html">WildFly 25 Beta1 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GqIG_PPUbFU/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/09/20/WildFly25-Beta-Released/</id><updated>2021-09-20T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 25.0.0.Beta1 releases are available for download at . Work during the WildFly 25 development cycle has been primarily oriented toward completing a long planned evolution of our security layer. Before diving into that topic, there are a few significant new feature areas where feedback from our users would be most welcome. * WildFly still provides MicroProfile OpenTracing as an alternative, but I encourage users to switch to the new OpenTelemetry subsystem. * This allows you to do things like configure integration with a Keycloak server without needing to install the Keycloak client adapters. * An , along with and security integration related improvements, particularly the * Ability to use the same expression as the value of a server configuration attribute and This makes it easier to reuse configuraiton in different deployment enviroments, particularly in cloud environments where environment variables are more readily used than system properties. ELYTRON ALL THE WAY; REMOVAL OF LEGACY SECURITY SUPPORT Over four years ago with the WildFly 11 release, WildFly introduced integration of the Elytron security project as its next generation security layer. From that point onward, use of Elytron has been our recommended approach to security in WildFly. However, we’ve continued to provide as well what we call "legacy security", i.e. security services that often integrate with Elytron under the covers, but which use to some degree the Picketbox security project that was the basis for security in JBoss AS and WildFly releases prior to WildFly 11. We continued to provide "legacy security" both to give our users time to shift their use to Elytron, and to give the Elytron layer time to evolve. Which it very much has, often accounting for the largest number of new features in a WildFly release! We deprecated the use of legacy security long ago, but now with the WildFly 25 release, we are removing support for it. There are a few reasons for this: * The Picketbox and related Picketlink projects have not had an active community for some years now, and their project repos on Github are archived. There is no feature or enhancement work in these projects. For bug fixes WildFly has needed to rely on occasional bug fix releases produced by Red Hat for users of its Red Hat JBoss Enterprise Application Platform product. * Picketbox makes extensive use of Java SE packages that were pruned in SE 14. With the advent of the SE 17 LTS release, we need to eliminate use of libraries that cannot function on SE 17. * We feel the Elytron solution is better and that our users who have not yet migrated to it are better served by doing so. As part of this change you will see a number of significant changes in WildFly 25 Beta1: * Our standard configuration files no longer include legacy security realms. These are the 'security-realm' elements found under the 'management' element in a standalone.xml or host.xml file, administered via the CLI at '/core-service=management/security-realm=*' addresses. The xml parsers no longer support these elements and the management API no longer provides resources at these addresses. Elytron subsystem resources are now used. * Use of the Picketbox-based security vault is no longer supported. Elytron credential stores should be used instead. * The 'org.wildlfy.extension.picketlink' extension and the 'picketlink-federation' and 'picketlink-idm' subsystems it provided are no longer supported on servers not running in 'admin-only' mode. They can still be used on a WildFly 25 Domain Controller to allow it to manage hosts running earlier versions of WildFly. * The 'org.jboss.as.security' extension and the 'security' subsystem it provides are no longer part of our standard configuration files. By the time WildFly 25.0.0.Final is released our intent is that these will no longer be supported on servers not running in 'admin-only' mode. The extension and subystem can still be used on a WildFly 25 Domain Controller to allow it to manage hosts running earlier versions of WildFly. Note that the reason use of the legacy security and picketlink extensions is allowed on an 'admin-only' server is to allow a server with a configuration using those to boot so an administrator can then use the CLI to alter the server configuration to use Elytron. I very much encourage any of you still using legacy security in your configuration to start experimenting with WildFly 25. SE 17 While there are a few small issues related to running WildFly and WildFly Preview on SE 17 that we’ll sort out before WildFly 25 Final, overall it runs well and I encourage our community to try it and let us know if you have any problems. DOMAIN MODE AND "MIXED DOMAINS" One aspect of WildFly’s domain mode of operation that doesn’t get a lot of attention is the ability for a Domain Controller running the latest version to manage remote Host Controllers running earlier versions. We call this a "mixed domain". Every few years we prune the number of legacy versions that a Domain Controller can support. We’re doing this with WildFly 25, restricting mixed domain support to hosts running WildFly 23 or later. WILDFLY SOURCE-TO-IMAGE (S2I) IMAGES The s2i image for WildFly 25.0.0.Beta1 is not yet available, but we expect it will be within a few days. When it is we’ll announce it separately. RELEASE NOTES The release notes for the release are , with issues fixed in the underlying WildFly Core betas listed . Please try it out and give us your feedback, while we get to work WildFly 25 Final! Best regards, Brian&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GqIG_PPUbFU" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/09/20/WildFly25-Beta-Released/</feedburner:origLink></entry></feed>
