<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>MicroProfile 3.2 is now available on Open Liberty in Red Hat Runtimes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/C7UNl7AIc1s/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Open Liberty" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Application Runtimes" scheme="searchisko:content:tags" /><author><name>Laura Cowen</name></author><id>searchisko:content:id:jbossorg_blog-microprofile_3_2_is_now_available_on_open_liberty_in_red_hat_runtimes</id><updated>2019-12-11T13:56:41Z</updated><published>2019-12-11T13:56:41Z</published><content type="html">&lt;p&gt;&lt;a href="https://openliberty.io/about/"&gt;Open Liberty&lt;/a&gt; 19.0.0.12 provides support for &lt;a href="https://microprofile.io/"&gt;MicroProfile 3.2&lt;/a&gt;, allowing users to provide their own health check procedures and monitor microservice applications easily with metrics. Additionally, updates allow trust to be established using the JDK&amp;#8217;s default truststore or a certificate through an environment variable.&lt;/p&gt; &lt;p&gt;In Open Liberty 19.0.0.12:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#mp32"&gt;MicroProfile 3.2 support&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#hc21"&gt;Provide your own health check procedures (MicroProfile Health Check 2.1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#hm22"&gt;Monitor microservice applications easily wth metrics (MicroProfile Metrics 2.2)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#jmo"&gt;Jaeger support in MicroProfile Open Tracing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#ssl"&gt;Trusted certificate enhancements (Transport Security 1.0)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#rrs"&gt;Liberty reader role support&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Run your apps using 19.0.0.12&lt;/h2&gt; &lt;p&gt;If you&amp;#8217;re using &lt;a href="https://openliberty.io/guides/maven-intro.html"&gt;Maven&lt;/a&gt;, here are the coordinates:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.openliberty&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;openliberty-runtime&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;19.0.0.12&amp;#60;/version&amp;#62; &amp;#60;type&amp;#62;zip&amp;#60;/type&amp;#62; &amp;#60;/dependency&amp;#62; &lt;/pre&gt; &lt;p&gt;Or for &lt;a href="https://openliberty.io/guides/gradle-intro.html"&gt;Gradle&lt;/a&gt;:&lt;/p&gt; &lt;pre class="brush: plain; title: ; notranslate"&gt; dependencies { libertyRuntime group: 'io.openliberty', name: 'openliberty-runtime', version: '[19.0.0.12,)' } &lt;/pre&gt; &lt;p&gt;Or if you&amp;#8217;re using &lt;a href="https://openliberty.io/guides/containerize.html"&gt;Docker&lt;/a&gt;:&lt;/p&gt; &lt;pre class="brush: plain; title: ; notranslate"&gt; FROM open-liberty &lt;/pre&gt; &lt;div id="mp32"&gt; &lt;h2&gt;MicroProfile 3.2 support&lt;/h2&gt; &lt;p&gt;Add the whole of MicroProfile 3.2 to your application with this convenience feature in your &lt;code&gt;server.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;featureManager&amp;#62; &amp;#60;feature&amp;#62;microProfile-3.2&amp;#60;/feature&amp;#62; &amp;#60;featureManager&amp;#62; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;microProfile-3.2&lt;/code&gt; feature automatically includes the following features in your app: JAX-RS 2.1, CDI 2.0, JSON-P 1.1, JSON-B 1.0, MicroProfile Config 1.3, MicroProfile Fault Tolerance 2.0, MicroProfile Health Check 2.1, MicroProfile JWT 1.1, MicroProfile Metrics 2.2, MicroProfile OpenAPI 1.1, MicroProfile OpenTracing 1.3, and MicroProfile Rest Client 1.3.&lt;/p&gt; &lt;p&gt;The MicroProfile Health Check and Metrics features contain updates.&lt;/p&gt; &lt;div id="hc21"&gt; &lt;h3&gt;Provide your own health check procedures (MicroProfile Health Check 2.1)&lt;/h3&gt; &lt;p&gt;MicroProfile Health Check 2.1 enables you to provide your own health check procedures to be invoked by Liberty, to verify the health of your microservice:&lt;/p&gt; &lt;pre class="brush: java; title: ; notranslate"&gt; HealthCheckResponse.up(&amp;#34;myCheck&amp;#34;); &lt;/pre&gt; &lt;p&gt;In previous versions, to define a simple successful/failed named health check response, the application level code is always expected to use several static methods together from the HealthCheckResponse API to retrieve a HealthCheckResponseBuilder used to construct a HealthCheck response.&lt;/p&gt; &lt;p&gt;In the &lt;code&gt;mpHealth-2.1&lt;/code&gt; feature for OpenLiberty, you can now use convenient and simpler methods from standard Java APIs, to construct UP/DOWN named health check responses, in your applications, such as &lt;code&gt;HealthCheckResponse.named(“myCheck”).up().build();&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To make it work include the following line in the &lt;code&gt;server.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;feature&amp;#62;mpHealth-2.1&amp;#60;/feature&amp;#62; &lt;/pre&gt; &lt;p&gt;Applications are expected to provide health check procedures by implementing the HealthCheck interface with the &lt;code&gt;@Liveness&lt;/code&gt; or &lt;code&gt;@Readiness&lt;/code&gt; annotations, which are used by Liberty to verify the Liveness or Readiness of the application, respectively. Add the logic of your health check in the &lt;code&gt;call()&lt;/code&gt; method, and return the HealthCheckResponse object by constructing using the simple &lt;code&gt;up()&lt;/code&gt; and &lt;code&gt;down()&lt;/code&gt; methods from the API. To view the status of each health check, access either the &lt;code&gt;+http://:/health/live+&lt;/code&gt; or &lt;code&gt;+http://:/health/ready+&lt;/code&gt; endpoints.&lt;/p&gt; &lt;pre class="brush: java; title: ; notranslate"&gt; **Liveness Check** @Liveness @ApplicationScoped public class AppLiveCheck implements HealthCheck { ... @Override public HealthCheckResponse call() { ... HealthCheckResponse.up(&amp;#34;myCheck&amp;#34;); ... } } &lt;/pre&gt; &lt;p&gt;For more information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/eclipse/microprofile-health/releases/tag/2.1"&gt;MicroProfile Health Check 2.1 Release Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://download.eclipse.org/microprofile/microprofile-health-2.1/apidocs/"&gt;Javadocs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://download.eclipse.org/microprofile/microprofile-health-2.1/microprofile-health-spec.html"&gt;Specification document&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div id="hm22"&gt; &lt;h3&gt;Monitor microservice applications easily wth metrics (MicroProfile Metrics 2.2 )&lt;/h3&gt; &lt;p&gt;MicroProfile Metrics 2.2 enables developers to instrument metrics in their (microservice) applications for easy monitoring by their operations team.&lt;/p&gt; &lt;p&gt;Previously, the MetadataBuilder API had &lt;code&gt;reusable()&lt;/code&gt; and &lt;code&gt;notReusable()&lt;/code&gt; method to set the reusable field to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;. The MetadataBuilder API has been changed to include a new setter method for the reusable attribute. This change is implemented so the MetadataBuilder API follows the builder pattern.&lt;/p&gt; &lt;p&gt;To enable the feature in the &lt;code&gt;server.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;feature&amp;#62;mpMetrics-2.2&amp;#60;/feature&amp;#62; &lt;/pre&gt; &lt;p&gt;The example shows how to set the reusable field with the MetadataBuilder API:&lt;/p&gt; &lt;pre class="brush: java; title: ; notranslate"&gt; MetadataBuilder mdb = Metadata.builder(); &lt;/pre&gt; &lt;pre class="brush: java; title: ; notranslate"&gt; mdb = mdb.withName(&amp;#34;metricName&amp;#34;).withType(MetricType.COUNTER) .reusable(resolveIsReusable()); &lt;/pre&gt; &lt;p&gt;For more information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Changes to MicroProfile metrics&lt;/li&gt; &lt;li&gt;Microserice observability metrics&lt;/li&gt; &lt;/ul&gt; &lt;div id="jmo"&gt; &lt;h2&gt;Jaeger support added for tracing (MicroProfile OpenTracing 1.3)&lt;/h2&gt; &lt;p&gt;Open Liberty has added support for Jaeger in MicroProfile OpenTracing. A &lt;a href="https://github.com/WASdev/sample.opentracing.zipkintracer"&gt;sample tracer is available&lt;/a&gt; for using Zipkin as a tracing backend. With the addition of Jaeger support, developers can also use Jaeger as a tracing backend.&lt;/p&gt; &lt;p&gt;You can download the Jaeger client version 0.34.0 library and its dependencies from &lt;a href="https://mvnrepository.com/artifact/io.jaegertracing/jaeger-client/0.34.0"&gt;Maven repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In the server.xml:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;library id=&amp;#34;jaegerLib&amp;#34; apiTypeVisibility=&amp;#34;+third-party&amp;#34; &amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/gson-2.8.2.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/jaeger-client-0.34.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/jaeger-core-0.34.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/jaeger-thrift-0.34.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/jaeger-tracerresolver-0.34.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/libthrift-0.12.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/slf4j-api-1.7.25.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/slf4j-jdk14-1.7.25.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/opentracing-util-0.31.0.jar&amp;#34; /&amp;#62; &amp;#60;file name=&amp;#34;&amp;#60;path&amp;#62;/jaegerLib_0.34/opentracing-noop-0.31.0.jar&amp;#34; /&amp;#62; &amp;#60;/library&amp;#62; &lt;/pre&gt; &lt;p&gt;Define your application:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;webApplication location=&amp;#34;yourapp.war&amp;#34; contextRoot=&amp;#34;/yourapp&amp;#34;&amp;#62; &amp;#60;!-- enable visibility to third party apis --&amp;#62; &amp;#60;classloader commonLibraryRef=&amp;#34;jaegerLib&amp;#34; apiTypeVisibility=&amp;#34;+third-party&amp;#34; /&amp;#62; &amp;#60;/webApplication&amp;#62; &lt;/pre&gt; &lt;p&gt;You can find out more about Jaeger settings set up using environment variables by looking at &lt;a href="https://github.com/jaegertracing/jaeger-client-java/blob/10c641f8df6316f1eac4d5b1715513275bcd724e/jaeger-core/README.md"&gt;jaeger-client-java readme&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For the &lt;code&gt;JAEGER_PASSWORD&lt;/code&gt; environment variable, the password can be encoded using the securityUtility command. Depending on Jaeger&amp;#8217;s sampling settings &lt;code&gt;JAEGER_SAMPLER_TYPE&lt;/code&gt; and &lt;code&gt;JAEGER_SAMPLER_PARAM&lt;/code&gt;, Jaeger may not report every spans created by the applications.&lt;/p&gt; &lt;div id="ssl"&gt; &lt;h2&gt;Trusted certificate enhancements (Transport Security 1.0)&lt;/h2&gt; &lt;p&gt;Open Liberty now offers new options to help establish trust for TLS connections. An easy way to use the JDK&amp;#8217;s default truststore for trust and a way to pass the certificate needed to establish trust to a truststore through an environment variable is now provided.&lt;/p&gt; &lt;h3&gt;Establishing trust using the JDK&amp;#8217;s default truststore&lt;/h3&gt; &lt;p&gt;By default, the JDK default truststore is the &lt;code&gt;cacerts&lt;/code&gt; file. The default truststore may be set by the &lt;code&gt;javax.net.ssl.truststore&lt;/code&gt; system property or the &lt;code&gt;jssecacerts&lt;/code&gt; file if users have one configured. For Open Liberty to use what is configured as the JDK default truststore the &lt;code&gt;useDefaultCerts&lt;/code&gt; attribute needs to be set to &lt;code&gt;true&lt;/code&gt; on the &lt;code&gt;ssl&lt;/code&gt; element. It is set to &lt;code&gt;false&lt;/code&gt; by default. For example:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;ssl id=&amp;#34;defaultSSLConfig&amp;#34; keyStoreRef=&amp;#34;defaultKeyStore&amp;#34; trustStoreRef=&amp;#34;defaultTrustStore&amp;#34; trustDefaultCerts=&amp;#34;true&amp;#34; /&amp;#62; &amp;#60;keyStore id=&amp;#34;defaultKeyStore&amp;#34; location=&amp;#34;key.p12&amp;#34; type=&amp;#34;PKCS12&amp;#34; password=&amp;#60;your password&amp;#62; /&amp;#62; &amp;#60;keyStore id=&amp;#34;defaultTrustStore&amp;#34; location=&amp;#34;trust.p12&amp;#34; type=&amp;#34;PKCS12&amp;#34; password=&amp;#60;your password&amp;#62; /&amp;#62; &lt;/pre&gt; &lt;p&gt;With &lt;code&gt;trustDefaultCerts&lt;/code&gt; set to &lt;code&gt;true&lt;/code&gt;, the server will try to establish trust with the configured truststore, in this case &lt;code&gt;defaultTrustStore&lt;/code&gt;, first. If trust is not establish with the configured truststore then it will try to use the JDK&amp;#8217;s default truststore to establish trust.&lt;/p&gt; &lt;h3&gt;Providing a certificate through an environment variable to establish trust&lt;/h3&gt; &lt;p&gt;Open Liberty will read a certificate from an environment variable and add it to a keystore or truststore so it can be used for trust. The certificate will be added to the runtime copy of the keystore or truststore and will not be stored to the file system. If the keystore configuration includes the &lt;code&gt;readOnly&lt;/code&gt; attribute set to &lt;code&gt;true&lt;/code&gt; then the certificate will not be included.&lt;/p&gt; &lt;p&gt;The environment variable key must be in the format &lt;code&gt;cert_ + keystore id&lt;/code&gt;. For example:&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;keyStore id=&amp;#34;myKeyStore&amp;#34; location=&amp;#34;myKey.p12&amp;#34; type=&amp;#34;PKCS12&amp;#34; password=&amp;#60;your password&amp;#62; /&amp;#62; &lt;/pre&gt; &lt;p&gt;The key of the environment variable should be &lt;code&gt;cert_myKeyStore&lt;/code&gt; (it is case-sensitive).&lt;/p&gt; &lt;p&gt;The value of the environment variable can either be a certificate in the base 64-bit format or the path to a file containing a base 64-bit encode certificate or DER-encoded certificate. If using the base 64-bit encode certificate directly on the environment variable, it must contain the &lt;code&gt;-----BEGIN CERTIFICATE-----&lt;/code&gt; and &lt;code&gt;-----END CERTIFICATE-----&lt;/code&gt; tags. For example:&lt;/p&gt; &lt;pre class="brush: plain; title: ; notranslate"&gt; cert_myKeyStore=&amp;#34;-----BEGIN CERTIFICATE----- .... -----END CERTIFICATE-----&amp;#34; &lt;/pre&gt; &lt;p&gt;The environment variable for a file will look similar to:&lt;/p&gt; &lt;p&gt;&lt;code&gt;cert_myKeyStore=/Users/me/servercert.crt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any value not starting with the &lt;code&gt;-----BEGIN CERTIFICATE-----&lt;/code&gt; tag will be treated like a file.&lt;/p&gt; &lt;p&gt;[#rrs]&lt;/p&gt; &lt;h2&gt;Liberty reader role support (Application Security 2.0 and Application Security 3.0)&lt;/h2&gt; &lt;p&gt;The reader role is a management role that allows read-only access to select administrative REST APIs as well as the Admin Center UI (&lt;code&gt;adminCenter-1.0&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Prior to this release, the Administrator management role was the only management role within Open Liberty and it provided read and write access. The new Reader management role provides the ability to assign a read-only role to users and groups. This will allow those users and groups to monitor the server without granting those users the ability to modify the server in anyway.&lt;/p&gt; &lt;p&gt;Using the new Reader management role is nearly identical to using the Administrator management role. In the &lt;code&gt;server.xml&lt;/code&gt; include the &lt;code&gt;appSecurity-2.0&lt;/code&gt; or &lt;code&gt;appSecurity-3.0&lt;/code&gt; feature and also add the new &lt;code&gt;reader-role&lt;/code&gt; configuration element, that specifies the group(s), user(s), and/or the access ID of the group(s) or user(s) that should be granted the Reader management role.&lt;/p&gt; &lt;pre class="brush: xml; title: ; notranslate"&gt; &amp;#60;server&amp;#62; &amp;#60;featureManager&amp;#62; &amp;#60;feature&amp;#62;appSecurity-3.0&amp;#60;/feature&amp;#62; &amp;#60;/featureManager&amp;#62; &amp;#60;reader-role&amp;#62; &amp;#60;group&amp;#62;group&amp;#60;/group&amp;#62; &amp;#60;group-access-id&amp;#62;group:realmName/groupUniqueId&amp;#60;/group-access-id&amp;#62; &amp;#60;user&amp;#62;user&amp;#60;/user&amp;#62; &amp;#60;user-access-id&amp;#62;user:realmName/userUniqueId&amp;#60;/user-access-id&amp;#62; &amp;#60;/reader-role&amp;#62; &amp;#60;/server&amp;#62; &lt;/pre&gt; &lt;h2&gt;Try Open Liberty 19.0.0.12 in Red Hat Runtimes now&lt;/h2&gt; &lt;p&gt;Open Liberty is part of the Red Hat Runtimes offering. If you&amp;#8217;re a &lt;a href="https://access.redhat.com/products/red-hat-runtimes"&gt;Red Hat Runtimes subscriber&lt;/a&gt;, you can try Open Liberty now.&lt;/p&gt; &lt;p&gt;To learn more about deploying Open Liberty applications to OpenShift, take a look at our &lt;a href="https://openliberty.io/guides/cloud-openshift.html"&gt;Open Liberty guide: Deploying microservices to OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#38;linkname=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fmicroprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes%2F&amp;#038;title=MicroProfile%203.2%20is%20now%20available%20on%20Open%20Liberty%20in%20Red%20Hat%20Runtimes" data-a2a-url="https://developers.redhat.com/blog/2019/12/11/microprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes/" data-a2a-title="MicroProfile 3.2 is now available on Open Liberty in Red Hat Runtimes"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/11/microprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes/"&gt;MicroProfile 3.2 is now available on Open Liberty in Red Hat Runtimes&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/C7UNl7AIc1s" height="1" width="1" alt=""/&gt;</content><summary>Open Liberty 19.0.0.12 provides support for MicroProfile 3.2, allowing users to provide their own health check procedures and monitor microservice applications easily with metrics. Additionally, updates allow trust to be established using the JDK’s default truststore or a certificate through an environment variable. In Open Liberty 19.0.0.12: MicroProfile 3.2 support Provide your own health check ...</summary><dc:creator>Laura Cowen</dc:creator><dc:date>2019-12-11T13:56:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/11/microprofile-3-2-is-now-available-on-open-liberty-in-red-hat-runtimes/</feedburner:origLink></entry><entry><title>Keycloak: Core concepts of open source identity and access management</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0MFvLHTm98k/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="keycloak" scheme="searchisko:content:tags" /><category term="Red Hat SSO" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><author><name>akoserwa</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_core_concepts_of_open_source_identity_and_access_management</id><updated>2019-12-11T08:00:38Z</updated><published>2019-12-11T08:00:38Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2018/03/19/sso-made-easy-keycloak-rhsso/"&gt;Keycloak&lt;/a&gt; provides the flexibility to export and import configurations easily, using a single view to manage everything. Together, these technologies let you integrate front-end, mobile, and monolithic applications into a &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservice&lt;/a&gt; architecture. In this article, we discuss the core concepts and features of &lt;a href="https://www.keycloak.org" target="_blank" rel="noopener noreferrer"&gt;Keycloak&lt;/a&gt; and its application integration mechanisms. You will find links to implementation details near the end.&lt;/p&gt; &lt;h2&gt;Core concepts&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with Keycloak&amp;#8217;s core concepts, as shown in Figure 1:&lt;/p&gt; &lt;div id="attachment_657657" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657657" class="wp-image-657657 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak1-1024x587.png" alt="Keycloak core concepts" width="640" height="367" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak1-1024x587.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak1-300x172.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak1-768x440.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak1.png 1565w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657657" class="wp-caption-text"&gt;Figure 1: Keycloak&amp;#8217;s core concepts.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;A Keycloak &lt;em&gt;realm&lt;/em&gt; is like a namespace that allows you to manage all of your metadata and configurations. You can have multiple realms based on your requirements. Generally, it is recommended to avoid using the &lt;em&gt;master realm&lt;/em&gt;, which is for administration purposes only.&lt;/p&gt; &lt;p&gt;In Figure 1, you can see the information that Keycloak lets you manage, namely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clients (per application)&lt;/li&gt; &lt;li&gt;Configuration management&lt;/li&gt; &lt;li&gt;Custom themes (UI)&lt;/li&gt; &lt;li&gt;Events&lt;/li&gt; &lt;li&gt;Federation&lt;/li&gt; &lt;li&gt;LDAP or Active Directory integration&lt;/li&gt; &lt;li&gt;User management (users and groups)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can have one client that contains configuration information for a single application, such as the URL, protocol, and redirect URI.&lt;/p&gt; &lt;p&gt;Figure 2 shows how Keycloak gives you access to all of this information in a single view:&lt;/p&gt; &lt;div id="attachment_657707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657707" class="size-large wp-image-657707" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak2-1024x562.png" alt="Keycloak's UI offers access to many settings." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak2-1024x562.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak2-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak2-768x421.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657707" class="wp-caption-text"&gt;Figure 2: Keycloak&amp;#8217;s UI offers access to many settings.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Why should you use Keycloak?&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s take a look at why you might choose Keycloak, aside from the sheer amount of management you can accomplish within a single view.&lt;/p&gt; &lt;h3&gt;Keycloak is reliable&lt;/h3&gt; &lt;p&gt;Keycloak is a reliable solution, designed following standard security protocols to provide a dynamic single sign-on solution. Red Hat runs on Red Hat products, which includes single sign-on (SSO), and Red Hat trusts the upstream product Keycloak for their downstream product &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on" target="_blank" rel="noopener noreferrer"&gt;Red Hat SSO&lt;/a&gt;. Red Hat SSO handles Red Hat&amp;#8217;s entire authentication and authorization system. Additionally, Keycloak is licensed under &lt;a href="https://github.com/keycloak/keycloak/blob/master/License.html" target="_blank" rel="noopener nofollow noreferrer"&gt;Apache License Version 2.0&lt;/a&gt; and has a strong and active open source community.&lt;/p&gt; &lt;h3&gt;Keycloak supports standard protocols&lt;/h3&gt; &lt;p&gt;Keycloak supports the following standard protocols:&lt;/p&gt; &lt;ul class=""&gt; &lt;li&gt;OAuth 2.0&lt;/li&gt; &lt;li&gt;OpenID Connect&lt;/li&gt; &lt;li&gt;SAML 2.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This support means that any tool or application that supports integration with the above protocols can be plugged into with Keycloak (for example, enterprise applications like &lt;a href="https://access.redhat.com/products/ansible-tower-red-hat" target="_blank" rel="noopener noreferrer"&gt;Red Hat Ansible Tower&lt;/a&gt; or SAP Business Intelligence Platform).&lt;/p&gt; &lt;h3&gt;Keycloak is ready for production&lt;/h3&gt; &lt;p&gt;As mentioned earlier, Keycloak is already being used in production. Before doing so yourself, make sure to go through the production-readiness documentation.&lt;/p&gt; &lt;h2&gt;Launching Keycloak&lt;/h2&gt; &lt;p&gt;To launch Keycloak with Docker, use:&lt;/p&gt; &lt;pre&gt;$ docker pull jboss/keycloak $ docker run -d -e KEYCLOAK_USER=&amp;#60;USERNAME&amp;#62; -e KEYCLOAK_PASSWORD=&amp;#60;PASSWORD&amp;#62; -p 8081:8080 jboss/keycloak&lt;/pre&gt; &lt;p&gt;However, your configuration information (like realm settings, clients, or certificates) will be temporary in this scenario. Therefore, export the configuration and re-import every time before you instantiate a new container. In other words, use a persistent volume for storing the state.&lt;/p&gt; &lt;p&gt;To do a standalone Keycloak launch (&lt;a href="https://www.keycloak.org/downloads.html" target="_blank" rel="noopener nofollow noreferrer"&gt;https://www.keycloak.org/downloads.html&lt;/a&gt;) with JBoss WildFly, use:&lt;/p&gt; &lt;pre&gt;$ keycloak-x.x.x.Final/bin&amp;#62;./standalone.sh&lt;/pre&gt; &lt;h2&gt;Preparing to integrate with Keycloak&lt;/h2&gt; &lt;p&gt;Once you&amp;#8217;re ready to integrate your apps, tools, and services with Keycloak, you have decisions to make (see Figure 3):&lt;/p&gt; &lt;div id="attachment_657747" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657747" class="wp-image-657747 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak3-1024x576.png" alt="Keycloak integration relationship map." width="640" height="360" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak3-1024x576.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak3-300x169.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/keycloak3-768x432.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657747" class="wp-caption-text"&gt;Figure 3: Keycloak integration map.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;First, you need to decide which protocol you intend to use, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OAuth2&lt;/li&gt; &lt;li&gt;OpenID Connect&lt;/li&gt; &lt;li&gt;Security Assertion Markup Language (SAML).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are you looking for &lt;em&gt;authentication&lt;/em&gt; or &lt;em&gt;authorization&lt;/em&gt;?&lt;/p&gt; &lt;pre&gt;OAuth 2&lt;em&gt; != Authentication&lt;/em&gt;, only &lt;strong&gt;Authorization &lt;/strong&gt; OpenID Connect = Identity + Authentication + Authorization&lt;/pre&gt; &lt;p&gt;Now, regarding the application:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is it running on a container (stateless) or is it in a legacy clustered (shared state) environment?&lt;/li&gt; &lt;li&gt;What does the architecture consist of, such as single-page applications (SPA), microservices, serverless, or MVC?&lt;/li&gt; &lt;li&gt;Identify the resources and endpoints you want to secure. Is your integration between, for example, client and server, service-to-service, or API endpoints.&lt;/li&gt; &lt;li&gt;Identify which adapter will be suited for your architecture.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Integrating with Keycloak&lt;/h2&gt; &lt;p&gt;To integrate your apps with Keycloak:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a realm. You can use &lt;code&gt;master&lt;/code&gt; for a dev environment or base it on your business domain (for example, &lt;code&gt;external-apps&lt;/code&gt;or &lt;code&gt;internal-apps&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Create a client for your application (for example, &lt;code&gt;hello-world-app&lt;/code&gt;). Client configuration requires details like this: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Protocol:&lt;/strong&gt; Which protocol, such as SAML or OpenID.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resource Endpoint:&lt;/strong&gt; The application hostname or REST endpoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Redirect URI:&lt;/strong&gt; Where to redirect the user when authentication is granted.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Provide the client configuration to your application as input, such as: &lt;ul class=""&gt; &lt;li&gt;The clientId (i.e.,&lt;code&gt;hello-world-app&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;The realm (i.e.,&lt;code&gt;external-apps&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;The Keycloak server&amp;#8217;s URL.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That’s all you need to do in order to configure your application with Keycloak.&lt;/p&gt; &lt;h2&gt;Wrapping up&lt;/h2&gt; &lt;p&gt;In conclusion, you can refer to the following integration patterns when you work with Keycloak yourself:&lt;/p&gt; &lt;ul class=""&gt; &lt;li&gt;&lt;a href="https://medium.com/keycloak/secure-vue-js-app-with-keycloak-94814181e344" target="_blank" rel="noopener noreferrer"&gt;Vue&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/keycloak/secure-angular-app-with-keycloak-63ec934e5093" target="_blank" rel="noopener noreferrer"&gt;Angular&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/keycloak/secure-react-app-with-keycloak-4a65614f7be2" target="_blank" rel="noopener noreferrer"&gt;React&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/keycloak/secure-spring-boot-2-using-keycloak-f755bc255b68" target="_blank" rel="noopener noreferrer"&gt;Spring-boot 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a class="bo dc jz ka kb kc" href="https://medium.com/keycloak/quarkus-and-react-integration-with-keycloak-e03eb82d8cd" target="_blank" rel="noopener noreferrer"&gt;Quarkus and React&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.keycloak.org/documentation.html" target="_blank" rel="noopener noreferrer"&gt;Keycloak Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.keycloak.org/docs/latest/securing_apps/index.html"&gt;Securing Applications and Services Guide&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy secure coding!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#38;linkname=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F11%2Fkeycloak-core-concepts-of-open-source-identity-and-access-management%2F&amp;#038;title=Keycloak%3A%20Core%20concepts%20of%20open%20source%20identity%20and%20access%20management" data-a2a-url="https://developers.redhat.com/blog/2019/12/11/keycloak-core-concepts-of-open-source-identity-and-access-management/" data-a2a-title="Keycloak: Core concepts of open source identity and access management"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/11/keycloak-core-concepts-of-open-source-identity-and-access-management/"&gt;Keycloak: Core concepts of open source identity and access management&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0MFvLHTm98k" height="1" width="1" alt=""/&gt;</content><summary>Keycloak provides the flexibility to export and import configurations easily, using a single view to manage everything. Together, these technologies let you integrate front-end, mobile, and monolithic applications into a microservice architecture. In this article, we discuss the core concepts and features of Keycloak and its application integration mechanisms. You will find links to implementation...</summary><dc:creator>akoserwa</dc:creator><dc:date>2019-12-11T08:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/11/keycloak-core-concepts-of-open-source-identity-and-access-management/</feedburner:origLink></entry><entry><title>This Week in JBoss, 10th December 2019 - Camel 3, Infinispan 10.1, Keycloak 8... A bucketload of releases!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/a5ToOBWtoac/this-week-in-jboss-10th-december-2019-camel-3-infinispan-101-keycloak-8-a-bucketload-of-releases" /><category term="AMQ" scheme="searchisko:content:tags" /><category term="Camel" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="infinispan-10" scheme="searchisko:content:tags" /><category term="keycloak" scheme="searchisko:content:tags" /><category term="kogito" scheme="searchisko:content:tags" /><category term="microprofile" scheme="searchisko:content:tags" /><category term="narayana" scheme="searchisko:content:tags" /><category term="Operators" scheme="searchisko:content:tags" /><author><name>Romain Pelisse</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_10th_december_2019_camel_3_infinispan_10_1_keycloak_8_a_bucketload_of_releases</id><updated>2019-12-10T14:49:26Z</updated><published>2019-12-10T14:49:26Z</published><content type="html">&lt;!-- [DocumentBodyStart:5ee2f36e-7030-4c29-b663-04f6ff4b06a9] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;&lt;em&gt;There've been some noteworthy releases in the last two weeks such as &lt;a class="jive-link-external-small" href="https://infinispan.org/blog/2019/12/09/infinispan-10/" rel="nofollow"&gt;Infinispan 10.1.0.CR1&lt;/a&gt;, &lt;a class="jive-link-external-small" href="https://www.keycloak.org/2019/12/keycloak-801-released.html" rel="nofollow"&gt;Keycloak 8.0.1&lt;/a&gt; and, of course, &lt;a class="jive-link-external-small" href="http://janstey.blogspot.com/2019/11/apache-camel-3-is-out.html" rel="nofollow"&gt;Camel 3.0&lt;/a&gt; !&amp;#160; Just taking a look at all the new cool features coming with those should already keep you busy! But if it&amp;#8217;s not enough, don&amp;#8217;t worry, the rest of the JBoss community has you covered!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://farm1.staticflickr.com/755/22604188601_612696b9a7_b.jpg"&gt;&lt;img alt="" class="image-1 jive-image" src="https://farm1.staticflickr.com/755/22604188601_612696b9a7_b.jpg" style="width: 620px; height: 453px; display: block; margin-left: auto; margin-right: auto;"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;Pimp your tooling&lt;/h1&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Developers like sysadmins can only accomplish their work properly with the right tooling. What could a developer do nowadays without Github or a decent IDE? Same goes for admin. That&amp;#8217;s why you might be interested to know about a couple of new tools that have been released in the last weeks. The first one is a &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/25/kogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github/" rel="nofollow"&gt;Kogito tooling for friendly DMN and BPMN visualization on GitHub &lt;/a&gt;&amp;mdash; if you do anything with BPMN and/or Kogito, you should definitely check it out! We&amp;#8217;ve mentioned IDE as being a crucial tool for the developer, so you&amp;#8217;ll be happy to read about the &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/21/new-features-in-quarkus-tools-for-visual-studio-code-1-2-0/" rel="nofollow"&gt;New features in Quarkus Tools for Visual Studio Code 1.2.0&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;Beyond tooling, knowledge is also a strong ally of the developer, so maybe checking this &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/12/02/new-eclipse-microprofile-book-provides-introduction-to-enterprise-java-microservices/" rel="nofollow"&gt;New Eclipse MicroProfile book provides introduction to enterprise Java microservices&lt;/a&gt; might do you good &lt;span aria-label="Happy" class="emoticon_happy emoticon-inline" style="height:16px;width:16px;"&gt;&lt;/span&gt;. As we are talking theorical matter and concept, you should also take a look at this article on&lt;/p&gt;&lt;p&gt;&lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/12/03/testing-in-production-from-devtestoops-to-devtestops/" rel="nofollow"&gt;Testing in production: From DevTestOops to DevTestOps&lt;/a&gt;...&lt;/p&gt;&lt;p&gt;&lt;a href="https://live.staticflickr.com/6193/6074298666_2017626332_b.jpg"&gt;&lt;img alt="" class="image-2 jive-image" src="https://live.staticflickr.com/6193/6074298666_2017626332_b.jpg" style="width: 620px; height: 465px; display: block; margin-left: auto; margin-right: auto;"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h1&gt;All you ever wanted to know about AMQ Streams (even on OpenShift!)&lt;/h1&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;OK, if you ever wanted to learn anything or everything on the AMQ Streams you are in for a treat. First, you have a nice overview of &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/21/event-based-microservices-with-red-hat-amq-streams/" rel="nofollow"&gt;Event-based microservices with Red Hat AMQ Streams&lt;/a&gt;&lt;/p&gt;&lt;p&gt;, but if it&amp;#8217;s not enough you have a three parts detailed series on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/" rel="nofollow"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes (Part 1) &lt;/a&gt;, &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/" rel="nofollow"&gt;Part 2&lt;/a&gt; and &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/" rel="nofollow"&gt;Part 3&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href="https://farm5.staticflickr.com/4641/38237140825_f078a14863_b.jpg"&gt;&lt;img alt="" class="image-3 jive-image" src="https://farm5.staticflickr.com/4641/38237140825_f078a14863_b.jpg" style="width: 620px; height: 414px; display: block; margin-left: auto; margin-right: auto;"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h1&gt;Releases, releases, releases...&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="http://janstey.blogspot.com/2019/11/apache-camel-3-is-out.html" rel="nofollow"&gt;Jon Anstey's Blog: Apache Camel 3 is out!! &lt;/a&gt;&lt;/li&gt;&lt;li&gt;Check out &lt;a class="jive-link-external-small" href="http://www.davsclaus.com/2019/12/apache-camel-3-whats-new-top-10.html" rel="nofollow"&gt;this article&lt;/a&gt; to know more about the main new features&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://infinispan.org/blog/2019/12/09/infinispan-10/" rel="nofollow"&gt;Infinispan 10.1.0.CR1 - Infinispan&lt;/a&gt; along with&lt;ul&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://infinispan.org/blog/2019/11/25/infinispan-operator-1/" rel="nofollow"&gt;Infinispan Operator 1.0.1 - Infinispan&lt;/a&gt; and&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://infinispan.org/blog/2019/12/02/image/" rel="nofollow"&gt;Infinispan's new image - Infinispan&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="http://jbossts.blogspot.com/2019/12/narayana-5101final-released.html" rel="nofollow"&gt;Narayana team blog: Narayana 5.10.1.Final released&lt;/a&gt; &lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://www.keycloak.org/2019/12/keycloak-801-released.html" rel="nofollow"&gt;Keycloak - Blog - Keycloak 8.0.1 released&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;em&gt;That's all for another edition of the JBoss Editorial, please join us again for more exciting development from the JBoss Communities.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:5ee2f36e-7030-4c29-b663-04f6ff4b06a9] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/a5ToOBWtoac" height="1" width="1" alt=""/&gt;</content><summary>There've been some noteworthy releases in the last two weeks such as Infinispan 10.1.0.CR1, Keycloak 8.0.1 and, of course, Camel 3.0 !  Just taking a look at all the new cool features coming with those should already keep you busy! But if it’s not enough, don’t worry, the rest of the JBoss community has you covered! Pimp your tooling   Developers like sysadmins can only accomplish their work prope...</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2019-12-10T14:49:26Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2019/12/10/this-week-in-jboss-10th-december-2019-camel-3-infinispan-101-keycloak-8-a-bucketload-of-releases</feedburner:origLink></entry><entry><title>LoRaWAN setup at the EclipseCon IoT playground</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-e9rSu8APmA/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="Internet of Things" scheme="searchisko:content:tags" /><category term="IoT solutions" scheme="searchisko:content:tags" /><category term="LoRaWAN" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><author><name>Jens Reimann</name></author><id>searchisko:content:id:jbossorg_blog-lorawan_setup_at_the_eclipsecon_iot_playground</id><updated>2019-12-10T08:00:41Z</updated><published>2019-12-10T08:00:41Z</published><content type="html">&lt;p&gt;At the recent &lt;a href="https://www.eclipsecon.org/europe2019" target="_blank" rel="noopener noreferrer"&gt;EclipseCon Europe&lt;/a&gt; in Ludwigsburg, Germany, we had a big dashboard in the IoT playground area showing graphs of the number of WiFi devices, the temperature, and air quality, all transmitted via &lt;a href="https://lora-alliance.org/about-lorawan" target="_blank" rel="noopener noreferrer"&gt;LoRaWAN&lt;/a&gt;. We worked on this project during the community day and kept the setup throughout the conference, where we showed it and played with it even further. This article describes the architecture of the setup and gives pointers to replicate it.&lt;/p&gt; &lt;p&gt;&lt;span id="more-653217"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;The sensors&lt;/h2&gt; &lt;p&gt;We chose this &lt;a href="https://github.com/cyberman54/ESP32-Paxcounter" target="_blank" rel="noopener noreferrer"&gt;PAX Counter&lt;/a&gt; as our sensor device. Based on the ESP32 and LoRaWAN, it allows you to measure the number of unique WiFi clients in the area. The PAX counter, as the name suggests, only counts devices. It doesn&amp;#8217;t track them, and this functionality is exactly what we wanted. Also, you can add extra sensors, and the BME680 air quality sensor is supported right out of the box. While it is great to have open source firmware, having a ready-to-run experience is great as well. Fortunately, this PAX counter gave us both.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-653367 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pax-counter-1024x602.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pax-counter-1024x602.jpg" alt="Photo of LoRaWAN PAX Counter" width="640" height="376" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pax-counter-1024x602.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pax-counter-300x176.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pax-counter-768x452.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;h2&gt;The cloud side&lt;/h2&gt; &lt;p&gt;To have good LoRa coverage, we deployed two &lt;a href="https://www.thethingsnetwork.org/docs/gateways/thethingsindoor/"&gt;Things Network Indoor&lt;/a&gt; gateways. Those devices are rather cheap and easy to set up, and two of them were fine to cover the whole venue. Our initial goal was to provide the data visualization in a simple Grafana dashboard so that people could get a feeling of what we deployed.&lt;/p&gt; &lt;p&gt;Our back-end architecture made use of Eclipse Hono, the Qpid Dispatch Router, and Apache Kafka, and looked like this:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-653237 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlaygroundOverview-1920px-1024x809.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlaygroundOverview-1920px-1024x809.png" alt="Architectural Diagram of the IoT Playground" width="640" height="506" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlaygroundOverview-1920px-1024x809.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlaygroundOverview-1920px-300x237.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlaygroundOverview-1920px-768x607.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;Forwarding telemetry data from The Things Network to a Grafana dashboard shouldn&amp;#8217;t require that many components. However, we had a little more in mind than just showing the data on a dashboard.&lt;/p&gt; &lt;h2&gt;Ready for more than LoRaWAN&lt;/h2&gt; &lt;p&gt;We teamed up with people from &lt;a href="https://developers.redhat.com/videos/youtube/GKYROutwJHU/"&gt;Eclipse MicroProfile&lt;/a&gt;, offering a connection to the data stream as well. For that, &lt;a href="https://developers.redhat.com/videos/youtube/CZhOJ_ysIiI/"&gt;Apache Kafka&lt;/a&gt; seemed to be the right choice. Kafka can persist the data stream and allow you to connect to it at a later time, restarting to consume from the beginning while the already existing dashboard would continue to receive data without any change or interruption. We also could add as many users as we liked, consuming the data like all the others.&lt;/p&gt; &lt;p&gt;We didn&amp;#8217;t want to limit ourselves to this single sensor, and only to LoRaWAN or The Things Network. This is where Eclipse Hono comes into play. It has the ability to normalize different IoT protocols, like the LoRaWAN uplink, or MQTT and Sigfox into AMQP 1.0. Using this tool gave us the ability to plug in any other data provider without the rest of the data pipeline noticing.&lt;/p&gt; &lt;h2&gt;Completing the setup&lt;/h2&gt; &lt;p&gt;Of course, there is the issue of data formats. Both Eclipse Hono and Kafka are payload agnostic. Unfortunately, the sensors have their own data format, and InfluxDB has its custom API. Bringing both together isn&amp;#8217;t too difficult, but it&amp;#8217;s simpler when using Apache Camel. A few lines of XML or Java, and you have two running bridges, one forwarding raw IoT data from the EnMasse messaging address to the Kafka topic. The other one, for parsing the payload, and injects it into the InfluxDB.&lt;/p&gt; &lt;p&gt;Why did we decide to parse the payload in the second step? Kafka makes it so easy to store each and every message coming from the IoT layer. The sensors provided more data than we wanted to insert into the time series database. By handling things this way, we still had the raw values available in the Kafka cluster, ready for everyone else to consume them if necessary.&lt;/p&gt; &lt;p&gt;After deploying all of the sensors, it was awesome to see this visualization come to life:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-653307 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlayground-Screenshot-Dashboard-1024x456.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlayground-Screenshot-Dashboard-1024x456.png" alt="Devices overview dashboard" width="640" height="285" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlayground-Screenshot-Dashboard-1024x456.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlayground-Screenshot-Dashboard-300x134.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/IoTPlayground-Screenshot-Dashboard-768x342.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;The big dashboard showed each sensor’s information with the number of WiFi devices, the current temperature, and an air quality index. By default, you could see the last three hours.&lt;/p&gt; &lt;h2&gt;Playing with Microprofile&lt;/h2&gt; &lt;p&gt;Of course, it would be great to do something more interesting with the data than just having a nice dashboard. That is what we started to play with during the community day by connecting a &lt;a href="https://quarkus.io/" target="_blank" rel="noopener noreferrer"&gt;Quarkus&lt;/a&gt; application to the Kafka data stream and beginning to process it. Unfortunately, setting up proper TLS and authentication took longer than anticipated, so we ran out of time.&lt;/p&gt; &lt;p&gt;However, that doesn’t stop us from continuing. The repository in the next section will definitely see Quarkus examples related to IoT.&lt;/p&gt; &lt;h2&gt;Try it for yourself&lt;/h2&gt; &lt;p&gt;Deploying this solution was actually rather easy, and it should be easy enough for you to replicate. After all, we reused existing components like EnMasse and Strimzi to deploy Hono and Kafka. Of course, you can do the same with &lt;a href="https://developers.redhat.com/blog/2019/05/14/bringing-iot-to-red-hat-amq-online/"&gt;Red Hat AMQ Online&lt;/a&gt; and &lt;a href="https://access.redhat.com/products/red-hat-amq" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams&lt;/a&gt;, as the LoRaWAN adapter is part of Red Hat AMQ Online 1.3&amp;#8217;s IoT tech preview.&lt;/p&gt; &lt;p&gt;You can find the deployment scripts in the GitHub repository &lt;a href="https://github.com/ctron/ece2019-iot-playground/" target="_blank" rel="noopener noreferrer"&gt;ctron/ece2019-iot-playground&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#38;linkname=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F10%2Florawan-setup-at-the-eclipsecon-iot-playground%2F&amp;#038;title=LoRaWAN%20setup%20at%20the%20EclipseCon%20IoT%20playground" data-a2a-url="https://developers.redhat.com/blog/2019/12/10/lorawan-setup-at-the-eclipsecon-iot-playground/" data-a2a-title="LoRaWAN setup at the EclipseCon IoT playground"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/10/lorawan-setup-at-the-eclipsecon-iot-playground/"&gt;LoRaWAN setup at the EclipseCon IoT playground&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-e9rSu8APmA" height="1" width="1" alt=""/&gt;</content><summary>At the recent EclipseCon Europe in Ludwigsburg, Germany, we had a big dashboard in the IoT playground area showing graphs of the number of WiFi devices, the temperature, and air quality, all transmitted via LoRaWAN. We worked on this project during the community day and kept the setup throughout the conference, where we showed it and played with it even further. This article describes the architec...</summary><dc:creator>Jens Reimann</dc:creator><dc:date>2019-12-10T08:00:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/10/lorawan-setup-at-the-eclipsecon-iot-playground/</feedburner:origLink></entry><entry><title>Infinispan 10.1.0.CR1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-pmfPi_HXVs/" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><category term="release candidate" scheme="searchisko:content:tags" /><author><name>Tristan Tarrant</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_10_1_0_cr1</id><updated>2019-12-09T12:55:22Z</updated><published>2019-12-09T12:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Dear Infinispan community,&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;as we are closing in on 10.1, we have been working on a lot of polishing and bugfixing.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_server"&gt;&lt;a class="anchor" href="#_server"&gt;&lt;/a&gt;Server&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The new console has received a lot of improvements,&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A new welcome page&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A command-line switch to specify an alternate logging configuration file&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_query"&gt;&lt;a class="anchor" href="#_query"&gt;&lt;/a&gt;Query&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The query components have been reorganized so that they are more modular.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_monitoring"&gt;&lt;a class="anchor" href="#_monitoring"&gt;&lt;/a&gt;Monitoring&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The introduction of histogram and timer metrics.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_stores"&gt;&lt;a class="anchor" href="#_stores"&gt;&lt;/a&gt;Stores&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The REST cache store has been updated to use the v2 RESTful API.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_removals_and_deprecations"&gt;&lt;a class="anchor" href="#_removals_and_deprecations"&gt;&lt;/a&gt;Removals and deprecations&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The old RESTful API (v1) has been removed&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The Infinispan Lucene Directory has been deprecated.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The memcached protocol server has been deprecated. If you were relying on this, come and talk to us about working on a binary protocol implementation.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_bug_fixes_clean_ups_and_documentation"&gt;&lt;a class="anchor" href="#_bug_fixes_clean_ups_and_documentation"&gt;&lt;/a&gt;Bug fixes, clean-ups and documentation&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Over 40 issues fixed including a lot of documentation updates. See the &lt;a href=""&gt;full list of changes and fixes&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_get_it_use_it_ask_us"&gt;&lt;a class="anchor" href="#_get_it_use_it_ask_us"&gt;&lt;/a&gt;Get it, Use it, Ask us!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Please &lt;a href="http://infinispan.org/download/"&gt;download&lt;/a&gt;, &lt;a href="https://issues.jboss.org/projects/ISPN"&gt;report bugs&lt;/a&gt;, &lt;a href="https://infinispan.zulipchat.com/"&gt;chat with us&lt;/a&gt;, ask questions on &lt;a href="https://stackoverflow.com/questions/tagged/?tagnames=infinispan&amp;amp;sort=newest"&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Infinispan 10.1.0.Final is scheduled for December the 20th.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-pmfPi_HXVs" height="1" width="1" alt=""/&gt;</content><summary>Dear Infinispan community, as we are closing in on 10.1, we have been working on a lot of polishing and bugfixing. Server The new console has received a lot of improvements, A new welcome page A command-line switch to specify an alternate logging configuration file Query The query components have been reorganized so that they are more modular. Monitoring The introduction of histogram and timer met...</summary><dc:creator>Tristan Tarrant</dc:creator><dc:date>2019-12-09T12:00:00Z</dc:date><feedburner:origLink>http://infinispan.org/blog/2019/12/09/infinispan-10/</feedburner:origLink></entry><entry><title>CodeReady Workspaces devfile, demystified</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gmuhd6JAhHI/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="Red Hat CodeReady Workspaces" scheme="searchisko:content:tags" /><author><name>Don Schenck</name></author><id>searchisko:content:id:jbossorg_blog-codeready_workspaces_devfile_demystified</id><updated>2019-12-09T08:00:55Z</updated><published>2019-12-09T08:00:55Z</published><content type="html">&lt;p&gt;With the exciting advent of &lt;a href="https://developers.redhat.com/blog/2019/12/03/red-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development/"&gt;CodeReady Workspaces (CRW) 2.0&lt;/a&gt; comes some important changes. Based on the upstream project Eclipse Che 7, CRW brings even more of the &amp;#8220;Infrastructure as Code&amp;#8221; idea to fruition. Workspaces mimic the environment of a PC, an operating system, programming language support, the tools needed, and an editor. The real power comes by defining a workspace using a YAML file—a text file that can be stored and versioned in a source control system such as Git. This file, called &lt;code&gt;devfile.yaml&lt;/code&gt;, is powerful and complex. This article will attempt to demystify the devfile.&lt;/p&gt; &lt;p&gt;&lt;span id="more-640867"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;A Java Maven example&lt;/h2&gt; &lt;p&gt;The following devfile defines a workspace that has &lt;a href="https://developers.redhat.com/developer-tools/java"&gt;Java&lt;/a&gt; language support, includes the Maven build tool, and has two custom commands.&lt;/p&gt; &lt;pre&gt;&lt;img class=" alignnone size-large wp-image-641947 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2-727x1024.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2-727x1024.png" alt="" width="640" height="901" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2-727x1024.png 727w, https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2-213x300.png 213w, https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2-768x1082.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/10/donsBlogImage2.png 878w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s break this down.&lt;/p&gt; &lt;h2&gt;Section 1: metadata&lt;/h2&gt; &lt;p&gt;This section is required, and in this case &lt;code&gt;name&lt;/code&gt; is specified as the name of this workspace (wksp-javamaven). You also have the option of a name prefix, in which case the system will generate the rest of the name (e.g., &lt;code&gt;generateName: javamaven-&lt;/code&gt;). Using &lt;code&gt;generateName&lt;/code&gt; means a user can have multiple instances of this workspace at the same time. This is an important distinction and one which must be addressed by management.&lt;/p&gt; &lt;h2&gt;Section 2: projects&lt;/h2&gt; &lt;p&gt;This section is optional (but probably a very good idea) and tells the workspace where to find the project that will be included. In this case, we&amp;#8217;re pointing to the master branch of a Git repository located on GitHub. You can specify multiple projects for a workspace, and the type can be a zip file as well. While the name of the project does not need to match the name of the repo it&amp;#8217;s, again, probably a very good idea. If you make them different, well &amp;#8230; then that&amp;#8217;s a create opportunity to add confusion and really mess things up.&lt;/p&gt; &lt;h2&gt;Section 3: attributes&lt;/h2&gt; &lt;p&gt;This part is optional and can pretty much define anything you wish. In this example, it&amp;#8217;s specifying that any values stored in any specified volumes are &lt;em&gt;not&lt;/em&gt; stored. This will likely be the value you&amp;#8217;ll always want. The idea is that, unless you commit your changes to the Git repo, any work done will be lost. Think of it this way: Whereas on your local PC you perform a &lt;em&gt;File &amp;#8211;&amp;#62; Save&lt;/em&gt; command to keep your work, you&amp;#8217;ll instead do &lt;code&gt;git commit&lt;/code&gt;. In &amp;#8220;devfile-speak,&amp;#8221; &lt;code&gt;persistVolumes: false&lt;/code&gt; makes the data ephemeral. This setting, &lt;code&gt;false&lt;/code&gt;, also makes the workspace perform better.&lt;/p&gt; &lt;h2&gt;Section 4: components&lt;/h2&gt; &lt;p&gt;This is the heaviest part of this example, where we specify what bits and pieces make up our workspace.&lt;/p&gt; &lt;p&gt;The first component is a Che Plugin, identified at &lt;code&gt;redhat/java/latest&lt;/code&gt;. You can see the description of this plug on &lt;a href="https://github.com/eclipse/che-plugin-registry/blob/master/v3/plugins/redhat/java/latest/meta.yaml"&gt;this GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The next component is a type &lt;code&gt;dockerimage&lt;/code&gt; that is the maven support for this workspace. Of special note is the setting &lt;code&gt;mountSources: true&lt;/code&gt;, which makes the source code available to the container that is running this image. In this particular case, we want our Maven build to have access to the source code—which makes sense.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;volumes:&lt;/code&gt; setting defines a directory within the container that is available to this workspace. This is used, for example, to give the workspace access to a needed directory that would otherwise be outside the container and blocked by lack of permissions. In other words, if you run a command in a workspace and get an error message because you are denied access to a directory, you can get around that by defining that directory here, in your devfile, that will be created &lt;em&gt;inside your container&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The remaining settings associated with this component are related to memory limits, security, etc.&lt;/p&gt; &lt;h2&gt;Section 5: apiVersion&lt;/h2&gt; &lt;p&gt;This section is required and is how you specify which API version you are using. This is pretty much boilerplate text.&lt;/p&gt; &lt;h2&gt;Section 6: commands&lt;/h2&gt; &lt;p&gt;This is the fun part. In this section, you can define custom commands that are available to the user. Typically, this is where you&amp;#8217;ll specify command-line commands that can be run from within the IDE rather than dropping to the command line and typing what may be a lengthy command. The properties here will look pretty much self-explanatory. Note that a macro can be used instead of hard-coded value for the project root directory (e.g., &lt;code&gt;${CHE_PROJECTS_ROOT}&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Start exploring&lt;/h2&gt; &lt;p&gt;There are many settings and variations of devfiles. If you have the ability, I suggest going into your CRW 2.0 instance and exploring any existing workspaces&amp;#8217; devfiles. Take an existing devfile, clone it, then change it and implement it to see what happens. Like any good developer, make changes until you break things, then gain understanding.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#38;linkname=CodeReady%20Workspaces%20devfile%2C%20demystified" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fcodeready-workspaces-devfile-demystified%2F&amp;#038;title=CodeReady%20Workspaces%20devfile%2C%20demystified" data-a2a-url="https://developers.redhat.com/blog/2019/12/09/codeready-workspaces-devfile-demystified/" data-a2a-title="CodeReady Workspaces devfile, demystified"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/09/codeready-workspaces-devfile-demystified/"&gt;CodeReady Workspaces devfile, demystified&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gmuhd6JAhHI" height="1" width="1" alt=""/&gt;</content><summary>With the exciting advent of CodeReady Workspaces (CRW) 2.0 comes some important changes. Based on the upstream project Eclipse Che 7, CRW brings even more of the “Infrastructure as Code” idea to fruition. Workspaces mimic the environment of a PC, an operating system, programming language support, the tools needed, and an editor. The real power comes by defining a workspace using a YAML file—a text...</summary><dc:creator>Don Schenck</dc:creator><dc:date>2019-12-09T08:00:55Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/09/codeready-workspaces-devfile-demystified/</feedburner:origLink></entry><entry><title>Building freely distributed containers with Podman and Red Hat UBI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7ZiSI5Cg_w8/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="devnation" scheme="searchisko:content:tags" /><category term="events" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Podman" scheme="searchisko:content:tags" /><category term="Universal Base Images (UBI)" scheme="searchisko:content:tags" /><author><name>Scott McCarty (fatherlinux)</name></author><id>searchisko:content:id:jbossorg_blog-building_freely_distributed_containers_with_podman_and_red_hat_ubi</id><updated>2019-12-09T08:00:31Z</updated><published>2019-12-09T08:00:31Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/devnation/"&gt;DevNation tech talks&lt;/a&gt; are hosted by the Red Hat technologists who create our products. These sessions include real solutions and code and sample projects to help you get started. In this talk, you’ll learn about building containers with &lt;a href="https://developers.redhat.com/blog/2019/02/21/podman-and-buildah-for-docker-users/"&gt;Podman&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2019/10/09/what-is-red-hat-universal-base-image/"&gt;Red Hat Universal Base Image (UBI)&lt;/a&gt; from &lt;a href="https://developers.redhat.com/blog/author/fatherlinux/"&gt;Scott McCarty&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/author/burrsutter/"&gt;Burr Sutter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We will cover how to build and run &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt;s based on UBI using just your regular user account—no daemon, no root, no fuss. Finally, we will order the de-resolution of all of our containers with a really cool command. After this talk, you will have new tools at the ready to help you find, run, build, and share container images.&lt;span id="more-657947"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Watch the entire presentation:&lt;br /&gt; &lt;iframe src="https://www.youtube.com/embed/Qcys7fKSzB0" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h3&gt;Learn more&lt;/h3&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Join us at an upcoming&lt;/span&gt;&lt;a href="https://developers.redhat.com/events/"&gt; &lt;span style="font-weight: 400;"&gt;developer event&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;, and see our collection of&lt;/span&gt;&lt;a href="https://developers.redhat.com/devnation/?page=0"&gt; &lt;span style="font-weight: 400;"&gt;past DevNation Live tech talks&lt;/span&gt;&lt;/a&gt;&lt;a href="https://developers.redhat.com/events/"&gt;&lt;span style="font-weight: 400;"&gt;.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#38;linkname=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F09%2Fbuilding-freely-distributed-containers-with-podman-and-red-hat-ubi%2F&amp;#038;title=Building%20freely%20distributed%20containers%20with%20Podman%20and%20Red%20Hat%20UBI" data-a2a-url="https://developers.redhat.com/blog/2019/12/09/building-freely-distributed-containers-with-podman-and-red-hat-ubi/" data-a2a-title="Building freely distributed containers with Podman and Red Hat UBI"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/09/building-freely-distributed-containers-with-podman-and-red-hat-ubi/"&gt;Building freely distributed containers with Podman and Red Hat UBI&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7ZiSI5Cg_w8" height="1" width="1" alt=""/&gt;</content><summary>DevNation tech talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions and code and sample projects to help you get started. In this talk, you’ll learn about building containers with Podman and Red Hat Universal Base Image (UBI) from Scott McCarty and Burr Sutter. We will cover how to build and run containers based on UBI using just your regular...</summary><dc:creator>Scott McCarty (fatherlinux)</dc:creator><dc:date>2019-12-09T08:00:31Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/09/building-freely-distributed-containers-with-podman-and-red-hat-ubi/</feedburner:origLink></entry><entry><title>Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/j3QGjMa_oUQ/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat AMQ Streams" scheme="searchisko:content:tags" /><category term="Red Hat Integration" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Pramod Padmanabhan</name></author><id>searchisko:content:id:jbossorg_blog-understanding_red_hat_amq_streams_components_for_openshift_and_kubernetes_part_3</id><updated>2019-12-06T08:00:14Z</updated><published>2019-12-06T08:00:14Z</published><content type="html">&lt;p&gt;In the previous articles in this series, we first covered the &lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-1/"&gt;basics of Red Hat AMQ Streams on OpenShift&lt;/a&gt; and then showed &lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-2/"&gt;how to set up Kafka Connect, a Kafka Bridge, and Kafka Mirror Maker.&lt;/a&gt; Here are a few key points to keep in mind before we proceed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Streams is based on Apache Kafka.&lt;/li&gt; &lt;li&gt;AMQ Streams for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift_container_platform/" target="_blank" rel="noopener noreferrer"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; is based on the Strimzi project.&lt;/li&gt; &lt;li&gt;AMQ Streams on containers has multiple components, such as the Cluster Operator, Entity Operator, Mirror Maker, Kafka connect, and Kafka Bridge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now that we have everything set up (or so we think), let&amp;#8217;s look at monitoring and alerting for our new environment.&lt;span id="more-652107"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Kafka Exporter&lt;/h2&gt; &lt;p&gt;A Kafka cluster, by default, does not export all of its metrics. Hence, we need to use Kafka Exporter to collect the cluster&amp;#8217;s broker state, usage, and performance. It is important to have more insight so you can understand if the consumer&amp;#8217;s message consumption is at the same rate as the producer&amp;#8217;s message pushes. If not, this slow consumption behavior could cost the system. Catching these issues as early as possible is recommended.&lt;/p&gt; &lt;p&gt;To set up Kafka Exporter, begin by editing the existing Kafka cluster config, or creating a new one to include the Kafka Exporter. For example:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: simple-cluster spec: kafka: version: 2.3.0 replicas: 5 listeners: plain: {} tls: {} config: offsets.topic.replication.factor: 5 transaction.state.log.replication.factor: 5 transaction.state.log.min.isr: 2 log.message.format.version: "2.3" storage: type: jbod volumes: - id: 0 type: persistent-claim size: 5Gi deleteClaim: false zookeeper: replicas: 3 storage: type: persistent-claim size: 5Gi deleteClaim: false entityOperator: topicOperator: {} userOperator: {} kafkaExporter: {} &lt;/pre&gt; &lt;p&gt;Next, apply the new changes to the existing cluster:&lt;/p&gt; &lt;pre&gt;$ oc apply -f amq-kafka-cluster-kafka-exporter.yml&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 1:&lt;/p&gt; &lt;div id="attachment_653737" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef.png"&gt;&lt;img aria-describedby="caption-attachment-653737" class="wp-image-653737" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef-300x27.png" alt="Kafka Exporter is deployed." width="500" height="45" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef-300x27.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef.png 707w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653737" class="wp-caption-text"&gt;Figure 1: Your new Kafka Exporter instance.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Prometheus and Grafana&lt;/h2&gt; &lt;p&gt;&lt;a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener noreferrer"&gt;Prometheus&lt;/a&gt; is a system monitoring and alerting toolkit that scrapes metrics from the Kafka cluster. The downside of this tool is that it does not have a good GUI. Hence, we create operational dashboards using &lt;a href="https://grafana.com/oss/grafana/" target="_blank" rel="noopener noreferrer"&gt;Grafana&lt;/a&gt; for the interface and Prometheus for the data feeds.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s get started with the metrics and creating the dashboard:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Add Kafka metrics to the Kafka resource. This snippet was referenced from the &lt;code&gt;examples/metrics/kafka-metrics.yaml&lt;/code&gt; file provided as part of the Red Hat AMQ Streams product:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams $ oc edit kafka simple-cluster # Add the below in the spec-&amp;#62;kafka metrics: # Inspired by config from Kafka 2.0.0 example rules: # https://github.com/prometheus/jmx_exporter/blob/master/example_configs/kafka-2_0_0.yml lowercaseOutputName: true rules: # Special cases and very specific rules - pattern : kafka.server&amp;#60;type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_server_$1_$2 type: GAUGE labels: clientId: "$3" topic: "$4" partition: "$5" - pattern : kafka.server&amp;#60;type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_server_$1_$2 type: GAUGE labels: clientId: "$3" broker: "$4:$5" # Some percent metrics use MeanRate attribute # Ex) kafka.server&amp;#60;type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)&amp;#62;&amp;#60;&amp;#62;MeanRate - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*&amp;#62;&amp;#60;&amp;#62;MeanRate name: kafka_$1_$2_$3_percent type: GAUGE # Generic gauges for percents - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3_percent type: GAUGE - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*, (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3_percent type: GAUGE labels: "$4": "$5" # Generic per-second counters with 0-2 key/value pairs - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*, (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER # Generic gauges with 0-2 key/value pairs - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's. # Note that these are missing the '_sum' metric! - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" "$6": "$7" quantile: "0.$8" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.*)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" quantile: "0.$6" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: quantile: "0.$4" &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;This process will restart the &lt;code&gt;simple-cluster-kafka&lt;/code&gt; pods one by one, as shown in Figure 2:&lt;/p&gt; &lt;div id="attachment_654297" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4.png"&gt;&lt;img aria-describedby="caption-attachment-654297" class="wp-image-654297" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4-300x106.png" alt="Restarting the pods." width="500" height="177" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4-300x106.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4.png 674w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654297" class="wp-caption-text"&gt;Figure 2: Restarting the pods.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;Confirm you have Prometheus running in the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get pod -n openshift-monitoring | grep prometheus prometheus-k8s-0 4/4 Running 140 3d prometheus-k8s-1 4/4 Running 140 3d prometheus-operator-687784bd4b-56vsk 1/1 Running 127 3d&lt;/pre&gt; &lt;p&gt;In case the above does not return any Prometheus pods, check with your infrastructure team to know where they are installed. If they are not installed, then check the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift/assembly-metrics-setup-str#assembly-metrics-prometheus-str" target="_blank" rel="noopener noreferrer"&gt;Prometheus installation steps from the docs&lt;/a&gt;.&lt;/p&gt; &lt;ol start="3"&gt; &lt;li&gt;Install Grafana:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc create -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/grafana/grafana.yaml&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You can see the results in Figure 3:&lt;/p&gt; &lt;div id="attachment_654307" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144.png"&gt;&lt;img aria-describedby="caption-attachment-654307" class="wp-image-654307" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144-300x36.png" alt="Graphana is deployed." width="500" height="60" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144-300x36.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144.png 704w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654307" class="wp-caption-text"&gt;Figure 3: Grafana is now deployed.&lt;/p&gt;&lt;/div&gt; &lt;ol start="4"&gt; &lt;li&gt;Create a route for the Grafana service:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc expose svc grafana --name=grafana-route&lt;/pre&gt; &lt;ol start="5"&gt; &lt;li&gt;Log into the admin console (shown in Figure 4):&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get route | grep grafana grafana-route grafana-route-amq-streams.apps.redhat.demo.com grafana&lt;/pre&gt; &lt;div id="attachment_654317" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910.png"&gt;&lt;img aria-describedby="caption-attachment-654317" class="wp-image-654317" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910-300x130.png" alt="The Grafana admin window." width="500" height="216" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910.png 762w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654317" class="wp-caption-text"&gt;Figure 4: The Grafana admin window.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The default credentials are &lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;When you log in for the first time, Grafana will ask you to change the password.&lt;/p&gt; &lt;ol start="6"&gt; &lt;li&gt;Add Prometheus as a data source. Go to the settings icon and select &lt;em&gt;Data Sources&lt;/em&gt;, and then &lt;em&gt;Prometheus,&lt;/em&gt; as shown in Figure 5:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654337" style="width: 217px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png"&gt;&lt;img aria-describedby="caption-attachment-654337" class="wp-image-654337 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png" alt="Graphana settings." width="207" height="205" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png 207w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd-150x150.png 150w" sizes="(max-width: 207px) 100vw, 207px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654337" class="wp-caption-text"&gt;Figure 5: The Graphana settings menu.&lt;/p&gt;&lt;/div&gt; &lt;ol start="7"&gt; &lt;li&gt;Enter the Prometheus details and save them:&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;URL:&lt;/strong&gt; https://prometheus-k8s.openshift-monitoring.svc:9091&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Basic Auth:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip TLS Verify:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User:&lt;/strong&gt; Internal&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pass:&lt;/strong&gt; Get these details from the cluster admin.&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;The message &amp;#8220;Data source is working&amp;#8221; should appear at the bottom before you continue, as shown in Figure 6:&lt;/p&gt; &lt;div id="attachment_654347" style="width: 310px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277.png"&gt;&lt;img aria-describedby="caption-attachment-654347" class="wp-image-654347" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277-253x300.png" alt="Configuring Graphana." width="300" height="356" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277-253x300.png 253w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277.png 766w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654347" class="wp-caption-text"&gt;Figure 6: Ready to proceed with the new configuration.&lt;/p&gt;&lt;/div&gt; &lt;ol start="8"&gt; &lt;li&gt;Open the &lt;em&gt;Import&lt;/em&gt; options by selecting &lt;strong&gt;+&lt;/strong&gt; and then &lt;em&gt;Import&lt;/em&gt;, as shown in Figure 7:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654327" style="width: 229px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde5947757f.png"&gt;&lt;img aria-describedby="caption-attachment-654327" class="wp-image-654327 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde5947757f.png" alt="Open the import menu to import a sample dashboard." width="219" height="241" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654327" class="wp-caption-text"&gt;Figure 7: Select + and then Import to access the Import options dialog box.&lt;/p&gt;&lt;/div&gt; &lt;ol start="9"&gt; &lt;li&gt;Click &lt;em&gt;Upload .json file&lt;/em&gt; to add the&lt;code&gt; strimzi-kafka.json&lt;/code&gt;file from &lt;code&gt;examples/metrics/grafana-dashboards/strimzi-kafka.json&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Select &lt;em&gt;Prometheus &lt;/em&gt;as a data source and then click &lt;em&gt;Import:&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654357" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351.png"&gt;&lt;img aria-describedby="caption-attachment-654357" class="wp-image-654357" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-300x106.png" alt="Importing the sample dashboard." width="500" height="177" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-300x106.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-768x272.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351.png 871w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654357" class="wp-caption-text"&gt;Figure 8: Importing the sample dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;Doing this should result in the sample Grafana dashboard.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored tools that can help us monitor Red Hat AMQ Streams. This piece fully rounds out our series, where we covered:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre%E2%80%A6ubernetes-part-1/"&gt;Zookeeper, Kafka, and Entity Operator creation.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre%E2%80%A6ubernetes-part-2/"&gt;Kafka Connect, Kafka Bridge, and Mirror Maker.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Monitoring and admin (this article).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hopefully, you now have a better understanding of how to run AMQ Streams in a container ecosystem. Work through our examples and see the results for yourself.&lt;/p&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confluent.io/current/connect/index.html" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index" target="_blank" rel="noopener noreferrer"&gt;Using AMQ Streams on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/2019/10/14/improving-prometheus-metrics.html" target="_blank" rel="noopener noreferrer"&gt;Improving Prometheus metrics&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#038;title=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" data-a2a-url="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/" data-a2a-title="Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/j3QGjMa_oUQ" height="1" width="1" alt=""/&gt;</content><summary>In the previous articles in this series, we first covered the basics of Red Hat AMQ Streams on OpenShift and then showed how to set up Kafka Connect, a Kafka Bridge, and Kafka Mirror Maker. Here are a few key points to keep in mind before we proceed: AMQ Streams is based on Apache Kafka. AMQ Streams for the Red Hat OpenShift Container Platform is based on the Strimzi project. AMQ Streams on contai...</summary><dc:creator>Pramod Padmanabhan</dc:creator><dc:date>2019-12-06T08:00:14Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/</feedburner:origLink></entry><entry><title>Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KOJekK8uNAg/" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="OpenShift VSTS extension" scheme="searchisko:content:tags" /><author><name>luca stocchi</name></author><id>searchisko:content:id:jbossorg_blog-introduction_to_the_red_hat_openshift_deployment_extension_for_microsoft_azure_devops</id><updated>2019-12-05T08:00:11Z</updated><published>2019-12-05T08:00:11Z</published><content type="html">&lt;p&gt;We are extremely pleased to present the new version of the &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; deployment extension (OpenShift VSTS) 1.4.0 for Microsoft Azure DevOps. This extension enables users to deploy their applications to any OpenShift cluster directly from their Microsoft Azure DevOps account. In this article, we will look at how to install and use this extension as part of a YAML-defined pipeline with both Microsoft-hosted and self-hosted agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The OpenShift VSTS extension can be downloaded directly from the marketplace at this &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article offers a demonstration where we explain how easy it is to set up everything and start working with the extension. Look at the &lt;a href="https://github.com/redhat-developer/openshift-vsts/blob/master/docs/getting-started.md" target="_blank" rel="noopener noreferrer"&gt;README file&lt;/a&gt; for further installation and usage information.&lt;span id="more-650877"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;iframe src="https://www.youtube.com/embed/RBwpedmkvow" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;The benefits&lt;/h2&gt; &lt;p&gt;The new OpenShift VSTS 1.4.0 extension has three major benefits:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It allows users to use an &lt;code&gt;oc&lt;/code&gt; CLI already installed on their machine when using a local agent.&lt;/li&gt; &lt;li&gt;It supports and automatically downloads &lt;code&gt;oc&lt;/code&gt; versions greater than four.&lt;/li&gt; &lt;li&gt;It changes the way the &lt;code&gt;oc&lt;/code&gt; CLI is downloaded: No more &amp;#8220;API rate limit exceeded&amp;#8221; error from the GitHub REST API.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Installing the OpenShift VSTS extension&lt;/h2&gt; &lt;p&gt;Before to start using the OpenShift VSTS extension, you first need a running OpenShift instance. In our demo video, we use OpenShift Online, which is hosted and managed by Red Hat. You can &lt;a href="https://www.openshift.com/trial/" target="_blank" rel="noopener noreferrer"&gt;sign up here&lt;/a&gt; and start using OpenShift in the cloud for free.&lt;/p&gt; &lt;p&gt;You also need a Microsoft Azure DevOps account. Once you log into this account, you should see a list of your organizations on the left, and all projects related to your organization on the right. If you do not have any projects, it is time to add a new one. To do so, follow these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clicking on &lt;em&gt;New Project&lt;/em&gt; and fill in the required fields, as shown in Figure 1:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651117" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651117" class="wp-image-651117 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-1024x409.png" alt="" width="640" height="256" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-300x120.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-768x307.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-651117" class="wp-caption-text"&gt;Figure 1: Creating a new Microsoft Azure DevOps project.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;Go to &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;Get it free.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Select your Azure DevOps organization and click &lt;em&gt;Install&lt;/em&gt;. Once this process finishes, the OpenShift VSTS extension install is complete, and you can start setting up your account.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Connecting to your OpenShift cluster&lt;/h2&gt; &lt;p&gt;Now, you need to configure the OpenShift service connection, which connects Microsoft Azure DevOps to your OpenShift cluster:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log into your Azure DevOps project.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;Project Settings&lt;/em&gt; (the cogwheel icon) on the page&amp;#8217;s bottom left.&lt;/li&gt; &lt;li&gt;Select &lt;em&gt;Service Connections&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;New service connection&lt;/em&gt; and search for OpenShift.&lt;/li&gt; &lt;li&gt;Pick the authentication method you would like to use (basic, token, or &lt;code&gt;kubeconfig&lt;/code&gt;). See the details for each option in the next few sections.&lt;/li&gt; &lt;li&gt;Insert your own OpenShift cluster data.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Congratulations! You have connected your Azure DevOps account to your OpenShift cluster.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s look at how to set up each authentication method.&lt;/p&gt; &lt;h3&gt;Basic authentication&lt;/h3&gt; &lt;p&gt;When you select &lt;em&gt;Basic Authentication&lt;/em&gt;, use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; The OpenShift username for this instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; The password for the specified user.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accept untrusted SSL certificates:&lt;/strong&gt; Whether it is ok to accept self-signed (untrusted) certificates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 2:&lt;/p&gt; &lt;div id="attachment_651127" style="width: 638px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651127" class="wp-image-651127 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form.png" alt="" width="628" height="457" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form.png 628w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form-300x218.png 300w" sizes="(max-width: 628px) 100vw, 628px" /&gt;&lt;p id="caption-attachment-651127" class="wp-caption-text"&gt;Figure 2: Using basic authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Token authentication&lt;/h3&gt; &lt;p&gt;When you select &lt;em&gt;Token Based Authentication&lt;/em&gt;, use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accept untrusted SSL certificates:&lt;/strong&gt; Whether it is ok to accept self-signed (untrusted) certificates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Token:&lt;/strong&gt; The API token used for authentication.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 3:&lt;/p&gt; &lt;div id="attachment_651187" style="width: 645px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651187" class="wp-image-651187 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form.png" alt="" width="635" height="422" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form.png 635w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form-300x199.png 300w" sizes="(max-width: 635px) 100vw, 635px" /&gt;&lt;p id="caption-attachment-651187" class="wp-caption-text"&gt;Figure 3: Using token authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Kubeconfig&lt;/h3&gt; &lt;p&gt;To use &lt;code&gt;kubeconfig&lt;/code&gt;-based authentication, select &lt;em&gt;No Authentication &lt;/em&gt;and use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kubeconfig:&lt;/strong&gt; The contents of the &lt;code&gt;kubectl&lt;/code&gt; configuration file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 4:&lt;/p&gt; &lt;div id="attachment_651177" style="width: 646px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651177" class="wp-image-651177 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form.png" alt="" width="636" height="397" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form.png 636w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form-300x187.png 300w" sizes="(max-width: 636px) 100vw, 636px" /&gt;&lt;p id="caption-attachment-651177" class="wp-caption-text"&gt;Figure 4: Using kubeconfig authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Exploring the extension&lt;/h2&gt; &lt;p&gt;Once the extension can authenticate to the Red Hat OpenShift cluster, you are ready to create your own YAML pipeline, and then perform operations in OpenShift by executing &lt;code&gt;oc&lt;/code&gt; commands directly from Azure DevOps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This extension uses the &lt;code&gt;oc&lt;/code&gt; OpenShift client tool to interact with an OpenShift cluster, so a minimal knowledge of this OpenShift CLI tool is required.&lt;/p&gt; &lt;p&gt;The extension offers three different tasks: install and set up &lt;code&gt;oc&lt;/code&gt;, execute a single &lt;code&gt;oc&lt;/code&gt; command, and update the &lt;code&gt;ConfigMap&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Install and set up &lt;code&gt;oc&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This task allows you to install a specific version of the OpenShift CLI (&lt;code&gt;oc&lt;/code&gt;), adds it to your &lt;code&gt;PATH&lt;/code&gt;, and creates a &lt;code&gt;kubeconfig&lt;/code&gt; file for authenticating with the OpenShift cluster. First, we download and set up &lt;code&gt;oc&lt;/code&gt;, and then we execute &lt;code&gt;oc&lt;/code&gt; commands through a script:&lt;/p&gt; &lt;pre&gt;jobs: - job: myjob   displayName: MyJob   pool:     vmImage: 'windows-latest'   steps:   # Install oc so that it can be used within a 'script' or bash 'task'   - task: oc-setup@2     inputs:       openshiftService: 'My Openshift'       version: '3.11.154' # A script task making use of 'oc'   - script: |       oc new-project my-project       oc apply -f ${SYSTEM_DEFAULTWORKINGDIRECTORY}/openshift/config.yaml -n my-project&lt;/pre&gt; &lt;p&gt;The installed &lt;code&gt;oc&lt;/code&gt; binary will match your agent&amp;#8217;s OS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; It is possible to use variables defined in the agent. As seen in this example, to reference a file in &lt;code&gt;artefact _my_sources&lt;/code&gt;, you can use:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;${SYSTEM_DEFAULTWORKINGDIRECTORY}/_my_sources/my-openshift-config.yaml&lt;/pre&gt; &lt;p&gt;You can use this task as follows in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks&lt;/em&gt;, click &lt;em&gt;Install and setup oc&lt;/em&gt;. This action opens the dialog shown in Figure 5:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651167" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651167" class="wp-image-651167" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-300x113.png" alt="" width="500" height="188" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-300x113.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-768x289.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task.png 797w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651167" class="wp-caption-text"&gt;Figure 5: Installing and setting up oc.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection &lt;/em&gt;drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc to use&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Execute single &lt;code&gt;oc&lt;/code&gt; commands&lt;/h3&gt; &lt;p&gt;This task allows you to execute a single &lt;code&gt;oc&lt;/code&gt; command directly from Azure DevOps:&lt;/p&gt; &lt;pre&gt;jobs: - job: myjob   displayName: MyJob   pool:     name: 'Default'   steps:   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       version: '4.1'       cmd: 'oc new-app https://github.com/lstocchi/nodejs-ex -l name=demoapp'       uselocalOc: true&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Neither the &lt;code&gt;oc-cmd&lt;/code&gt; or &lt;code&gt;config-map&lt;/code&gt; tasks need to forcibly run after the setup task. If the extension does not find a valid &lt;code&gt;oc&lt;/code&gt; CLI during the execution of an &lt;code&gt;oc&lt;/code&gt; command, first it downloads a copy of a new &lt;code&gt;oc&lt;/code&gt;, and then it executes the command.&lt;/p&gt; &lt;p&gt;To use this task in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks,&lt;/em&gt; select &lt;em&gt;Execute oc command &lt;/em&gt;to pull up the dialog shown in Figure 6:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651147" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651147" class="wp-image-651147" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task-300x178.png" alt="" width="500" height="297" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task-300x178.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task.png 679w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651147" class="wp-caption-text"&gt;Figure 6: Fill out this dialog to execute an oc command.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection&lt;/em&gt; drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc to use&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Command to run&lt;/em&gt; text box, enter the actual &lt;code&gt;oc&lt;/code&gt; command to run.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can directly type the &lt;code&gt;oc&lt;/code&gt; sub-command by omitting &lt;code&gt;oc&lt;/code&gt; from the input (e.g., &lt;code&gt;rollout latest dc/my-app -n production&lt;/code&gt;).&lt;/p&gt; &lt;ol start="5"&gt; &lt;li&gt;Check or un-check the &lt;em&gt;Ignore non success return value&lt;/em&gt; check box, which specifies whether the &lt;code&gt;oc&lt;/code&gt;command&amp;#8217;s non-success return value has to be ignored (e.g., if a task with the command &lt;code&gt;oc create&lt;/code&gt; or &lt;code&gt;oc delete&lt;/code&gt;fails because the resource has already been created or deleted, the pipeline will continue its execution).&lt;/li&gt; &lt;li&gt;Check or un-check the &lt;em&gt;use local oc executable&lt;/em&gt; check box, which specified whether to force the extension to use, if present, the &lt;code&gt;oc&lt;/code&gt; CLI found on the machine containing the agent. &lt;em&gt;If no version is specified&lt;/em&gt;, the extension uses the local &lt;code&gt;oc&lt;/code&gt; CLI no matter what its version is. &lt;em&gt;If a version is specified&lt;/em&gt;, then the extension checks to see if the &lt;code&gt;oc&lt;/code&gt; CLI installed has the same version requested by the user (if not, the correct &lt;code&gt;oc&lt;/code&gt; CLI will be downloaded).&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Update a &lt;code&gt;ConfigMap&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This task allows you to update the properties of a given &lt;code&gt;ConfigMap&lt;/code&gt; using a grid:&lt;/p&gt; &lt;pre&gt;  jobs: - job: myjob   displayName: MyJob   pool:     name: 'Default' - task: config-map@2      inputs:        openshiftService: 'my_openshift_connection'        configMapName: 'my-config'        namespace: 'my-project'        properties: '-my-key1 my-value1 -my-key2 my-value2'&lt;/pre&gt; &lt;p&gt;It includes six configuration options, which you can fill out in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks&lt;/em&gt;, select &lt;em&gt;Update ConfigMap&lt;/em&gt; to access the dialog shown in Figure 7:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651137" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651137" class="wp-image-651137" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task-300x249.png" alt="" width="500" height="414" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task-300x249.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task.png 683w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651137" class="wp-caption-text"&gt;Figure 7: Updating a ConfigMap.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection&lt;/em&gt; drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Name of the ConfigMap&lt;/em&gt; text box, enter the name of the &lt;code&gt;ConfigMap&lt;/code&gt; to update. (This field is required.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Namespace of ConfigMap&lt;/em&gt; text box, enter the namespace in which to find the &lt;code&gt;ConfigMap&lt;/code&gt;. The current namespace is used if none is specified.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;ConfigMap Properties&lt;/em&gt; text box, enter the properties to set or update. Only the properties which need creating or updating need to be listed. Space-separated values need to be surrounded by quotes (&amp;#8220;).&lt;/li&gt; &lt;li&gt;Check or un-check the &lt;em&gt;use local oc executable&lt;/em&gt; checkbox, which specified whether to force the extension to use, if present, the &lt;code&gt;oc&lt;/code&gt; CLI found on the machine containing the agent. &lt;em&gt;If no version is specified&lt;/em&gt;, the extension uses the local &lt;code&gt;oc&lt;/code&gt; CLI no matter what its version is. &lt;em&gt;If a version is specified&lt;/em&gt;, then the extension checks to see if the &lt;code&gt;oc&lt;/code&gt; CLI installed has the same version requested by the user (if not, the correct &lt;code&gt;oc&lt;/code&gt; CLI will be downloaded).&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Work with OpenShift&lt;/h3&gt; &lt;p&gt;It is finally time to create your YAML pipeline by using the OpenShift VSTS extension. In our example, we have the application &lt;code&gt;nodejs-ex&lt;/code&gt; already running on our OpenShift cluster, and our goal is to create a pipeline to push a new version of our application whenever our GitHub master branch is updated. Here is our task:&lt;/p&gt; &lt;pre&gt;jobs: - job: demo   displayName: MyDemo   pool:     name: 'Default'   steps:   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       cmd: 'oc start-build nodejs-ex --follow'       uselocalOc: true   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       cmd: 'oc status'       uselocalOc: true&lt;/pre&gt; &lt;p&gt;Every time the pipeline is triggered, a new build starts, and our application is pushed to the cluster eventually. It is important to note that because we are using a local agent to run this pipeline (which is on a machine with the &lt;code&gt;oc&lt;/code&gt; CLI already installed, we set the flag &lt;code&gt;uselocalOc&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and did not specify any version. The extension will use the &lt;code&gt;oc&lt;/code&gt; CLI that is installed on the machine, whatever its version is.&lt;/p&gt; &lt;p&gt;Next, we check the status of our cluster to see if there are any misconfigured components (services, deployment configs, build configurations, or active deployments).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to use a specific &lt;code&gt;oc&lt;/code&gt; version, be sure to type it correctly, otherwise the latest release will be used (e.g., if you type &lt;code&gt;v3.5&lt;/code&gt; as your version input, the extension will download version 3.5.5, because 3.5 does not exist in our repo. Check the &lt;a href="https://github.com/redhat-developer/openshift-vsts/blob/master/README.md" target="_blank" rel="noopener noreferrer"&gt;README&lt;/a&gt; file for more information).&lt;/p&gt; &lt;h2&gt;Wrapping up&lt;/h2&gt; &lt;p&gt;At this point, you should be able to set up your OpenShift VSTS extension and use it to create your own YAML-defined pipeline, then deploy your application to your OpenShift cluster from Azure DevOps. OpenShift VSTS is an open source project, and we welcome contributions and suggestions. Please reach out to us if you have any requests for further deployments, ideas to improve the extension, questions, or if you encounter any issues. Contacting us is simple: &lt;a href="https://github.com/redhat-developer/openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;Open a new issue&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#038;title=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" data-a2a-url="https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/" data-a2a-title="Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/"&gt;Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KOJekK8uNAg" height="1" width="1" alt=""/&gt;</content><summary>We are extremely pleased to present the new version of the Red Hat OpenShift deployment extension (OpenShift VSTS) 1.4.0 for Microsoft Azure DevOps. This extension enables users to deploy their applications to any OpenShift cluster directly from their Microsoft Azure DevOps account. In this article, we will look at how to install and use this extension as part of a YAML-defined pipeline with both ...</summary><dc:creator>luca stocchi</dc:creator><dc:date>2019-12-05T08:00:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/</feedburner:origLink></entry><entry><title>Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XV4puC8LLGM/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="event-driven" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat AMQ Streams" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Strimzi" scheme="searchisko:content:tags" /><author><name>Pramod Padmanabhan</name></author><id>searchisko:content:id:jbossorg_blog-understanding_red_hat_amq_streams_components_for_openshift_and_kubernetes_part_2</id><updated>2019-12-05T08:00:07Z</updated><published>2019-12-05T08:00:07Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-1/"&gt;In the previous article in this series&lt;/a&gt;, we discussed the basics of &lt;a href="https://access.redhat.com/products/red-hat-amq" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams&lt;/a&gt; on &lt;a href="http://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Here are a few key points to keep in mind before we proceed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Streams is based on Apache Kafka.&lt;/li&gt; &lt;li&gt;AMQ Streams for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift_container_platform/"&gt;OpenShift Container Platform&lt;/a&gt; is based on the Strimzi project.&lt;/li&gt; &lt;li&gt;AMQ Streams on containers has multiple components, such as the Cluster Operator, Entity Operator, Mirror Maker, Kafka connect, and Kafka Bridge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, let&amp;#8217;s continue on to setting up Kafka Connect, the Kafka Bridge, and Mirror Maker.&lt;/p&gt; &lt;h2&gt;Kafka Connect&lt;/h2&gt; &lt;p&gt;Kafka Connect is mainly used to stream data &lt;em&gt;in&lt;/em&gt; and &lt;em&gt;out&lt;/em&gt; of Kafka clusters; for instance, getting a Twitter feed and then pushing it to the cluster. We need to understand Kafka Connect&amp;#8217;s concepts before continuing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connectors define where the data should be copied &lt;em&gt;to&lt;/em&gt; or &lt;em&gt;from&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Tasks are the actors that actually copy the data.&lt;/li&gt; &lt;li&gt;Workers are used to schedule units of work for &lt;em&gt;connectors&lt;/em&gt; and &lt;em&gt;tasks&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Converters are used by &lt;em&gt;task units&lt;/em&gt; to change data format.&lt;/li&gt; &lt;li&gt;Transforms are used by &lt;em&gt;connector&lt;/em&gt; units to do simple data adjustments, routing, and chain transformations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more details on the basic concepts, I recommend reading &lt;a href="https://docs.confluent.io/current/connect/concepts.html#connect-concepts" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect Concepts&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Creating a simple Kafka Connect instance&lt;/h3&gt; &lt;p&gt;A Kafka Connect instance in OpenShift can be created using two different Kube objects: KafkaConnect and KafkaConnectS2I. By default, Kafka Connect includes two built-in connectors: FileStreamSourceConnector and FileStreamSinkConnector. However, before you build a new connector, first check the &lt;a href="https://www.confluent.io/hub/" target="_blank" rel="noopener noreferrer"&gt;catalog of existing connectors&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us set up a simple Kafka Connect instance and then perform the source and sink operations. Then, we can add a default producer sample app and a consumer sample app. This process will show multiple publishers and multiple consumers, as shown in Figure 1:&lt;/p&gt; &lt;div id="attachment_652237" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0.png"&gt;&lt;img aria-describedby="caption-attachment-652237" class="wp-image-652237" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-300x214.png" alt="The structure for our example." width="500" height="357" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-300x214.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-768x548.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0.png 943w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652237" class="wp-caption-text"&gt;Figure 1: The overall structure for this example.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Begin by creating the Kafka Connect config &lt;code&gt;amq-kafka-connect.yml&lt;/code&gt;. The example file present in &lt;code&gt;examples/kafka-connect/kafka-connect.yml&lt;/code&gt; was used as a reference for this config file:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata: name: simple-connect-cluster spec: version: 2.3.0 replicas: 1 bootstrapServers: simple-cluster-kafka-bootstrap:9093 tls: trustedCertificates: - secretName: simple-cluster-cluster-ca-cert certificate: ca.crt&lt;/pre&gt; &lt;p&gt;Next, execute the YAML:&lt;/p&gt; &lt;pre&gt;$ oc create -f amq-kafka-connect.yml&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 2:&lt;/p&gt; &lt;div id="attachment_652207" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849.png"&gt;&lt;img aria-describedby="caption-attachment-652207" class="wp-image-652207 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-1024x115.png" alt="Kafka Connect is is deployed." width="640" height="72" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-1024x115.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-300x34.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-768x87.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849.png 1348w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652207" class="wp-caption-text"&gt;Figure 2: The new deployment is in place.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Preparing to test your new Kafka Connect instance&lt;/h3&gt; &lt;p&gt;Let us set up to test the new instance by doing the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a new config, which will &lt;em&gt;sink&lt;/em&gt; from the &lt;code&gt;redhat-demo-topics&lt;/code&gt; topic content to the file &lt;code&gt;amq-demo-sink.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-cluster-kafka-0 sh-4.2$ curl -X POST -H "Content-Type: application/json" --data '{"name": "redhat-file-sink-demo", "config": {"connector.class":"FileStreamSinkConnector", "tasks.max":"1", "file":"/tmp/amq-demo-sink.txt", "topic":"redhat-demo-topics", "value.converter.schemas.enable" : "false", "value.converter" : "org.apache.kafka.connect.storage.StringConverter", "value.converter.schemas.enable" : "false", "key.converter" : "org.apache.kafka.connect.storage.StringConverter", "key.converter.schemas.enable" : "false"}}' http://simple-connect-cluster-connect-api.amq-streams.svc:8083/connectors&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"name":"redhat-file-sink-demo","config":{"connector.class":"FileStreamSinkConnector","tasks.max":"1","file":"/tmp/amq-demo-sink.txt","topics":"redhat-demo-topics","value.converter.schemas.enable":"false","value.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter.schemas.enable":"false","name":"redhat-file-sink-demo"},"tasks":[],"type":"sink"}&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create a new config that will &lt;em&gt;source&lt;/em&gt; into the &lt;code&gt;redhat-demo-topics&lt;/code&gt; topic content from the file &lt;code&gt;amq-demo-source.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-cluster-kafka-0 sh-4.2$ curl -X POST -H "Content-Type: application/json" --data '{"name": "redhat-file-source-demo", "config": {"connector.class":"FileStreamSourceConnector", "tasks.max":"1", "file":"/tmp/amq-demo-source.txt", "topic":"redhat-demo-topics", "value.converter.schemas.enable" : "false", "value.converter" : "org.apache.kafka.connect.storage.StringConverter", "value.converter.schemas.enable" : "false", "key.converter" : "org.apache.kafka.connect.storage.StringConverter", "key.converter.schemas.enable" : "false"}}' http://simple-connect-cluster-connect-api.amq-streams.svc:8083/connectors&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"name":"redhat-file-source-demo","config":{"connector.class":"FileStreamSourceConnector","tasks.max":"1","file":"/tmp/amq-demo-source.txt","topic":"redhat-demo-topics","value.converter.schemas.enable":"false","value.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter.schemas.enable":"false","name":"redhat-file-source-demo"},"tasks":[],"type":"source"}&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;In a new terminal start the producer sample app:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;In a new terminal start the consumer sample app:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics --from-beginning&lt;/pre&gt; &lt;h3&gt;Testing your new Kafka Connect instance&lt;/h3&gt; &lt;p&gt;To test your instance, do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log into the Kafka Connect pod and watch the &lt;code&gt;/tmp/amq-demo-sink.txt&lt;/code&gt; file:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get po | grep connect simple-connect-cluster-connect-7479b86c7-tmdbp 1/1 Running 0 3h $ oc rsh simple-connect-cluster-connect-7479b86c7-tmdbp sh-4.2$ tail -100f /tmp/amq-demo-sink.txt hello world from pramod&lt;/pre&gt; &lt;p&gt;Here, you can see that the connector has already sunk two messages into the file.&lt;/p&gt; &lt;ol start="2"&gt; &lt;li&gt; Log into the Kafka Connect pod in a different terminal, then add content into &lt;code&gt;/tmp/amq-demo-source.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-connect-cluster-connect-7479b86c7-tmdbp sh-4.2$ echo redhat-is-my-world &amp;#62; /tmp/amq-demo-source.txt&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;This set of commands writes a message in &lt;code&gt;/tmp/amq-demo-sink.txt&lt;/code&gt;, and also to the consumer sample app. Figure 3 shows the connector source push and sink output:&lt;/p&gt; &lt;div id="attachment_652277" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a.png"&gt;&lt;img aria-describedby="caption-attachment-652277" class="wp-image-652277 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-1024x270.png" alt="The connector's push and sink output." width="640" height="169" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-1024x270.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-300x79.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-768x203.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a.png 1380w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652277" class="wp-caption-text"&gt;Figure 3: The connector source pushing the message redhat-is-my-world in the second terminal.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;Figure 4 shows the sample app consuming the message:&lt;/p&gt; &lt;div id="attachment_652297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c.png"&gt;&lt;img aria-describedby="caption-attachment-652297" class="wp-image-652297 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-1024x83.png" alt="The sample app consuming the message." width="640" height="52" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-1024x83.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-300x24.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-768x62.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652297" class="wp-caption-text"&gt;Figure 4: The results on the consuming side.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Now, send a message from the producer sample app. Figure 5 shows how this message flows through the system in three parts. From top to bottom, these are the connector sink, the consumer sample app, and then the producer sample app pushing the message &lt;code&gt;kafka connect sample app&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_652317" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3.png"&gt;&lt;img aria-describedby="caption-attachment-652317" class="wp-image-652317 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-1024x444.png" alt="The message flowing through each step." width="640" height="278" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-1024x444.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-768x333.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652317" class="wp-caption-text"&gt;Figure 5: The message kafka connect sample app passing through each stage.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Kafka Bridge&lt;/h2&gt; &lt;p&gt;The Bridge component helps us connect to the Kafka Cluster using the HTTP or AMQP protocol. In this article, we demo the HTTP usage as shown in Figure 6:&lt;/p&gt; &lt;div id="attachment_652947" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257.png"&gt;&lt;img aria-describedby="caption-attachment-652947" class="wp-image-652947" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-300x215.png" alt="Our structure including the Kafka Bridge." width="500" height="358" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-768x549.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257.png 938w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652947" class="wp-caption-text"&gt;Figure 6: How the Kafka Bridge fits in through the HTTP protocol.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Kafka Bridge produces a REST API for the HTTP protocol, through which it provides multiple operations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sending messages.&lt;/li&gt; &lt;li&gt;Subscribing to topics.&lt;/li&gt; &lt;li&gt;Receiving messages.&lt;/li&gt; &lt;li&gt;Committing offsets.&lt;/li&gt; &lt;li&gt;Seeking specific positions.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Creating your Kafka Bridge&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;Create the &lt;code&gt;kafka-bridge&lt;/code&gt; config file &lt;code&gt;amq-kafka-bridge.yml&lt;/code&gt;. The example file present in &lt;code&gt;examples/kafka-bridge/kafka-bridge.yaml&lt;/code&gt; was used as a reference for the following config:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaBridge metadata: name: simple-bridge spec: replicas: 1 bootstrapServers: simple-cluster-kafka-bootstrap:9092 http: port: 8080&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create the bridge in OCP:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc create -f amq-kafka-bridge.yml&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You can see the results in Figure 7:&lt;/p&gt; &lt;div id="attachment_652977" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05.png"&gt;&lt;img aria-describedby="caption-attachment-652977" class="wp-image-652977" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05-300x38.png" alt="The Kafka Bridge is deployed." width="500" height="63" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05-300x38.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05.png 726w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652977" class="wp-caption-text"&gt;Figure 7: Your new Kafka Bridge.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Create a route so that we can access the bridge from outside the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc expose svc simple-bridge-bridge-service --name=simple-bridge-route&lt;/pre&gt; &lt;h3&gt;Testing your HTTP protocol-based Kafka Bridge&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;Create the Kafka topic config &lt;code&gt;amq-kafka-topic.yml&lt;/code&gt; and apply it to the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: simple-topic labels: strimzi.io/cluster: simple-cluster spec: partitions: 5 replicas: 1 config: retention.ms: 7200000 segment.bytes: 1073741824 oc create -f amq-kafka-topic.yml&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Get the route URL for the bridge endpoints:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;#get the route to do the curl command oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD simple-bridge-route simple-bridge-route-amq-streams.apps.redhat.demo.com simple-bridge-bridge-service rest-api None&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;Publish a message on &lt;code&gt;simple-topic&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST \ http://simple-bridge-route-amq-streams.apps.redhat.demo.com/topics/simple-topic \ -H 'content-type: application/vnd.kafka.json.v2+json' \ -d '{ "records": [ { "value": "all hail the shadowman" } ] }'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"offsets":[{"partition":0,"offset":0}]}&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;Create the consumer group &lt;code&gt;simple-rh-bridge-consumer-group&lt;/code&gt; and the instance &lt;code&gt;simple-rh-bridge-consumer&lt;/code&gt;. For this task, we set the message format to JSON:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST \ http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group \ -H 'content-type: application/vnd.kafka.v2+json' \ -d '{ "name": "simple-rh-bridge-consumer", "auto.offset.reset": "earliest", "format": "json", "enable.auto.commit": false, "fetch.min.bytes": 512, "consumer.request.timeout.ms": 30000 }'&lt;/pre&gt; &lt;ol start="5"&gt; &lt;li&gt;Create a subscriber for the &lt;code&gt;simple-topic&lt;/code&gt; created in step one:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/subscription \ -H 'content-type: application/vnd.kafka.v2+json' \ -d '{ "topics": [ "simple-topic" ] }'&lt;/pre&gt; &lt;ol start="6"&gt; &lt;li&gt;Consume the messages (note that the first request will register and the subsequent calls will provide the array of messages):&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X GET http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/records \ -H 'accept: application/vnd.kafka.json.v2+json'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;[] curl -X GET http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/records \ -H 'accept: application/vnd.kafka.json.v2+json'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;And the additional output:&lt;/p&gt; &lt;pre&gt;[{"topic":"simple-topic","key":null,"value":"all hail the shadowman","partition":0,"offset":0}]&lt;/pre&gt; &lt;h2&gt;Mirror Maker&lt;/h2&gt; &lt;p&gt;Kafka Mirror Maker replicates data from one Kafka cluster to another. The usual use case is across different data centers.&lt;/p&gt; &lt;p&gt;For the purpose of this demo, we use two different namespaces and projects, namely &lt;code&gt;amq-streams&lt;/code&gt; and &lt;code&gt;amq-streams-dc2&lt;/code&gt;. Doing so is the same as having multiple data centers with the same names. This setup is shown in Figure 8:&lt;/p&gt; &lt;div id="attachment_653107" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f.png"&gt;&lt;img aria-describedby="caption-attachment-653107" class="wp-image-653107" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-300x212.png" alt="Our structure for replicating multiple data centers." width="500" height="354" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-300x212.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-768x543.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f.png 942w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653107" class="wp-caption-text"&gt;Figure 8: Replicating multiple data centers with Kafka Mirror Maker.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Kafka Mirror maker consumes from the active Kafka cluster and produces to the mirror (backup) Kafka cluster.&lt;/p&gt; &lt;h3&gt;Setting up for the example&lt;/h3&gt; &lt;p&gt;To demo the Kafka Mirror Maker, we need to create another namespace and Kafka cluster. First, create the new namespace &lt;code&gt;amq-streams-dc2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ oc new-project amq-streams-dc2&lt;/pre&gt; &lt;p&gt;Next, create a new Kafka cluster:&lt;/p&gt; &lt;pre&gt;$ sed -i 's/namespace: .*/namespace: amq-streams-dc2/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;p&gt;On macOS, use the following instead:&lt;/p&gt; &lt;pre&gt;$ sed -i '' 's/namespace: .*/namespace: amq-streams-dc2/' install/cluster-operator/*RoleBinding*.yaml $ oc apply -f install/cluster-operator -n amq-streams-dc2 $ oc apply -f amq-kafka-cluster.yml -n amq-streams-dc2&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 9:&lt;/p&gt; &lt;div id="attachment_653477" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192.png"&gt;&lt;img aria-describedby="caption-attachment-653477" class="wp-image-653477" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-300x178.png" alt="All of the components installed so far." width="500" height="296" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-300x178.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-768x455.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192.png 941w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653477" class="wp-caption-text"&gt;Figure 9: All of the pieces are in place so far.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Creating a mirror with Kafka Mirror Maker&lt;/h3&gt; &lt;p&gt;Create a &lt;code&gt;kafka-mirror-maker&lt;/code&gt; config &lt;code&gt;amq-kafka-mirror-maker.yml&lt;/code&gt;. In this file, we increase the consumer stream to two for faster response and use the group ID &lt;code&gt;simple-source-group-id&lt;/code&gt; for the consumer. Additionally, we whitelist all of the topics using wildcards. This example file present in &lt;code&gt;examples/kafka-mirror-maker/kafka-mirror-maker.yaml&lt;/code&gt; was used as a reference for the config:&lt;/p&gt; &lt;div&gt; &lt;div&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaMirrorMaker metadata: name: simple-mirror-maker spec: version: 2.3.0 replicas: 1 consumer: bootstrapServers: simple-cluster-kafka-bootstrap:9092 groupId: simple-source-group-id numStreams: 2 producer: bootstrapServers: simple-cluster-kafka-bootstrap.amq-streams-dc2.svc:9092 whitelist: ".*"&lt;/pre&gt; &lt;div&gt;Create the Mirror Maker in the &lt;code&gt;amq-streams&lt;/code&gt; namespace:&lt;/div&gt; &lt;/div&gt; &lt;pre&gt;$ oc create -f amq-kafka-mirror-maker.yml -n amq-streams&lt;/pre&gt; &lt;div&gt; &lt;p&gt;You can see the result in Figure 10:&lt;/p&gt; &lt;div id="attachment_653487" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec.png"&gt;&lt;img aria-describedby="caption-attachment-653487" class="wp-image-653487" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec-300x30.png" alt="Mirror Maker is deployed." width="500" height="50" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec-300x30.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec.png 729w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653487" class="wp-caption-text"&gt;Figure 10: Your new Mirror Maker instance.&lt;/p&gt;&lt;/div&gt; &lt;/div&gt; &lt;h3&gt;Testing Kafka Mirror Maker&lt;/h3&gt; &lt;p&gt;To test the Mirror Maker, create the following two namespaces: &lt;code&gt;amq-streams&lt;/code&gt; and &lt;code&gt;amq-streams-dc2&lt;/code&gt;. The &lt;code&gt;amq-streams&lt;/code&gt; namespace will contain the producer sample app to produce new messages, and the consumer sample app to consume new messages. The &lt;code&gt;amq-streams-dc2&lt;/code&gt; namespace will contain the consumer sample app so it can consume new messages, so it can show that the messages are getting pushed to the DC2 cluster.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a producer sample app and consumer sample app in the &lt;code&gt;amq-streams&lt;/code&gt; namespace:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams $ oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics $ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create a Consumer sample app in the &lt;code&gt;amq-streams-dc2&lt;/code&gt; namespace&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams-dc2 $ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;Send a message from the producer sample app.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is Mirror Maker working? You should see the message in the consumer app in both the namespaces, as shown in Figure 11:&lt;/p&gt; &lt;div id="attachment_653507" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135.png"&gt;&lt;img aria-describedby="caption-attachment-653507" class="wp-image-653507" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-300x144.png" alt="The message flowing through the components." width="500" height="239" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-300x144.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-768x368.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135.png 942w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653507" class="wp-caption-text"&gt;Figure 11: Your message flowing from the producer to both the consumer and the backup consumer.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored Red Hat AMQ Streams components like Kafka Connect, Kafka Bridge, and Mirror Maker. In the third and final part of the series, we will cover monitoring and administration.&lt;/p&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confluent.io/current/connect/index.html" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index" target="_blank" rel="noopener noreferrer"&gt;Using AMQ Streams on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ppatierno/amqp-kafka-demo" target="_blank" rel="noopener noreferrer"&gt;AMQP Kafka Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/docs/latest/" target="_blank" rel="noopener noreferrer"&gt;Using Strimzi (latest)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#038;title=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" data-a2a-url="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/" data-a2a-title="Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XV4puC8LLGM" height="1" width="1" alt=""/&gt;</content><summary>In the previous article in this series, we discussed the basics of Red Hat AMQ Streams on Red Hat OpenShift. Here are a few key points to keep in mind before we proceed: AMQ Streams is based on Apache Kafka. AMQ Streams for the OpenShift Container Platform is based on the Strimzi project. AMQ Streams on containers has multiple components, such as the Cluster Operator, Entity Operator, Mirror Maker...</summary><dc:creator>Pramod Padmanabhan</dc:creator><dc:date>2019-12-05T08:00:07Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/</feedburner:origLink></entry></feed>
