<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Event Driven Drools: CEP (Complex Event Processing) Explained</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/LS5Vilh35h0/event-driven-drools-cep-complex-event-processing-explained.html" /><author><name>Luca Molteni</name></author><id>https://blog.kie.org/2021/10/event-driven-drools-cep-complex-event-processing-explained.html</id><updated>2021-10-14T10:32:32Z</updated><content type="html">In this article we’ll introduce you to a powerful feature of Drools, CEP (Complex Event Processing) To understand the context, let me introduce the idea about real time computing, a specific branch of computing in which the response of the system is a key component of the result. In other terms, in a real-time system we care not only about the result, but also when we receive it. As an example, if a big company is creating a report out of a database we probably don’t care about how long it takes to process it. Most probably management wants to have it as soon as possible, but most of the time, they won’t care if it’s delivered in thirty minutes or in one hour. Let’s imagine that you’re travelling by plane to your destination, and suddenly one of the engines of the airplane stops working. The pilot wants to be alerted as soon as possible to take the eventual steps to land the airplane safely. In this case time is an essential factor of the computation. If the information is delivered late, the safety of the people can be put at risk. It’s important that the software of the airplane runs on a real time system, so that we’re sure that the information comes instantly. In this example, we call the information on the engine stopping an event. WHAT IS AN EVENT? After the examples, you might have a general idea of what an event is: it’s something that happens in a specific moment in time and that changes the state of the system significantly. Event processing is then a set of tools that let developers write programs against those facts to process them and act accordingly. For example, we might react to the fact that the price of a Bitcoin is currently less than 2000$ and it could be a good idea to buy it. Definitely not as important as an airplane having a problem, but it might be useful if you’re into cryptocurrencies trading. Drools is mainly a rule engine so not an end-to-end real time solution, it is not intended to provide any guarantees of delivery time similar to ad-hoc trading systems. But it does execute a lot of rules very fast and moreover it does provide tools to process events by writing rules in the same language we use to write business rules: DRL. Finally, together with Kogito it becomes an end to end cloud-native business automation solution that can react to domain specific events. WHAT IS CEP? Complex event processing is an evolution of event processing allowing it to react to temporal correlations among events. The system, in this case Drools, provides tools to make it easy for the users to mix the information available by creating extractions and projections. CEP systems have been out for a while, but it’s an always actual topic. For example Reactive systems use events to promote decoupling between various parts of your architecture and can ease the maintenance. In this example we have three different microservices tightly coupled together. One microservice calls directly the other, and inserting a new service in between might be complicated. Another problem with this architecture is handling distributed transactions. When introducing some kind of event processing to orchestrate microservices we decouple the services from one another and adding a new service will be a matter of creating new types of events and handling them. Events will also inform the services of eventual problems or errors in the process so that each service can rollback its transaction independently, leading to a more robust distributed transaction mechanism. Also in modern reactive Java development we saw some example of Complex Event Processing, if we take a look at Vert.X or its Camel Bridge () we might see that idea of passing Events is ubiquitous. WHAT IS CEP USEFUL FOR? There are many use cases for CEP, some of them might be * Stock Market * IOT * Fraud Detection * Monitoring In this article we’re going to take a look at an example of monitoring. You can find the source code of the example on EXAMPLE A MONITORING SERVICE WRITTEN WITH DROOLS The example provided is targeting Drools 7.59.0.Final, while we expect that most of the CEP related concepts will be identical in Kogito, the syntax of the rule and the API will probably differ slightly. First of all we have to understand what an event is for Drools. An event is a fact like every other object inserted inside a Drools working memory with some kind of temporal information. If we provide a single timestamp the event is a Punctual Event, for example an alarm. There is another type of events which are called Interval events that have a begin timestamp and a duration. In our monitoring service, we have a heartbeat event that is expected to signal that our monitored system is alive. All we need is a few annotations on Java POJOs. There are other ways to define an event, please refer to . @Role(Role.Type.EVENT) @Timestamp("ts") public class HeartBeat { private Date ts; public HeartBeat() { } public Date getTs() { return ts; } public void setTs(Date ts) { this.ts = ts; } } To enable CEP processing in Drools we must configure the KieBase to work in the STREAM mode. To do so, in Drools we can configure it via the kmodule.xml file like this: &lt;kmodule xmlns="; &lt;kbase name="CEPExplained" eventProcessingMode="stream" packages="kie.live"&gt; &lt;/kbase&gt; &lt;/kmodule&gt; Let’s now write the first rule, we want to check whether the heartbeat event is coming every five seconds. If it’s late for any reason, we want to communicate it, in this case using a System.out.println but surely in a real monitoring system it would send a notification of some kind. We’ll also add the last heartbeat to a collection so that we can check the last event inserted. rule "heartbeat rule" when $h : HeartBeat( $ts: ts) not(HeartBeat( this != $h, this after[0s, 5s] $h)) then System.out.println("Heartbeat not received in 5s after: " + $ts); controlSet.push($h); // clear this in production end Then we can write a test testing this functionality. Deque check = new ArrayDeque&lt;&gt;(); session.setGlobal("controlSet", check); HeartBeat hb1 = new HeartBeat(); Date hb1Date = Date.from(Instant.now()); hb1.setTs(hb1Date); session.insert(hb1); // You shouldn't probably test like this Thread.sleep(6000); session.fireAllRules(); assertEquals(hb1Date, check.pop().getTs()); This test looks strange: there’s a Thread.sleep inside. This means that your test will stay idle for six seconds without doing anything. If you write a lot of tests like this, the whole suite will slow down significantly for no reason. Drools provides a better tool to deal with time testing, called Pseudo Clock. Pseudo Clock is a mechanism to have a kind of simulated clock, totally unrelated to the actual computer clock, that we can handle accordingly. This is the previous test written with the Pseudo Clock: KieSessionConfiguration conf=KieServices.Factory.get().newKieSessionConfiguration(); conf.setOption(ClockTypeOption.PSEUDO); KieSession session=kieBase.newKieSession(conf,null); ... SessionPseudoClock clock=session.getSessionClock(); HeartBeat hb1=new HeartBeat(); Date hb1Date=Date.from(Instant.ofEpochMilli(clock.getCurrentTime())); hb1.setTs(hb1Date); session.insert(hb1); clock.advanceTime(5,TimeUnit.SECONDS); session.fireAllRules(); assertEquals(hb1,check.pop()); The first lines are to configure the pseudo clock programmatically only in this test, it’s also possible to do it in the kmodule.xml like this. &lt;kbase name="CEPExplained" eventProcessingMode="stream" packages="kie.live" &gt; &lt;ksession name="default" clockType="pseudo" /&gt; &lt;/kbase&gt; The rest of the test looks similar, but instead of calling the Java Instant.now we ask to the Pseudo Clock for the timestamp with clock.getCurrentTime() and instead of waiting with Thread.sleep we use clock.advanceTime(5, TimeUnit.SECONDS). If you try running this test from the example you’ll see that it runs very fast, without test interruptions. COMPLEX EVENT PROCESSING The previous basic monitoring scenario is basic event processing. Let’s try and promote it to Complex Event Processing, and by that I mean to correlate various types of events. For example our monitoring system might decide to notify some external service of how many time a computer has been rebooted. We’ll have another kind of event called SystemRebootEvent that will have a timestamp and a duration of the reboot, and we can count the events with an accumulate function. The SystemRebootEvent will also have a userId field that we’ll use to get the information about the system administrator of the computer from the Drools memory, exactly how we do with a normal rule with another pattern, by creating a new User pattern with a condition. rule "Computer is rebooting too many times" when $r1 : SystemRebootEvent($cId : computerId, $uId: userId) $numberOfTimes : Number(this &gt;= 2) from accumulate( $r2 : SystemRebootEvent(this != $r1, computerId == $cId, this meets[1h] $r1), count($r2) ) $u : User(id == $uId, notified == false) then System.out.println($u.getUsername() + ", your computer was rebooted " + $numberOfTimes + " times after a reboot" ); modify($u) { setNotified(true); } usersNotified.push($u); // clear this in production end FUTURE OF CEP IN KOGITO As stated before, most of the CEP functionalities of Drools 7 will be supported in Drools 8 and in Kogito as well, albeit probably with some differences in the API. For Kogito specifically we’re planning to also provide some new features built on top that will allow users to leverage the power of the hybrid cloud to get high availability and some form of distributed processing by coordinating different rule units. These features are still in an Alpha stage, so if you’re interested please continue following this blog. Here’s a sneak peek of these features: In Kogito 1.12, bound to be released in the following days, we added the possibility to use a CloudEvents message via Kafka to trigger the execution of a DRL (see . This is currently supported only for stateless use cases but it is the first step to bring cloud-native replicated CEP Drools capabilities in Kogito. CONCLUSION We understood the idea behind having time based events in a rule engine and we saw a simple example of a monitoring service with the correct approach to testing CEP. We also saw how the monitoring service can be evolved using the same DRL syntax that we normally use to write business rules. For more information, please refer to the . The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/LS5Vilh35h0" height="1" width="1" alt=""/&gt;</content><dc:creator>Luca Molteni</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/event-driven-drools-cep-complex-event-processing-explained.html</feedburner:origLink></entry><entry><title type="html">MicroProfile Reactive Messaging 2.0 in WildFly 25</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9Q4oDD5ymOQ/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2021/10/14/MicroProfile-Reactive-Messaging-2.0-in-WildFly-25/</id><updated>2021-10-14T00:00:00Z</updated><content type="html">For WildFly 25, we upgraded the support from version 1 to 2. It contains a new annotation, which in conjunction with the new interface, were introduced in order to make it possible to push data into the MicroProfile Reactive streams from code initiated by a user. The MicroProfile Reactive Messaing implementation in WildFly is based on the project. The version included in WildFly 25, introduces a new to have more control over how we interact with Kafka. We expose this API in WildFly 25. This post will: * Take a simple web application, consisting of a few html pages, and add a Servlet filter to push information about page visits into Reactive Messaging via an Emitter. * These messages will be forwarded onto Kafka * Show a standalone application to read the last visited page per user from Kafka via the Kafka Streams API * Deploy the above application into WildFly, bundling the Kafka Streams API (which we don’t ship in WildFly) to read the last visited page per user. The code for the application is on . Additionally, you can find more information about the MicroProfile Reactive Messaging functionality in WildFly in the . RUNNING THE APPLICATION See the GitHub repository for instructions on how to build and run the different parts of the application. Here we will focus on explaining how it works. THE MAIN APPLICATION The main application is contained in the folder. The core of the application is a which link to each other. Now, we want to track which user visited which page. We do this by enhancing the application with a Servlet filter called : public class MessagingFilter extends HttpFilter { @Inject @Channel("from-filter") Emitter&lt;PageVisit&gt; messagingEmitter; FIrst we have field injected via CDI of type Emitter, called messagingEmitter, which is annotated with @Channel. This Emitter instance makes it a breeze to push data to the MicroProfile Reactive Messaging stream indicated by the value of the @Channel annotation (i.e. from-filter). @Override public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws IOException, ServletException { String user = getUsername(request); String address = request.getRemoteAddr(); String page = Paths.get(request.getRequestURI()).getFileName().toString(); PageVisit pv = new PageVisit(user, address, page); messagingEmitter.send(pv); Next we gather information about the request (user, address and page name), and bundle this information up in a bean. We then use the injected Emitter to send the PageVisit instance. The Emitter will then send the PageVisit to the from-filter stream. // Disable caching for the html pages ((HttpServletResponse)response).addHeader("Cache-control", "no-store"); ((HttpServletResponse)response).addHeader("Pragma", "no-cache"); filterChain.doFilter(request, response); } Getting the user name is simulated in the following method which randomly chooses a user for the current request. For the purposes of this demo this is to get a few different users in the data recorded when we click on the links when running the application. private String getUsername(HttpServletRequest servletRequest) { // Pretend we're looking up the authenticated user switch ((int)Math.round(Math.random() * 3)) { case 0: return "bob"; case 1: return "emma"; case 2: return "frank"; case 3: return "linda"; } return null; } } Next, we have an ApplicationScoped CDI bean called @ApplicationScoped public class MessagingBean { @Inject @Channel("special") Emitter&lt;PageVisit&gt; special; @Incoming("from-filter") @Outgoing("kafka-visits") public Message&lt;PageVisit&gt; fromFilter(PageVisit pageVisit) { if (pageVisit.getPage().equals("3.html")) { special.send(pageVisit); } Message&lt;PageVisit&gt; msg = Message.of(pageVisit); msg = KafkaMetadataUtil.writeOutgoingKafkaMetadata( msg, OutgoingKafkaRecordMetadata .&lt;String&gt;builder() .withKey(pageVisit.getUserName()) .build()); return msg; } @Incoming("special") public void special(PageVisit pageVisit) { System.out.println("===&gt; " + pageVisit.getUserName() + " visited " + pageVisit.getPage()); } } The fromFilter() method is annotated with the @Incoming("from-filter") annotation (from version 1 of the specification) and will receive all messages that were sent on our previous Emitter. Since the both the @Incoming and @Channel annotations use the value from-filter (i.e. they match), we end up with a simple in-memory stream. We could of course have routed this via Kafka, but for this example I wanted to keep the configuration needed to map to Kafka as simple as possible. The goes into more details about how to configure MicroProfile Reactive Messaging streams to consume from Kafka topics. The fromFilter() method is also annotated with @Outgoing("kafka-visits"), and so it is expected that all incoming messages from the from-filter stream will be forwarded onto the kafka-visits stream. The kafka-visits stream is backed by Kafka (we will see how to map this stream onto a Kafka topic in a second). In this case we decide that we want messages sent on this topic to have a Kafka key, so we: * Wrap the incoming PageVisit object in a Message object, which comes from the MicroProfile Reactive Messaging specification. * We then create an instance, where we set the key of the record to be the user. We add this metadata to the message by calling . The mentioned classes come from the new . * Finally we return the massaged Message containing our received PageVisit instance, which will forward it to the kafka-visits stream. Another thing going on in this example, is that we’re using an injected Emitter to 'fork' the sending of the received data to an additional location. In fromFilter(), if the page 3.html was visited, we will also send the received PageVisit via the injected Emitter. This in turn will send the PageVisit instance on the special stream indicated in its @Channel annotation. The special() method, annotated with @Incoming(`special) receives messages from the special stream (i.e. the ones sent via the Emitter). When running the application, and clicking on the 3 link, you should see output in the server logs. Additionally, every click on any link will show up in the Kafka consumer logs mentioned in the example . So, in addition to being able to easily send data from user-initiated code, Emitter is useful for 'forking' streams, so you can send data to more than one location. This functionality was not present in version 1 of the specification. To map the kafka-visits stream to a Kafka topic we do the configuration in : mp.messaging.connector.smallrye-kafka.bootstrap.servers=localhost:9092 mp.messaging.outgoing.kafka-visits.connector=smallrye-kafka mp.messaging.outgoing.kafka-visits.topic=page-visits mp.messaging.outgoing.kafka-visits.value.serializer=org.wildfly.blog.reactive.messaging.common.PageVisitsSerializer This points the mapping towards localhost:9092 to connect to Kafka, maps the kafka-visits stream to the page-visits kafka topic, and specifies to be used to serialize the PageVisit instances that we send to Kafka. The contains more detailed information about this configuration. If you deploy the application into WildFly, as outlined in the example , and you performed the optional step of connecting a Kafka consumer, you should see the output similar to this in the Kafka consumer terminal as you click the links in the application hosted at : frank 127.0.0.1app emma 127.0.0.13.html frank 127.0.0.11.html linda 127.0.0.13.html frank 127.0.0.11.html emma 127.0.0.12.html frank 127.0.0.13.html When you visit 3.html, there will be additional output from the special() method in WildFly’s server.log ===&gt; emma visited 3.html ===&gt; linda visited 3.html ===&gt; frank visited 3.html READING DATA FROM KAFKA IN A STANDALONE APPLICATION While it is nice to be able to send (and receive, although not shown in this example) messages via Kafka, we may want to query the data in Kafka later. The code for the command line application to query data from Kafka is contained in the folder. It contains a very simple (I am a beginner at this part) application to get the most recent page visits per user. It uses the API to interact with Kafka. The class calls through to a more interesting DataStoreWrapper class. public static void main(String[] args) throws Exception { try (DataStoreWrapper dsw = new DataStoreWrapper()) { dsw.init(); Map&lt;String, String&gt; lastPagesByUser = Collections.emptyMap(); try { dsw.readLastVisitedPageByUsers(); } catch (InvalidStateStoreException e) { } if (lastPagesByUser.size() == 0) { // It seems that although the stream is reported as RUNNING // in dsw.init() it still needs some time to settle. Until that // happens there is no data or we get InvalidStateStoreException Thread.sleep(4000); lastPagesByUser = dsw.readLastVisitedPageByUsers(); } System.out.println("Last pages visited:\n" + lastPagesByUser); } } } Note There is some error handling here. In case you get no entries, or if you get InvalidStateStoreException, try increasing the timeout in the sleep. Looking at the class, the first thing to note is that it is 'CDI ready'. Although this section will run it as a standalone application where CDI is not relevant, we will reuse this class later in an application deployed in WildFly. @ApplicationScoped public class DataStoreWrapper implements Closeable { private volatile KafkaStreams streams; We will initialise this streams instance in the init() method below. @Inject private ConfigSupplier configSupplier = new ConfigSupplier() { @Override public String getBootstrapServers() { return "localhost:9092"; } @Override public String getTopicName() { return "page-visits"; } }; The configSupplier field is inititalised to an implementation of which hard codes the values of the Kafka bootstrap servers, and the topic name. When deploying this into WildFly later we will use MicroProfile Config to set these values to avoid hard coding them. DataStoreWrapper() { } Next, we will take a look at the init() method where we set up the ability to query the stream. @PostConstruct void init() { try { Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-pipe"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, configSupplier.getBootstrapServers()); props.putIfAbsent(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0); props.putIfAbsent(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); props.putIfAbsent(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, PageVisitSerde.class.getName()); // For this we want to read all the data props.putIfAbsent(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); The above sets configuration properties to connect to kafka, and sets s for (de)serializing the Kafka record keys and values. The class is used to (de)serialise our class from earlier. We also specify that we want all the data stored on this topic. final StreamsBuilder builder = new StreamsBuilder(); KeyValueBytesStoreSupplier stateStore = Stores.inMemoryKeyValueStore("test-store"); KTable&lt;String, PageVisit&gt; source = builder.table( configSupplier.getTopicName(), Materialized.&lt;String, PageVisit&gt;as(stateStore) .withKeySerde(Serdes.String()) .withValueSerde(new PageVisitSerde())); final Topology topology = builder.build(); this.streams = new KafkaStreams(topology, props); Now we create a associated with the Kafka topic, and create a from that. In this case since we are using the Kafka record key (above we used the user for this when sending to Kafka) as the KTable key, we will get one entry (the latest) for each user. Note this is a very simple example, and not an in-depth exploration of the Kafka Streams API, so of course more advanced views on the stored data are possible! final CountDownLatch startLatch = new CountDownLatch(1); final AtomicReference&lt;KafkaStreams.State&gt; state = new AtomicReference&lt;&gt;(); streams.setStateListener((newState, oldState) -&gt; { state.set(newState); switch (newState) { case RUNNING: case ERROR: case PENDING_SHUTDOWN: startLatch.countDown(); } }); this.streams.start(); startLatch.await(10, TimeUnit.SECONDS); System.out.println("Stream started"); if (state.get() != KafkaStreams.State.RUNNING) { throw new IllegalStateException(); } Finally, we start the stream and wait for it to start. } catch (Exception e) { if (this.streams != null) { this.streams.close(); } throw new RuntimeException(e); } } The readLastVisitedPageByUsers() method uses the StateStore we set up earlier and returns all the found entries: public Map&lt;String, String&gt; readLastVisitedPageByUsers() { StoreQueryParameters&lt;ReadOnlyKeyValueStore&lt;String, PageVisit&gt;&gt; sqp = StoreQueryParameters.fromNameAndType("test-store", QueryableStoreTypes.keyValueStore()); final ReadOnlyKeyValueStore&lt;String, PageVisit&gt; store = this.streams.store(sqp); Map&lt;String, String&gt; lastPageByUser = new HashMap&lt;&gt;(); KeyValueIterator&lt;String, PageVisit&gt; it = store.all(); it.forEachRemaining(keyValue -&gt; lastPageByUser.put(keyValue.key, keyValue.value.getPage())); return lastPageByUser; } @PreDestroy public void close() { this.streams.close(); } } If you run the application, following the instructions in the example , you should see output like this: Stream started Last pages visited: {frank=3.html, emma=2.html, linda=3.html} As already mentioned, this will be the latest page visited for each user. READING DATA FROM KAFKA IN A WILDFLY APPLICATION WildFly does not ship with the Kafka Streams API, but we can still deploy the application above into WildFly with some adjustments in how we package it. The example contains more details, but in a nutshell we: * Include the Kafka Streams API jar in our deployment * Make sure we don’t include all the Kafka Streams API jar’s transitive dependencies in our deployment since they already exist in WildFly. * Modify the deployment’s META-INF/MANIFEST.MF to set up a dependency on the org.apache.kafka.client JBoss Module. This module contains the Kafka client jar, which is needed by the Kafka Streams API. In our standalone application, we hardcoded the bootstrap servers and the topic name. When deploying to WildFly we would like to avoid recompiling the application if, say, Kafka moves somewhere else, so we specify this information in : kafka.bootstrap.servers=localhost:9092 kafka.topic=page-visits We then create an implementation of the interface in . This is an ApplicationScoped CDI bean which gets injected with the MicroProfile Config containing the properties from the microprofile-config.properties file: @ApplicationScoped public class MpConfigConfigSupplier implements ConfigSupplier { @Inject Config config; @Override public String getBootstrapServers() { return config.getValue("kafka.bootstrap.servers", String.class); } @Override public String getTopicName() { return config.getValue("kafka.topic", String.class); } } Our class from earlier is a CDI bean, and so our MpConfigConfigSupplier will get injected into its configSupplier field, overwriting the default implementation that was used in the standalone application case: @ApplicationScoped public class DataStoreWrapper implements Closeable { private volatile KafkaStreams streams; @Inject private ConfigSupplier configSupplier = new ConfigSupplier() { // -- SNIP -- // This implementation gets replaced by the injected MpConfigConfigSupplier In order to be able to call this from a client, we add a simple : @Path("/") @Produces(MediaType.APPLICATION_JSON) public class StreamsEndpoint { @Inject DataStoreWrapper wrapper; @GET @Path("/last-visited") public Map&lt;String, String&gt; getLastVisited() { return wrapper.readLastVisitedPageByUsers(); } } This simply delegates to our DataStoreWrapper. If you deploy the application as outlined in the example , and visit you should see output like: {"frank":"3.html","emma":"2.html","linda":"3.html"} CONCLUSION We have seen how to leverage the new Emitter in MicroProfile Reactive Messaging 2 to push data to MicroProfile Reactive Messaging Streams, and how to send data to Kafka. We also used the new Kafka User API to set the Kafka record key in the data sent to Kafka. Although we did not receive data from Kafka in this example, we leveraged the Kafka Streams API to read the data we stored in Kafka in a standalone application as well as in an application deployed to WildFly. REFERENCES The contains more information on the various configuration options for using MicroProfile Reactive Messaging with Kafka in WildFly. Also, the contains a fuller reference of configuration options for Kafka, as well as more information about MicroProfile Reactive Messaging in general. Finally, the MicroProfile Reactive Messaging specification can be found in the GitHub project.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9Q4oDD5ymOQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Kabir Khan</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/14/MicroProfile-Reactive-Messaging-2.0-in-WildFly-25/</feedburner:origLink></entry><entry><title type="html">JBang cheatsheet (2021)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NnBiAudJsh8/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/jbang-cheatsheet-2021/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=jbang-cheatsheet-2021</id><updated>2021-10-13T12:27:46Z</updated><content type="html">Here is my JBang cheatsheet which can help as a reference to code self-contained source-only Java programs with unmatched ease. Getting started with JBang Install JBang on Linux curl -Ls https://sh.jbang.dev | bash -s - app setup Create a JBang class  from a template jbang init --template=hello helloworld.java List of available JBang templates jbang template ... The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NnBiAudJsh8" height="1" width="1" alt=""/&gt;</content><dc:creator>F.Marchioni</dc:creator><feedburner:origLink>http://www.mastertheboss.com/java/jbang-cheatsheet-2021/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=jbang-cheatsheet-2021</feedburner:origLink></entry><entry><title>Printf-style debugging using GDB, Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ynVi4YgpSMk/printf-style-debugging-using-gdb-part-2" /><author><name>Kevin Buettner</name></author><id>8f0f522e-6890-42be-a93d-51f28d43bddf</id><updated>2021-10-13T07:00:00Z</updated><published>2021-10-13T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1"&gt;first article in this series&lt;/a&gt; introduced the &lt;a href="https://www.gnu.org/software/gdb/"&gt;GNU debugger, GDB&lt;/a&gt;, and in particular its &lt;code&gt;dprintf&lt;/code&gt; command, which displays variables from programs in a fashion similar to &lt;a href="https://developers.redhat.com/topics/c"&gt;C-language&lt;/a&gt; &lt;code&gt;printf&lt;/code&gt; statements. This article expands on the rich capabilities of printf-style debugging by showing how to save commands for reuse and how to save the output from the program and GDB for later examination.&lt;/p&gt; &lt;h2&gt;Listing currently defined breakpoints&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;dprintf&lt;/code&gt; command creates a special type of breakpoint. The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Set-Breaks.html#index-info-breakpoints"&gt;&lt;code&gt;info breakpoints&lt;/code&gt; command&lt;/a&gt; displays all breakpoints; however, at the moment, we have only &lt;code&gt;dprintf&lt;/code&gt; breakpoints defined:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) info breakpoints Num Type Disp Enb Address What 1 dprintf keep y 0x0000000000401281 in insert at tree.c:41 breakpoint already hit 7 times printf "Allocating node for data=%s\n", data 2 dprintf keep y 0x00000000004012b9 in insert at tree.c:47 breakpoint already hit 6 times printf "Recursing left for %s at node %s\n", data, tree-&gt;data 3 dprintf keep y 0x00000000004012de in insert at tree.c:49 breakpoint already hit 6 times printf "Recursing right for %s at node %s\n", data, tree-&gt;data (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Saving dprintf commands for a later session&lt;/h2&gt; &lt;p&gt;In traditional printf-style debugging, print statements added to the program persist until they are removed. This is not the case when using the &lt;code&gt;dprintf&lt;/code&gt; command with GDB; both &lt;code&gt;dprintf&lt;/code&gt; breakpoints and ordinary breakpoints will persist throughout a GDB session, but they won't persist between sessions. However, breakpoints may be saved to a file for later reuse.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Save-Breakpoints.html#index-save-breakpoints"&gt;&lt;code&gt;save breakpoints&lt;/code&gt; command&lt;/a&gt; saves breakpoints to a file. The following example shows how to save breakpoints to a file named &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) save breakpoints my-dprintf-breakpoints Saved to file 'my-dprintf-breakpoints'. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The resulting file consists of GDB breakpoint commands saved from the session. Thus, the file &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; contains three lines:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;dprintf /home/kev/ctests/tree.c:41,"Allocating node for data=%s\n", data dprintf /home/kev/ctests/tree.c:47,"Recursing left for %s at node %s\n", data, tree-&gt;data dprintf /home/kev/ctests/tree.c:49,"Recursing right for %s at node %s\n", data, tree-&gt;data &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If changes are made to the program in between GDB sessions, the line numbers specified by these commands may no longer be correct. If that happens, the most straightforward fix is to use a text editor to adjust them.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; file can be loaded into some future GDB session—by the programmer who saved them, or by another programmer debugging the same program—via the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Command-Files.html#index-source"&gt;&lt;code&gt;source&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) quit $ gdb -q ./tree Reading symbols from ./tree... (gdb) source my-dprintf-breakpoints Dprintf 1 at 0x401281: file tree.c, line 41. Dprintf 2 at 0x4012b9: file tree.c, line 47. Dprintf 3 at 0x4012de: file tree.c, line 49. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Redirecting output&lt;/h2&gt; &lt;p&gt;Printf-style debugging can generate a lot of output. It is often useful to send debugging output to a file for later analysis.&lt;/p&gt; &lt;p&gt;By default, output from a dynamic &lt;code&gt;printf&lt;/code&gt; is sent to GDB's console. Also, by default, the output from a program run under GDB is sent to the console, but via a different file descriptor. Therefore, output from GDB and the program are usually intermixed. But since different file descriptors are used, it's possible to redirect either GDB's output or program output to a file, or even both outputs to separate files.&lt;/p&gt; &lt;h3&gt;Logging GDB's output to a file&lt;/h3&gt; &lt;p&gt;GDB provides a number of commands for saving output from GDB to a file. I'll discuss a few of them here; see the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Logging-Output.html"&gt;GDB manual&lt;/a&gt; for more information.&lt;/p&gt; &lt;p&gt;Let's suppose that you wish to save a log of GDB output to a log file named &lt;code&gt;my-gdb-log&lt;/code&gt;. This is done by first issuing the command &lt;code&gt;set logging file my-gdb-log&lt;/code&gt;, followed by the command &lt;code&gt;set logging on&lt;/code&gt;. Later on, you can issue the &lt;code&gt;set logging off&lt;/code&gt; command to stop sending GDB output to the log file. Using the &lt;code&gt;dprintf&lt;/code&gt; commands established earlier, this is what the sequence of commands looks like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) set logging file my-gdb-log (gdb) set logging on Copying output to my-gdb-log. Copying debug output to my-gdb-log. (gdb) run Starting program: /home/kev/ctests/tree Allocating node for data=dog ... scorpion wolf [Inferior 1 (process 321429) exited normally] (gdb) set logging off Done logging to my-gdb-log. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As shown in the example, both program output and GDB's output are still sent to the console. (The &lt;code&gt;set logging debugredirect on&lt;/code&gt; command can be used to send GDB's output only to the log file.) However, only GDB's output is placed in &lt;code&gt;my-gdb-log&lt;/code&gt;, as you can see by viewing that file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;Starting program: /home/kev/ctests/tree Allocating node for data=dog Recursing left for cat at node dog ... Recursing right for scorpion at node javelina Allocating node for data=scorpion [Inferior 1 (process 321429) exited normally] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note, too, that no prompts or user-typed commands appear in the log output.&lt;/p&gt; &lt;h3&gt;Redirecting program output to a file&lt;/h3&gt; &lt;p&gt;The mechanism for redirecting program output to a file is simple; &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Input_002fOutput.html#index-redirection"&gt;the &gt; redirection operator is used with the &lt;code&gt;run&lt;/code&gt; command&lt;/a&gt; in much the same way that output is redirected by most shells. The example below shows how to run the program while redirecting program output to the file &lt;code&gt;my-program-output&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output Allocating node for data=dog ... Allocating node for data=scorpion [Inferior 1 (process 321813) exited normally] (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;my-program-output&lt;/code&gt; file now looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;cat coyote dog gecko javelina scorpion wolf cat coyote dog gecko javelina scorpion wolf&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Sending dprintf output to the same file as program output&lt;/h3&gt; &lt;p&gt;When saving program output to a file, you might want to place &lt;code&gt;dprintf&lt;/code&gt;-related output in the same file, intermixed with the rest of the program output. This can be done by making GDB invoke the program's &lt;code&gt;printf()&lt;/code&gt; function from the standard C library linked with the program. GDB's &lt;code&gt;dprintf-style&lt;/code&gt; setting is used to control where &lt;code&gt;dprintf&lt;/code&gt; related output is sent. The default &lt;code&gt;dprintf-style&lt;/code&gt; setting is &lt;code&gt;gdb&lt;/code&gt;; it causes GDB's internal &lt;code&gt;printf&lt;/code&gt; command to be used, sending output to the GDB console. When the &lt;code&gt;dprintf-style&lt;/code&gt; setting is &lt;code&gt;call&lt;/code&gt;, GDB will perform what is known as an &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Calling.html#index-inferior-functions_002c-calling"&gt;&lt;em&gt;inferior function call&lt;/em&gt;&lt;/a&gt;; i.e., it will call a function in the program being debugged, in this case &lt;code&gt;printf()&lt;/code&gt;. Therefore, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Dynamic-Printf.html#index-dprintf_002dstyle-call"&gt;&lt;code&gt;set dprintf-style call&lt;/code&gt;&lt;/a&gt; command causes the output that is printed when hitting a &lt;code&gt;dprintf&lt;/code&gt; breakpoint to be performed by calling &lt;code&gt;printf()&lt;/code&gt; from within the program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) set dprintf-style call (gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output [Inferior 1 (process 322195) exited normally] (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;my-program-output&lt;/code&gt; file now contains both &lt;code&gt;dprintf&lt;/code&gt; output and program output together:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;Allocating node for data=dog Recursing left for cat at node dog ... scorpion wolf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB provides other commands that send &lt;code&gt;dprintf&lt;/code&gt; output to a different file descriptor, much like using &lt;code&gt;fprintf()&lt;/code&gt; instead of &lt;code&gt;printf()&lt;/code&gt;. These same facilities can also be used to invoke printf-style logging functions defined in the program. Refer to the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Dynamic-Printf.html#Dynamic-Printf"&gt;GDB manual&lt;/a&gt; for an explanation of these commands.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Look for the third and final article in this series, which shows powerful ways to interact with functions in your program from GDB, and how to automate the execution of GDB commands.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/13/printf-style-debugging-using-gdb-part-2" title="Printf-style debugging using GDB, Part 2"&gt;Printf-style debugging using GDB, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ynVi4YgpSMk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2021-10-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/13/printf-style-debugging-using-gdb-part-2</feedburner:origLink></entry><entry><title type="html">Infinispan 13.0.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/nYsp4uRbOP0/infinispan-13-final" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2021/10/12/infinispan-13-final</id><updated>2021-10-13T00:00:00Z</updated><content type="html">Triskaidekaphobia (/ˌtrɪskaɪˌdɛkəˈfoʊbiə/ TRIS-kye-DEK-ə-FOH-bee-ə, /ˌtrɪskə-/ TRIS-kə-; from Ancient Greek τρεισκαίδεκα (treiskaídeka) 'thirteen', and Ancient Greek φόβος (phóbos) 'fear') is fear or avoidance of the number 13. However, as you should know by now, the Infinispan team is fearless, and for this reason we are not afraid to proudly announce “Infinispan 13 (Triskaidekaphobia)”. In the grand tradition of our codenames, this one also happens to be the name of a beer () So, don’t be scared and let’s dive into all the new great things that are in this release. CORE * Polyglot configuration: XML, JSON and YAML can now be used interchangeably to configure every part of Infinispan. Here is an example of how to configure a cache with eviction and Protobuf encoding: &lt;distributed-cache&gt; &lt;encoding media-type="application/x-protostream"/&gt; &lt;memory max-size="1.5GB" when-full="REMOVE"/&gt; &lt;/distributed-cache&gt; { "distributed-cache" : { "encoding" : { "media-type" : "application/x-protostream" }, "memory" : { "max-size" : "1.5GB", "when-full" : "REMOVE" } } } distributedCache: encoding: mediaType: "application/x-protostream" memory: maxSize: "1.5GB" whenFull: "REMOVE" * Max-idle asynchronous touch: max-idle expiration makes reading an entry behave like a write: all owners must update the last access timestamp. With asynchronous touch the reader does not wait for their confirmation, and reading a max-idle entry is as fast as reading any other entry. * Metrics for cache size are now optional: calculating the accurate size of a cache is an expensive operation. Starting with this release, currentNumberOfEntries and currentNumberOfEntriesInMemory and totalNumberOfEntries will return -1 by default. You can re-enable accurate computation of these metrics if you really need them. We will be adding high-performance estimates for these metrics in a future release. QUERY * Delete by query: Ickle (Infinispan’s query language) now supports DELETE FROM queries using all of the supported clauses, both for indexed and non-indexed caches: query.create("DELETE FROM books WHERE page_count &gt; 500").executeStatement(); PERSISTENCE * The Soft-index file store is now our default persistent file-based cache store. Compared to the old single-file store, it no longer needs to store all keys in memory, plus it supports persistent memory via the awesome library. Old file stores will be automatically migrated on first use. * SQL cache store which maps database tables and queries to Protobuf, greatly simplifying accessing your existing data. For example, you can expose a single table books: &lt;table-jdbc-store table-name="books"&gt; &lt;schema message-name="books_value" key-message-name="books_key" package="library" embedded-key="true"/&gt; &lt;/table-jdbc-store&gt; or use your own queries &lt;query-jdbc-store&gt; &lt;queries key-columns="isbn"&gt; &lt;select-single&gt;SELECT isbn, title FROM books WHERE isbn = :isbn&lt;/select-single&gt; &lt;select-all&gt;SELECT isbn, title FROM books&lt;/select-all&gt; &lt;delete&gt;DELETE FROM books WHERE isbn = :key&lt;/delete&gt; &lt;delete-all&gt;DELETE FROM books&lt;/delete-all&gt; &lt;upsert&gt;INSERT INTO books (isbn, title) VALUES (:key, :value) ON CONFLICT (isbn) DO UPDATE SET title = :value&lt;/upsert&gt; &lt;size&gt;SELECT COUNT(*) FROM books&lt;/size&gt; &lt;/queries&gt; &lt;schema message-name="books_value" key-message-name="books_key" package="library" embedded-key="true"/&gt; &lt;/query-jdbc-store&gt; SERVER * Configuration overlays: you can specify multiple configuration files, in any of the supported formats, and they will be combined to form the final configuration. You can even mix formats: server.sh -c base.xml -c layer.yml -c custom.json * Mutable cache configuration: you can now update cache configurations cluster-wide at runtime with the CLI. The following example changes the maximum number of entries of a cache: alter cache mycache --attribute=memory.max-count --value=10000 * Thread Pool Consolidation: The prior Infinispan non blocking thread pool has been consolidated with the Netty event loop reducing the number of threads required in the server. * REST listeners: It is now possible to listen to cache events over HTTP using Server-Sent Events. curl --digest -u user:password -N http://127.0.0.1:11222/rest/v2/caches/mycache?action=listen event: cache-entry-created data: data: { data: "_type": "string", data: "_value": "k1" data: } event: cache-entry-modified data: data: { data: "_type": "string", data: "_value": "k1" data: } event: cache-entry-removed data: data: { data: "_type": "string", data: "_value": "k1" data: } * Rebalancing management: control cluster re-balancing from the REST API, CLI, and Console. * Simple TLS for clustering: Infinispan Server can automatically enable TLS for the cluster transport simply by specifying a security realm with a keystore/truststore server identity: &lt;cache-container name="default" statistics="true"&gt; &lt;transport cluster="cluster" server:security-realm="cluster"/&gt; &lt;/cache-container&gt; * Distributed Security Realm: a server security realm which can aggregate multiple sub-realms, trying each one in turn. For example, you can use this to support both certificate and password authentication on the same server. * PEM key/trust stores: Support for PEM files for both keys and certificates without the need to convert them to Java keystores first. * Full support for TLSv1.3 via native OpenSSL. CLUSTER MIGRATION We’ve done quite a lot of work on the cluster igration operations, making the process smoother from the REST API, CLI, and with our Kubernetes Operator. * Manually changing configurations of the cache(s) is no longer necessary * New methods in the REST API to control the migration * Caches created dynamically are now supported * Simplified configuration CROSS-SITE REPLICATION * Improve cross-site replication observability * The cross-site view (sorted list of site names currently online) and relay-nodes (members who are responsible for relaying messages between sites) are now exposed via CLI/REST/JMX. * Detailed metrics exposed per site and per cache (response times, number of messages) * Improve some error messages with more details. INFINISPAN CONSOLE * Encoding-aware entry editing * Rebalancing operations per-cache and per-cluster CLOUD * Helm charts: create Infinispan clusters with a Helm chart that lets you specify values for build and deployment configuration. Server configuration is declared using Yaml in .Values. This allows the server configuration to be customized entirely without having to update helm-chart templates locally. * Operator: many fixes and improvements: * Migrated operator-sdk from v0.18.0 → v1.3.2 * Migrated packagemanifest → OLM bundle format * K8s 1.22 deprecated APIs removed KUBERNETES CLI * Easily connect a CLI to an operator-managed Infinispan cluster without having to specify connection details: kubectl infinispan shell -n default mycluster [mycluster-0-37266@mycluster//containers/default]&gt; TESTING * An InfinispanContainer which makes it easy to test your applications via the awesome library try (InfinispanContainer container = new InfinispanContainer()) { container.start(); try (RemoteCacheManager cacheManager = container.getRemoteCacheManager()) { RemoteCache&lt;Object, Object&gt; testCache = cacheManager.administration().getOrCreateCache("test", DefaultTemplate.DIST_SYNC); testCache.put("key", "value"); assertEquals("value", testCache.get("key")); } } CLUSTERED COUNTERS * Strong counters can now expire (experimental). The counter value is reset to its initial value which may be useful to code a cluster-wide rate limiter. OTHER * Works with JDK 17 (and still works with JDK 8 and JDK 11) * Lots of bug fixes DOCUMENTATION AND TUTORIALS * Updated cache configuration docs with tabbed examples with JSON and YAML. * Added new guides for indexing and querying caches, Hot Rod clients, and Helm charts. * Re-organized Infinispan simple tutorials as part of the ongoing effort to clearly separate remote caches from embedded caches in our content. Infinispan simple tutorials now have their own documentation on our site at: * Updated documentation for configuring persistent cache stores and JVM memory management, including revisions to improve style, grammar, and provide high-level scanning and readability. * Replaced the Integration Guide with a dedicated guide for Spring users as well as a guide for Hibernate caches. We’ve also linked to community projects with Quarkus, Vert.x, Keycloak, Camel, and WildFly. Check out the new Integrations category on the docs home page at: Be sure to read through before getting started with lucky 13.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/nYsp4uRbOP0" height="1" width="1" alt=""/&gt;</content><dc:creator>Tristan Tarrant</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/10/12/infinispan-13-final</feedburner:origLink></entry><entry><title>Tools and practices for remote development teams</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YKSf73GCfog/tools-and-practices-remote-development-teams" /><author><name>Jason Dudash, Andy Krohg</name></author><id>9642af13-ef3a-40d8-86ab-f12a83aacb4e</id><updated>2021-10-12T07:00:00Z</updated><published>2021-10-12T07:00:00Z</published><summary type="html">&lt;p&gt;During the height of the COVID-19 global pandemic, tens of millions of workers transitioned from the office to working from home. It was an unfamiliar way of doing things for many organizations—a true sink-or-swim scenario. Development teams are among those affected, and the challenges that we face are sometimes very specific. In this article, we explore a few tools and practices that can help distributed development teams work and collaborate from home. Hopefully, this exploration will be helpful to you and your team seeking a “new normal" after COVID-19.&lt;/p&gt; &lt;h2&gt;Accessing the development environment&lt;/h2&gt; &lt;p&gt;Perhaps the most poignant of remote development challenges is not having access to a physical workstation. When developers work together in a company office, it is straightforward to issue everyone a desktop PC that’s equipped with sufficient firepower for the job at hand. Without this convenience, however, many organizations fall short of offering an optimal experience. Let's consider a few common approaches.&lt;/p&gt; &lt;h3&gt;Laptops to the rescue?&lt;/h3&gt; &lt;p&gt;Company laptops are a standard issue for any full-time job, but developers represent a special case. The &lt;a href="https://developers.redhat.com/blog/2020/06/16/enterprise-kubernetes-development-with-odo-the-cli-tool-for-developers"&gt;inner loop of development&lt;/a&gt; demands a plethora of builds, tests, and local deployments. This demanding cycle renders the compute resources of a typical work laptop insufficient. The beefier machines sometimes given to developers to mitigate this are still a compromise between &lt;em&gt;user experience&lt;/em&gt; and &lt;em&gt;risk&lt;/em&gt;. Portable workstations are more likely to be damaged, lost, or stolen, presenting cogent risks of not only financial impact but the loss of data privacy. With physical access to a device that houses classified information, a malicious actor could create problems that eclipse concerns related to infrastructure costs.&lt;/p&gt; &lt;h3&gt;Okay, well then VMs?&lt;/h3&gt; &lt;p&gt;Another common practice is for development teams to use virtual machines in a data center for their development environment. The advantage here is that the laptops need not be anything special; the laptops only serve as a means to access the VM. The problem is network latency. Remote desktop environments aren’t as smooth or performant as I/O, especially with insufficient network bandwidth. Developers who are working from home won’t always have optimal network speed to work with, whether it’s unavailable where they live or because they’re competing with other household members for bandwidth. In the latter case, it’s not always as simple as kicking the kids off Netflix; remote developers may live with children using remote learning software or other adults conducting business remotely.&lt;/p&gt; &lt;h3&gt;What about an in-browser IDE?&lt;/h3&gt; &lt;p&gt;The advent of containers has utterly transformed the landscape for application workloads over the last several years. The dazzling metropolis of communities that have formed around the technology has advanced the maturity and consumability of containers for virtually all aspects of IT, including &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized development environments&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;True masters of remote development can serve up workspaces through an in-browser IDE like &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;CodeReady Workspaces&lt;/a&gt;. Centralizing workspace management to an underlying platform like &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; results in workspaces that are powerful and tool-ready enough that all you need on your laptop is a modern browser. This strategy offers better performance over virtual machines because the connection is powered by a web socket, which has lower bandwidth requirements than a remote desktop client. It’s also snappier in that the containers’ shared kernel enables faster start times than virtual machines. As if that's not enough, just think &lt;a href="https://developers.redhat.com/blog/2019/02/18/containerized-python-flask-development-environment-red-hat-codeready-workspaces"&gt;how much easier onboarding a new teammate will be&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you’re curious about the in-browser IDE option, you can try it out yourself using the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, a free shared OpenShift cluster with CodeReady Workspaces preconfigured. You can get started experimenting with it in as little as two minutes.&lt;/p&gt; &lt;h2&gt;Pair programming for remote teams&lt;/h2&gt; &lt;p&gt;The lack of in-person interaction is another remote workplace challenge that uniquely impacts developers. When everyone is working from home, having a colleague look over your shoulder at something you’re working on is more of an inventive metaphor than a literal practice. Sure, there are plenty of video conferencing apps, but they generally don’t deliver a quick and simple pair programming experience.&lt;/p&gt; &lt;p&gt;This is another area where CodeReady Workspaces excels, as it can inherit role-based access controls (RBAC) from OpenShift, permitting a developer to grant read or write access to collaborators in workspaces they own. With OpenShift’s software-defined networking, collaboratively fixing a bug is as easy as generating a link to a workspace, or to a locally-deployed app, and sharing it with others. As more companies embrace an increasingly remote workforce, preserving the rituals of &lt;a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging"&gt;rubber duck debugging&lt;/a&gt; and pair programming are of the utmost importance for long-term success.&lt;/p&gt; &lt;h2&gt;Automated pipelines for CI/CD handoffs&lt;/h2&gt; &lt;p&gt;Working from home deprives workers of all teams from a great many easily observable things, like chance encounters with Greg at the Keurig or quarreling with coworkers over who has to sit in the squeaky swivel chair. But remote work also disrupts something a bit less obvious: &lt;em&gt;continuity&lt;/em&gt;. Workers are prone to more tiny distractions and sequence breaks at home, as it presents a much different atmosphere than an office setting. This doesn’t necessarily reduce how much work people get done, but it makes achieving uninterrupted workstreams more challenging.&lt;/p&gt; &lt;p&gt;The impact this has on developers is the potential to introduce slowdowns into handoffs related to continuous integration and continuous deployment (&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;). When colleagues on ops teams and other stakeholder groups aren’t monitoring incoming workloads contiguously, there’s a greater chance developers will have to wait longer for their code to be deployed. For this reason, it’s more important than ever before to afford developers with self-service capabilities and &lt;a href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines?ts=1633456468158"&gt;automated pipelines&lt;/a&gt; so that teams can keep moving.&lt;/p&gt; &lt;p&gt;Pipelines turn the team build, test, verification, and release process into consistent and repeatable automation. By hooking up pipelines into source control repositories and into team chat, developers and operations (&lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;) can come together in a way that removes barriers and creates a shared awareness with a recorded history. While these benefits directly address the lament of continuity breaks in today’s IT landscape, they’re guaranteed to deliver improved lead times and developer experience regardless of how pandemic conditions unfold in the future.&lt;/p&gt; &lt;h2&gt;Working remotely is the new normal&lt;/h2&gt; &lt;p&gt;It’s widely expected that working remotely will become common for at least some of the workweek (almost no one wants that everyday commute again). If you fast-forward to a year from now, we expect to see successful development teams being the ones that embraced the shift. Those teams will be doing the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Providing shared IDEs to facilitate collaboration.&lt;/li&gt; &lt;li&gt;Establishing a repository of self-service and single-click workspaces for IDE consistency.&lt;/li&gt; &lt;li&gt;Triggering &lt;a href="https://www.redhat.com/en/topics/devops/what-is-ci-cd#ci/cd-tools"&gt;CI/CD pipelines&lt;/a&gt; off of source control to automate away manual operations tasks.&lt;/li&gt; &lt;li&gt;Providing dashboards and ChatOps feedback from the CI/CD for shared awareness.&lt;/li&gt; &lt;li&gt;Leveraging new tooling so "&lt;a href="https://containerjournal.com/topics/container-security/what-will-it-take-to-shift-kubernetes-security-left/"&gt;shifting left on security&lt;/a&gt;" doesn’t burden developers.&lt;/li&gt; &lt;li&gt;Leveraging code quality tools within their CI/CD pipeline to minimize the effort of manual code reviews.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;As your team evolves into remote capable operating models, keeping the status-quo of a local-only development process just won’t cut it. Those environments are a source of friction that will bottleneck your efforts and easily get you off schedule and over cost. Adopting new team culture, defining new ways to collaborate, and bringing in new shared development tools will help you meet the evolution into remote work in a way that becomes your “normal."&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Find out more about CodeReady Workspaces by watching this &lt;a href="https://www.youtube.com/watch?v=fAjx3bXcBZI"&gt;Developer Experience Office Hours video&lt;/a&gt; dedicated to CodeReady Workspaces and Eclipse Che. You can also learn about best practices for developer workspaces in this &lt;a href="https://go.govloop.com/Modern-Developer-Workspaces.html"&gt;short report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Want more insights like these? Follow Andy on GitHub, &lt;a href="https://github.com/andykrohg"&gt;@andykrohg&lt;/a&gt;. Follow Dudash on Twitter (&lt;a href="https://twitter.com/dudashtweets"&gt;@dudashtweets&lt;/a&gt;) or GitHub (&lt;a href="https://github.com/dudash"&gt;@dudash&lt;/a&gt;).&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams" title="Tools and practices for remote development teams"&gt;Tools and practices for remote development teams&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YKSf73GCfog" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jason Dudash, Andy Krohg</dc:creator><dc:date>2021-10-12T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams</feedburner:origLink></entry><entry><title type="html">WildFly Bootable JAR 6.0 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KkHe4krHf40/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/10/12/bootable-jar-6.0.Final-Released/</id><updated>2021-10-12T00:00:00Z</updated><content type="html">The 6.0.0.Final version of the has been released. For people who are not familiar with the WildFly Bootable JAR, I strongly recommend that you read this that covers it in detail. NEW EXAMPLES TO HIGHLIGHT NEW GALLEON LAYERS INTRODUCED IN WILDFLY 25 For each new release of the WildFly Bootable JAR Maven plugin we are upgrading the dependency of the to the latest WildFly release and highlight new features that you can use when building WildFly Bootable JARs. For 6.0.0.Final, we have added two new examples to cover some new Galleon layers that are of particualr importance for Wildfly Bootable JAR packaging. AUTOMATIC GENERATION OF SELF SIGNED CERTIFICATE Up to now, in order to configure HTTPS when using elytron, we had to include a keystore in the Bootable JAR during packaging. During development phases we generally don’t really care of the content of the keystore (and generating one implies extra steps… ). WildFly 25 introduces the undertow-https Galleon layer that adds an elytron secured https-listener that generates a self signed certificate on first connection. The has been evolved with the self-signed profile to provision a server that makes use of the undertow-https Galleon layer. OPENID CONNECT (OIDC) NATIVE SUPPORT Up to now, as highlighted in the , in order to secure a deployment using we had to provision both WildFly Galleon feature-pack and Keycloak OIDC adapter feature-pack. Starting with WildFly 25, we are now relying on the WildFly native OIDC support and are deprecating the use of the Keycloack Galleon feature-pack. WildFly 25 introduces the elytron-oidc-client Galleon layer to provision the elytron-oidc-client subsystem that allows to interact with OIDC compliant authorization servers (such as the server). This native support is very similar to the Keycloack one. Migrating from using Keycloak Galleon adapter to using WildFly OIDC native support when building a Bootable JAR is a matter of: * Removing the Keycloak OIDC adapter feature-pack (org.keycloak:keycloak-adapter-galleon-pack) from the plugin configuration. * Replacing the keycloak-client-oidc Galleon layer by the elytron-oidc-client Galleon layer in the plugin configuration. * Change the &lt;auth-method&gt;KEYCLOAK&lt;/auth-method&gt; to &lt;auth-method&gt;OIDC&lt;/auth-method&gt; in your web.xml file. * If you were using WEB-INF/keycloak.json file you would need to rename it to WEB-INF/oidc.json and update its content by following the . * If you were calling WildFly CLI scripts to update the subsystem configuration, you will need to adjust the management operations. More information can be found in the . The covers the steps required to use this new Galleon layer. TO CONCLUDE Finally we would really appreciate if if you would keep us posted with your feedback and new requirements. (You can log these as new .) This will help us evolve the WildFly Bootable JAR experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KkHe4krHf40" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/12/bootable-jar-6.0.Final-Released/</feedburner:origLink></entry><entry><title>Quarkus for Spring developers: Kubernetes-native design patterns</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zK_cU8Oz0L4/quarkus-spring-developers-kubernetes-native-design-patterns" /><author><name>Eric Deandrea</name></author><id>7089bac4-097e-4e59-a3d1-15b894297609</id><updated>2021-10-11T07:00:00Z</updated><published>2021-10-11T07:00:00Z</published><summary type="html">&lt;p class="Indent1"&gt;Want to learn more about developing applications with Quarkus? Download our free e-book &lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt;, which helps Java developers familiar with Spring make a quick and easy transition.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices/"&gt;Microservice&lt;/a&gt; applications designed today are often deployed on a platform such as &lt;a href="https://developers.redhat.com/topics/kubernetes//"&gt;Kubernetes&lt;/a&gt;. This platform can orchestrate the deployment and management of &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containerized&lt;/a&gt; microservices. Microservices development calls for sophisticated patterns, such as health checks, fault tolerance, load balancing, distributed tracing, and remote debugging and development. Because of this, it is essential to adopt technologies and frameworks that support these patterns while also providing a great developer experience.&lt;/p&gt; &lt;p&gt;This article will discuss some of these patterns and showcase why &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; is ideal for Kubernetes-native &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications.&lt;/p&gt; &lt;h2&gt;Challenges of Java in containers&lt;/h2&gt; &lt;p&gt;Some challenges of Java in the cloud, container, and Kubernetes world stem from Java's tendency to consume lots of memory and to suffer slower startup times than other languages and runtimes.&lt;/p&gt; &lt;p&gt;The immutable nature of containers forces rethinking the design and maintenance of Java applications. Highly reconfigurable and long-running applications with large heap sizes are no longer necessary. Moreover, essential parts of an application, such as the database driver or logging framework, are known in advance. Therefore it is unnecessary to use frameworks that rely heavily on runtime bindings or &lt;a href="https://www.oracle.com/technical-resources/articles/java/javareflection.html"&gt;Java reflection&lt;/a&gt;. Instead, application boot times can be accelerated and runtime footprints reduced by performing as much work at build time as possible.&lt;/p&gt; &lt;p&gt;Many Java-based frameworks embrace the dynamic flexibility Java provides. A lot happens in the first few seconds of Java application startup. For example, a &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring&lt;/a&gt; application can alter its runtime behavior by changing a few configuration properties. The dynamic runtime binding allowed by Java enables this adaptability. These frameworks need to rethink their underlying architectures in order to fit into today's architectures.&lt;/p&gt; &lt;p&gt;This is the situation Spring finds itself in today. By contast, Quarkus was designed with these characteristics in mind. Quarkus offers near-instant scale-up and high-density utilization in container orchestration platforms such as Kubernetes. You can run many more application instances within the same hardware resources. From the beginning, Quarkus was designed around container-first and Kubernetes-native philosophies, optimizing for low memory usage and fast startup times. Quarkus design principles reduce the size and, ultimately, the memory footprint of the application running on the JVM while also enabling Quarkus to be “natively native.” Quarkus's design accounted for native compilation from the onset.&lt;/p&gt; &lt;h2&gt;Preparing applications for Kubernetes&lt;/h2&gt; &lt;p&gt;Preparing an application to run as a container within Kubernetes requires numerous steps that aren't required for local development and deployment. It also requires tools, concepts, and configurations that might be new to developers.&lt;/p&gt; &lt;h3&gt;Building a container image&lt;/h3&gt; &lt;p&gt;Deploying an application on Kubernetes requires packaging an application's binary and resources as a container image. Quarkus contains out-of-the-box extensions for building container images, as described in Table 1.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1"&gt;&lt;caption&gt;Table 1: Quarkus extensions for building container images.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;Extension&lt;/th&gt; &lt;th scope="col"&gt;Description&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://quarkus.io/guides/container-image#jib"&gt;Quarkus Jib extension&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Powered by &lt;a href="https://github.com/GoogleContainerTools/jib"&gt;Google Jib&lt;/a&gt; for performing container image builds. The major benefit of using Jib with Quarkus is that all the dependencies are cached in a different layer than the application. This caching makes rebuilding images fast and small when pushing the image to a repository. Additionally, this extension provides the ability to create a container image and push it to a registry without any dedicated client-side tooling or a running daemon process.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://quarkus.io/guides/container-image#docker"&gt;Quarkus Docker extension&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Uses the Docker API to build a container image from one of the &lt;code&gt;Dockerfile&lt;/code&gt;s in the &lt;code&gt;src/main/docker&lt;/code&gt; directory. &lt;a href="https://code.quarkus.io"&gt;Code Quarkus&lt;/a&gt; can generate examples. The resulting image is hosted in the local Docker image repository.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://quarkus.io/guides/container-image#s2i"&gt;Quarkus S2I extension&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Uses &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;Red Hat OpenShift&lt;/a&gt;'s &lt;a href="https://docs.openshift.com/container-platform/4.8/openshift_images/create-images.html#images-create-s2i-build_create-images"&gt;Source-to-Image&lt;/a&gt; (S2I) build process to build a container image using a binary build. The Quarkus build process will build the application binary and use OpenShift to build the container image from the binary. The resulting image is hosted within the OpenShift container registry.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Building a container image is as simple as running &lt;code&gt;./mvnw package -Dquarkus.container-image.build=true&lt;/code&gt; once one of these extensions is added to an application. Pushing a container image to a configured registry is also as simple as adding the &lt;code&gt;-Dquarkus.container-image.push=true&lt;/code&gt; flag to the command. There are also many &lt;a href="https://quarkus.io/guides/container-image#customizing"&gt;configuration options available&lt;/a&gt; for each of these extensions.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; These flags can also be added to the &lt;code&gt;src/main/resources/application.properties&lt;/code&gt; file rather than specified on the command line.&lt;/p&gt; &lt;h3&gt;Kubernetes manifests&lt;/h3&gt; &lt;p&gt;Once a container image is created, deploying an application on Kubernetes requires the creation and deployment of &lt;a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-manifest"&gt;manifests&lt;/a&gt;. A Kubernetes manifest is a resource file described using YAML or JSON format. Many types of resources can be deployed, some of which are described in Table 2.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1"&gt;&lt;caption&gt;Table 2: Common Kubernetes manifest types.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;Type&lt;/th&gt; &lt;th scope="col"&gt;Description&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Manages the application, resources, volumes, properties, environment variables, replicas, etc., to be deployed.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;&lt;code&gt;Service&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Endpoint providing access to the application port of the &lt;code&gt;Pod&lt;/code&gt;s hosting an application. The &lt;code&gt;Service&lt;/code&gt; performs load balancing on incoming requests to &lt;code&gt;Pod&lt;/code&gt;s.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;&lt;code&gt;Namespace&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Virtual space isolating &lt;code&gt;Pod&lt;/code&gt;s.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;&lt;code&gt;Ingress&lt;/code&gt;&lt;/a&gt; (Kubernetes)&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openshift.com/container-platform/4.8/networking/routes/route-configuration.html"&gt;&lt;code&gt;Route&lt;/code&gt;&lt;/a&gt; (OpenShift)&lt;/p&gt; &lt;/td&gt; &lt;td&gt;Manages external access to the &lt;code&gt;Service&lt;/code&gt;.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/"&gt;&lt;code&gt;ConfigMap&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Store nonconfidential data in key-value pairs.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;&lt;code&gt;Secret&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Store sensitive data, such as a password, token, or key.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes"&gt;Quarkus Kubernetes extension&lt;/a&gt; (or the &lt;a href="https://quarkus.io/guides/deploying-to-openshift"&gt;Quarkus OpenShift extension&lt;/a&gt;, if using &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;) frees developers from manually creating the required Kubernetes manifests by automatically creating them during the build process. The generated resources can be customized further using well-defined properties. This type of developer experience doesn't exist in Spring Boot directly. You need to integrate the &lt;a href="https://github.com/dekorateio/dekorate#spring-boot"&gt;Dekorate Spring Boot library&lt;/a&gt; into your project manually.&lt;/p&gt; &lt;p&gt;Want to run your application serverless? The Kubernetes (or OpenShift) extension can also &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes#knative"&gt;generate Knative resources&lt;/a&gt;, enabling you to deploy your application in a serverless fashion, whether it's on the JVM or a native image.&lt;/p&gt; &lt;p&gt;Creating these manifests is as simple as running &lt;code&gt;./mvnw clean package&lt;/code&gt; once one of these extensions is added to an application. All the manifests in both YAML and JSON format will be located in the &lt;code&gt;/target/kubernetes&lt;/code&gt; directory. There are also many &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes#tuning-the-generated-resources-using-application-properties"&gt;configuration options available&lt;/a&gt; for each of these extensions. Deploying an application to Kubernetes directly is just as easy as running &lt;code&gt;./mvnw clean package -Dquarkus.kubernetes.deploy=true&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Developing applications on Kubernetes&lt;/h2&gt; &lt;p&gt;The development process is faster and more efficient with &lt;a href="https://quarkus.io/guides/maven-tooling#dev-mode"&gt;Quarkus live coding&lt;/a&gt;. Quarkus automatically detects changes within a project and transparently recompiles and redeploys the changes, making them visible typically in under a second. You can continuously write code and have the underlying platform seamlessly incorporate the changes into the running application.&lt;/p&gt; &lt;p&gt;In many instances, it might not be feasible, or even possible, to run necessary dependent or downstream services on a local developer workstation. There are limited options in such instances without Quarkus. One option is to continuously rebuild, redeploy, and retest the application each time a change is made. Another option involves lots of tedious port mapping and forwarding between the local workstation and the remote environment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/02/11/enhancing-the-development-loop-with-quarkus-remote-development#step_2__live_coding_in_your_local_environment"&gt;Quarkus Remote Dev Mode&lt;/a&gt; solves this challenge by extending live coding to push changes to a remote Quarkus instance, as shown in Figure 1. Quarkus Remote Dev is built into the Quarkus platform and does not require any IDE tooling or plug-ins. This capability greatly reduces the inner feedback loop while alleviating the "works on my machine" problem. It also allows for quick and easy prototyping of new features and capabilities.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Figure 1: Quarkus Remote Dev Mode." data-entity-type="file" data-entity-uuid="1114c7bf-b257-4db0-92bf-bec336e10fcd" src="https://developers.redhat.com/sites/default/files/inline-images/figure-1.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Quarkus Remote Dev Mode.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Operating applications on Kubernetes&lt;/h2&gt; &lt;p&gt;The composition of distributed applications communicating with each other introduces its own set of challenges. New and modern applications need to be designed around scalable distributed patterns to deal with these challenges. Users today expect near-instant response times and 100% uptime. The proliferation of distributed applications communicating to fulfill a user's request goes directly against these expectations.&lt;/p&gt; &lt;p&gt;Luckily, a set of patterns in the microservices world have evolved, helping to deal with some of these challenges.&lt;/p&gt; &lt;h3&gt;Health Check&lt;/h3&gt; &lt;p&gt;Kubernetes has &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"&gt;built-in probes&lt;/a&gt; for determining whether or not a &lt;code&gt;Pod&lt;/code&gt; is alive and ready to serve incoming requests. This is known as the Health Check pattern, which is is supported in Quarkus using the &lt;a href="https://quarkus.io/guides/smallrye-health"&gt;SmallRye Health extension&lt;/a&gt;. The SmallRye Health extension is an implementation of the &lt;a href="https://download.eclipse.org/microprofile/microprofile-health-3.1/microprofile-health-spec-3.1.html"&gt;MicroProfile Health specification&lt;/a&gt;. Spring Boot applications can use the &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.endpoints.kubernetes-probes"&gt;Kubernetes probes included in the Spring Boot Actuator&lt;/a&gt; starter; however, the Spring Boot Actuator only provides the endpoints and doesn't generate the necessary configuration or the required Kubernetes manifests. They will need to be created manually.&lt;/p&gt; &lt;h3&gt;Configuration management&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Configuration management&lt;/em&gt; refers to the management of properties and resources needed to configure an application. This information is usually injected into a container at runtime rather than included in the image at build time. The Kubernetes &lt;code&gt;Deployment&lt;/code&gt; resource contains information on how this injection would take place.&lt;/p&gt; &lt;p&gt;In Quarkus, the configuration can be injected in several ways, including system properties, environment variables, and an &lt;code&gt;application.properties&lt;/code&gt; file (or &lt;code&gt;application.yml&lt;/code&gt; file, if using the &lt;a href="https://quarkus.io/guides/config-yaml"&gt;Quarkus YAML extension&lt;/a&gt;). The Quarkus Kubernetes extension can be configured to generate this information when generating the Kubernetes manifests. Additionally, the &lt;a href="https://quarkus.io/guides/kubernetes-config"&gt;Quarkus Kubernetes Config extension&lt;/a&gt; can be used to read Kubernetes &lt;code&gt;ConfigMap&lt;/code&gt;s and &lt;code&gt;Secret&lt;/code&gt;s directly as configuration sources without mounting them into the &lt;code&gt;Pod&lt;/code&gt; running the application.&lt;/p&gt; &lt;p&gt;Both Quarkus and Spring can use the &lt;a href="https://cloud.spring.io/spring-cloud-config"&gt;Spring Cloud Config Server&lt;/a&gt; (SCCS) for centralized configuration management. The &lt;a href="https://quarkus.io/guides/spring-cloud-config-client"&gt;Quarkus Spring Cloud Config Client extension&lt;/a&gt; can read configuration properties from SCCS at application startup.&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;Both Quarkus and Spring use the &lt;a href="https://micrometer.io/"&gt;Micrometer metrics library&lt;/a&gt; for collecting metrics. Micrometer provides a vendor-neutral interface for registering dimensional metrics and metric types, such as counters, gauges, timers, and distribution summaries. These core types are an abstraction layer that can be adapted and delegated to specific implementations, such as &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/guides/micrometer#review-automatically-generated-metrics"&gt;Quarkus Micrometer extension&lt;/a&gt; automatically times all HTTP server requests. Other Quarkus extensions also automatically add their own metrics collections. Applications can add their own custom metrics as well. Quarkus will automatically export the metrics on the &lt;code&gt;/q/metrics&lt;/code&gt; endpoint. In Spring, Micrometer is &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#actuator.metrics"&gt;supported by the Spring Boot Actuator starter&lt;/a&gt; if the Micrometer libraries are included on the application's classpath.&lt;/p&gt; &lt;h3&gt;Distributed tracing&lt;/h3&gt; &lt;p&gt;A trace tracks the progress of a single request as it passes through services. Distributed tracing is a form of tracing that traverses process, network, and security boundaries. Each unit of work is called a &lt;em&gt;span&lt;/em&gt;. A &lt;em&gt;trace&lt;/em&gt; is a tree of spans. Think of a distributed trace like a Java stack trace, capturing every component of a system that a request flows through, while also tracking the amount of time spent in and between each component.&lt;/p&gt; &lt;p&gt;All Quarkus REST endpoints are automatically traced when using the &lt;a href="https://quarkus.io/guides/opentracing"&gt;Quarkus OpenTracing extension&lt;/a&gt;. The extension also allows custom traces to be added to an application. &lt;a href="https://quarkus.io/guides/opentracing#additional-instrumentation"&gt;Additional instrumentation&lt;/a&gt; can be added to trace all JDBC, Kafka, and MongoDB interactions. Samplers can then be configured to export some percentage of traces to a &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; collector.&lt;/p&gt; &lt;p&gt;The community in general is moving towards the &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt; observability framework. OpenTelemetry will most likely supersede OpenTracing at some point in the future. &lt;a href="https://quarkus.io/guides/opentelemetry"&gt;Quarkus has an extension for OpenTelemetry&lt;/a&gt;. Spring support is still incubating as part of the &lt;a href="https://github.com/spring-cloud-incubator/spring-cloud-sleuth-otel"&gt;Spring Cloud Sleuth OTel project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;There are many free resources available for learning about and getting started with Quarkus. Why wait for the future? Since its inception in 2019 and continuing today and into the future, Quarkus has provided a familiar and innovative framework for Java developers, supporting capabilities developers need and want today.&lt;/p&gt; &lt;p&gt;Check out these available resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Read &lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; to learn about what challenges led to Quarkus and see side-by-side examples of familiar Spring concepts, constructs, and conventions.&lt;/li&gt; &lt;li&gt;Learn &lt;a href="https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices"&gt;why you should choose Quarkus over Spring&lt;/a&gt; for microservices development.&lt;/li&gt; &lt;li&gt;Check out why organizations believe the &lt;a href="https://youtu.be/JpafbcOFIGI"&gt;developer experience is just as important as the product&lt;/a&gt; itself.&lt;/li&gt; &lt;li&gt;Explore &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Quarkus quick starts&lt;/a&gt; in the &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for experimenting with containerized applications.&lt;/li&gt; &lt;li&gt;Try free &lt;a href="https://learn.openshift.com/developing-with-quarkus/"&gt;15-minute interactive learning scenarios&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/get-started/"&gt;Get started with Quarkus&lt;/a&gt; on your own.&lt;/li&gt; &lt;li&gt;Learn about Quarkus's &lt;a href="https://quarkus.io/guides/spring-di#more-spring-guides"&gt;Spring compatibility features&lt;/a&gt;. &lt;ul&gt;&lt;li&gt;In some instances, there might not be any code changes needed to &lt;a href="https://developers.redhat.com/blog/2021/02/09/spring-boot-on-quarkus-magic-or-madness/"&gt;run a Spring application on Quarkus&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get &lt;a href="https://github.com/RedHat-Middleware-Workshops/spring-to-quarkus-todo"&gt;hands-on converting a Spring Boot application to Quarkus&lt;/a&gt; with little-to-no code changes.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Attend a &lt;a href="https://quarkus.io/worldtour/"&gt;Quarkus World Tour&lt;/a&gt; event when it comes to a city near you. You can even request a private stop.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/11/quarkus-spring-developers-kubernetes-native-design-patterns" title="Quarkus for Spring developers: Kubernetes-native design patterns"&gt;Quarkus for Spring developers: Kubernetes-native design patterns&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zK_cU8Oz0L4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2021-10-11T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/11/quarkus-spring-developers-kubernetes-native-design-patterns</feedburner:origLink></entry><entry><title type="html">Codeanywhere adventures - Getting started with developer process automation tooling (part 3)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zzjAMjL4Zho/codeanywhere-adventures-getting-started-with-developer-process-automation-tooling-part3.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/qY7a9VB1Wds/codeanywhere-adventures-getting-started-with-developer-process-automation-tooling-part3.html</id><updated>2021-10-11T05:00:00Z</updated><content type="html">In the , we introduced the world of , a cloud IDE and container development experience all available in just your browser.  Are you ready for some more amazing, easy to use, developer tooling that requires not a single tooling installation and no configuration?  That's what the team at  are promising us when I stumbled on their website last week. They "...don't require you to engage in complex installations and configuration setups. Simply access our in-browser IDE for everything you need to build amazing websites in a productive and more developer-friendly way." In part three of this series, we'll finish the setup of our process automation Java container project in . From , you should be logged in and in your to get started. From there you can find your PROCESS AUTOMATION TOOLING container listing and use the OPEN IDE button to create a tab with the  IDE and your project. You see the project is already in the IDE and automatically recognised by the EXPLORER view of our workspace. If we view the READM in this project we'll see that there are setup steps that install it on a local machine or you have the option to install it in a container using Podman on your local machine. As we are using the cloud IDE for this, we are going to consider our container project the local machine and use those instructions. Having already cloned the project to our IDE, we then see that it's going to require a few Red Hat products that we can pull from the Red Hat Developers site. The products we need to install process automation tooling are listed in the installs/README file and include: * JBoss EAP 7.3.0 (jboss-eap-7.3.0.zip) * Red Hat Process Automation Manager 7.11 deployable (rhpam-7.11.0-business-central-eap7-deployable.zip) * Red Hat PAM KIE server 7.11 (rhpam-7.11.0-kie-server-ee8.zip) * Red Hat PAM 7.11 add ons (rhpam-7.11.0-add-ons.zip) We can pull these into the project by finding the , needing only to have a registered user to access them. Locate the correct JBoss EAP 7.3.0 and three Red Hat Process Automation Manager downloads, and pull them to your local machine by clicking on their download buttons.  After that we need to push them with secure copy to the project installs directory in our container project. This is done by finding out what user I am on this container in the cloud IDE console: $ whoami cabox Now assuming you are in the same directory as the downloaded product files on your local machine, from a console run the following to copy the files to your container project installs directory: $ scp -P 31828 jboss-eap-7.3.0.zip cabox@host31.codeanyhost.com:~/workspace/rhpam-install-demo/installs/ cabox@host31.codeanyhost.com: Permission denied (publickey). lost connection This means we do not have access until we share our public key with the hosting container. Generating SSH keys is beyond the scope of this article, but assuming you have one, copy and past it into the file via your cloud IDE console in the file ~/.ssh/authorized_keys.  Once that's done you can now again try to copy the file securely and should see successful results so copy over all four files: $ scp -P 31828 *.zip cabox@host31.codeanyhost.com:~/workspace/rhpam-install-demo/installs/ At this point, you're ready to install the process automation developer tooling, so in your  IDE console make sure you're in the root directory of the rhpam-install-demo project and run the following: $ cd $HOME/workspace/rhpam-install-demo; ./init.sh You should see the installation script run and end with the login details... but we have to remember we are using the  IDE container project and refer back to the GETING STARTED page that was opened originally that included special URL's to access our applications. Before we start the server, note that there is a line in the GETTING STARTED page in our IDE project that stated "To access your web application make sure your application server is running and listening on 0.0.0.0 address..." This indicates that the normal setup for a JBoss EAP server needs to be adjusted as it's default configuration is to listen on 127.0.0.1 or localhost.  Open the file rhpam-install-demo/target/jboss-eap-7.3/standalone/configuration/standalone.xml and change all instances of 127.0.0.1 to 0.0.0.0, then close and save the file. Now we're ready to start the process automation tooling with the following command: $ ./target/jboss-eap-7.3/bin/standalone.sh We have to wait on the complete startup of the server, and then instead of localhost:8080 we need to replace the presented URL with the GETTING STARTED suggestions. This means we should find our process automation tooling on the following handy links that are provided as the server starts listening to ports (note you can find these under the PREVIEW PORTS link at the bottom right of the IDE): While the OPEN BROWSER links for port 8080 does give us the JBoss EAP server admin console, we need to add a forward slash with the business central application to reach the process automation tooling log in. https://port-8080-process-automation-tooling--eric863427.preview.codeanywhere.com/business-central Follow the rest of the project README file to find out how to log in to the tooling and for links to workshops that help you get started with developing your first real process automation project. This completes part three of the  adventures, where we installed, deployed, and accessed developer process automation tooling as a cloud IDE container experience.  Next up, part four takes us through the installation and deployment of decision management tooling for developers.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zzjAMjL4Zho" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/qY7a9VB1Wds/codeanywhere-adventures-getting-started-with-developer-process-automation-tooling-part3.html</feedburner:origLink></entry><entry><title>Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now in beta</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WxZuqDDPpdQ/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-beta" /><author><name>Brian Gollaher</name></author><id>d2d93515-451b-4bee-9b9e-bd98bedf27a5</id><updated>2021-10-11T03:00:00Z</updated><published>2021-10-11T03:00:00Z</published><summary type="html">&lt;p&gt;The latest versions of &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/developertoolset/overview"&gt;Red Hat Developer Toolset&lt;/a&gt; for Red Hat Enterprise Linux 7 are available now in beta. Red Hat Software Collections 3.8 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, the world’s leading enterprise Linux platform. These components are supported for up to five years, helping to enable a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;h2&gt;Updates to Red Hat Software Collections&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Nginx 1.20:&lt;/strong&gt; A release of this web and proxy server with a focus on high concurrency, performance, and low memory usage. This version supports client SSL certificate validation with the Online Certificate Status Protocol (OCSP) and improved support for HTTP/2.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Redis 6:&lt;/strong&gt; A new release of this persistent key-value database. Redis 6 now supports SSL on all channels, access control lists (ACLs), and Redis Serialization Protocol version 3.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JDK Mission Control (JMC) 8.0.1 (update):&lt;/strong&gt; An advanced set of tools for managing, monitoring, profiling, and troubleshooting &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications. This update to JMC 8 delivers a number of important bug and security fixes.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;New collections in Red Hat Software Collections 3.8 are also available as &lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;Red Hat Certified Containers&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy mission-critical applications using the supported components of Red Hat Software Collections for Red Hat Enterprise Linux and &lt;a href="products/openshift"&gt;Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;h2&gt;Updates to Red Hat Developer Toolset&lt;/h2&gt; &lt;p&gt;Also new in Red Hat Software Collections 3.8 is Red Hat Developer Toolset 11, an updated, curated collection of compilers, toolchains, debuggers, and other critical development tools. The foundation of Developer Toolset 11 is GCC 11.2, a new update of the popular free software compiler collection. Additional updates in Developer Toolset 11 deliver new features to debugging and performance tools for &lt;a href="https://developers.redhat.com/topics/c/"&gt;C/C++&lt;/a&gt; and Fortran.&lt;/p&gt; &lt;p&gt;In addition to the Developer Toolset, this release updates other compiler toolsets in Red Hat Enterprise Linux devtools. Go Toolset is updated to version 1.16, LLVM Toolset to version 12.0, and Rust Toolset to version 1.54.&lt;/p&gt; &lt;h2&gt;Other enhancements&lt;/h2&gt; &lt;p&gt;Red Hat Software Collections 3.8 continues Red Hat’s commitment to customer choice in underlying compute architectures, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;p&gt;Red Hat customers with active &lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Red Hat Software Collections via the &lt;a href="https://access.redhat.com/solutions/472793"&gt;Red Hat Software Collections repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information, please read the full &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3-beta/"&gt;beta release notes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/11/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-beta" title="Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now in beta"&gt;Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now in beta&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WxZuqDDPpdQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-10-11T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/11/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-beta</feedburner:origLink></entry></feed>
