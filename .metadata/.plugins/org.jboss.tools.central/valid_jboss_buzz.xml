<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Accessing Apache Kafka in Strimzi: Part 2 – Node ports</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bAFtnczHL0k/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="jboss a-mq" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Jakub Scholz</name></author><id>searchisko:content:id:jbossorg_blog-accessing_apache_kafka_in_strimzi_part_2_node_ports</id><updated>2019-06-07T07:00:20Z</updated><published>2019-06-07T07:00:20Z</published><content type="html">&lt;p&gt;This article series explains how &lt;a href="https://developers.redhat.com/videos/youtube/CZhOJ_ysIiI/"&gt;Apache Kafka&lt;/a&gt; and its clients work and how &lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt; makes it accessible for clients running outside of &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. In the first article, we provided an &lt;a href="https://developers.redhat.com/blog/?p=601077"&gt;introduction to the topic&lt;/a&gt;, and here we will look at exposing an Apache Kafka cluster managed by Strimzi using node ports.&lt;/p&gt; &lt;p&gt;Specifically, in this article, we&amp;#8217;ll look at how node ports work and how they can be used with Kafka. We also will cover the different configuration options available to users and the pros and cons of using node ports.&lt;span id="more-601137"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Productized and supported versions of the Strimzi and Apache Kafka projects are available as part of the &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/amq"&gt;Red Hat AMQ&lt;/a&gt; product.&lt;/p&gt; &lt;h2&gt;Node ports&lt;/h2&gt; &lt;p&gt;A &lt;code&gt;NodePort&lt;/code&gt; is a special type of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport" rel="nofollow"&gt;Kubernetes service&lt;/a&gt;. When such a service is created, Kubernetes will allocate a port on all nodes of the Kubernetes cluster and make sure that all traffic to this port is routed to the service and eventually to the pods behind this service.&lt;/p&gt; &lt;p&gt;The routing of the traffic is done by the kube-proxy Kubernetes component. It doesn&amp;#8217;t matter which node your pod is running on. The node ports will be open on all nodes, and the traffic will always reach your pod. So, your clients need to connect to the node port on any of the nodes of the Kubernetes cluster and let Kubernetes handle the rest.&lt;/p&gt; &lt;p&gt;The node port is selected from the port range 30000-32767 by default, but this range can be changed in Kubernetes configuration (see &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport" rel="nofollow"&gt;Kubernetes docs&lt;/a&gt; for more details about configuring the node port range).&lt;/p&gt; &lt;p&gt;How do we use &lt;code&gt;NodePort&lt;/code&gt; services in Strimzi to expose Apache Kafka?&lt;/p&gt; &lt;h2&gt;Exposing Kafka using node ports&lt;/h2&gt; &lt;p&gt;As a user, you can easily expose Kafka using node ports. All you need to do is to configure it in the Kafka custom resource.&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-cluster spec: kafka: # ... listeners: # ... external: type: nodeport tls: false # ... &lt;/pre&gt; &lt;p&gt;What happens after you configure it, however, is a bit more complicated.&lt;/p&gt; &lt;p&gt;The first thing we need to address is how the clients will access individual brokers. As explained in the previous article, having a service that will round-robin across all brokers in the cluster will not work with Kafka. The clients need to be able to reach each of the brokers directly.&lt;/p&gt; &lt;p&gt;Inside the Kubernetes cluster, we addressed this by using the pod DNS names as the advertised addresses. The pod hostnames or IP addresses are not recognized outside of Kubernetes, however, so we cannot use them. How does Strimzi solve this?&lt;/p&gt; &lt;p&gt;Instead, of using the pod hostnames or IP addresses, we create additional services—one for each Kafka broker. So, in a Kafka cluster with N brokers, we will have N+1 node port services:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One can be used by the Kafka clients as the bootstrap service for the initial connection and for receiving the metadata about the Kafka cluster.&lt;/li&gt; &lt;li&gt;Another N services—one for each broker—can address the brokers directly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of these services are created with the type &lt;code&gt;NodePort&lt;/code&gt;. Each of these services will be assigned different node port so that the traffic for the different brokers can be distinguished.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-604167 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Nodeport-access.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Nodeport-access.png" alt="Strimzi-Nodeport access" width="678" height="424" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Nodeport-access.png 678w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Nodeport-access-300x188.png 300w" sizes="(max-width: 678px) 100vw, 678px" /&gt;&lt;/p&gt; &lt;p&gt;Since Kubernetes 1.9, every pod in a stateful set is automatically labeled with &lt;code&gt;statefulset.kubernetes.io/pod-name&lt;/code&gt;, which contains the name of the pod. Using this label in the pod selector inside the Kubernetes service definition allows us to target only the individual Kafka brokers and not the whole Kafka cluster.&lt;/p&gt; &lt;p&gt;The following YAML snippet shows how the service created by Strimzi targets just one pod from the stateful set by using the &lt;code&gt;statefulset.kubernetes.io/pod-name&lt;/code&gt; label in the selector:&lt;/p&gt; &lt;pre&gt;apiVersion: v1 kind: Service metadata: name: my-cluster-kafka-0 # ... spec: # ... selector: statefulset.kubernetes.io/pod-name: my-cluster-kafka-0 strimzi.io/cluster: my-cluster strimzi.io/kind: Kafka strimzi.io/name: my-cluster-kafka type: NodePort # ... &lt;/pre&gt; &lt;p&gt;The node port services are just the infrastructure that can route the traffic to the brokers. We still need to configure the Kafka brokers to advertise the right address, so that the clients use this infrastructure. With node ports, the client connecting to the broker needs to connect to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the address of one of the Kubernetes nodes&lt;/li&gt; &lt;li&gt;the node port assigned to the service&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Strimzi needs to gather these and configure these as the advertised addresses in the broker configuration. Strimzi uses separate listeners for external and internal access. That means any applications running inside the Kubernetes or Red Hat OpenShift cluster will still use the old services and DNS names as described in the first article.&lt;/p&gt; &lt;p&gt;Although node port services can route the traffic to the broker from all Kubernetes nodes, we can use only a single address, which will be advertised to the clients. Using the address of the actual node where the broker is running will mean less forwarding, but every time the broker restarts, the node might change. Therefore, Strimzi uses an &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" rel="nofollow"&gt;init container&lt;/a&gt;, which is run every time the Kafka broker pod starts. It collects the address of the node and uses it to configure the advertised address.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-604177 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Init-containers.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Init-containers.png" alt="Strimzi Init containers" width="530" height="368" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Init-containers.png 530w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Init-containers-300x208.png 300w" sizes="(max-width: 530px) 100vw, 530px" /&gt;&lt;/p&gt; &lt;p&gt;To get the node address, the init container must talk with the Kubernetes API and get the node resource. In the status of the node resource, the address is normally listed as one of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;External DNS&lt;/li&gt; &lt;li&gt;External IP&lt;/li&gt; &lt;li&gt;Internal DNS&lt;/li&gt; &lt;li&gt;Internal IP&lt;/li&gt; &lt;li&gt;Hostname&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sometimes, only some of these are listed in the status. The init container will try to get one of them in the same order as they are listed above and will use the first one it finds.&lt;/p&gt; &lt;p&gt;Once the address is configured, the client can use the bootstrap node port service to make the initial connection. From there, the client will get the metadata containing the addresses of the individual brokers and start sending and receiving messages.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-604187 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Client-Connecting.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Client-Connecting.png" alt="Strimzi Client Connecting" width="605" height="520" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Client-Connecting.png 605w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Client-Connecting-300x258.png 300w" sizes="(max-width: 605px) 100vw, 605px" /&gt;&lt;/p&gt; &lt;p&gt;After Strimzi configures everything inside the Kubernetes and Kafka clusters, you need to do just two things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get the node port number of the external bootstrap service (replace &lt;code&gt;my-cluster&lt;/code&gt; with the name of your cluster):&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;kubectl get service my-cluster-kafka-external-bootstrap -o=jsonpath='{.spec.ports[0].nodePort}{"\n"}' &lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Get the address of one of the nodes in your Kubernetes cluster (replace &lt;code&gt;node-name&lt;/code&gt; with the name of one of your nodes &amp;#8211; use &lt;code&gt;kubectl get nodes&lt;/code&gt; to list all nodes):&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;kubectl get node node-name -o=jsonpath='{range .status.addresses[*]}{.type}{"\t"}{.address}{"\n"}' &lt;/pre&gt; &lt;p&gt;The node address and node port number give you all that is needed to connect to your cluster. The following example uses the &lt;code&gt;kafka-console-producer.sh&lt;/code&gt; utility, which is part of Apache Kafka:&lt;/p&gt; &lt;pre&gt;bin/kafka-console-producer.sh --broker-list : --topic &lt;/pre&gt; &lt;p&gt;For more details, see the &lt;a href="https://strimzi.io/docs/latest/full.html#proc-accessing-kafka-using-loadbalancers-deployment-configuration-kafka" rel="nofollow"&gt;Strimzi documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Troubleshooting node ports&lt;/h2&gt; &lt;p&gt;Node ports are fairly complex to configure. There are many things which can go wrong.&lt;/p&gt; &lt;p&gt;A common problem is that the address presented to Strimzi by the Kubernetes API in the node resource is not accessible from outside. This can happen, for example, because the DNS name or IP address used is only internal and cannot be reached by the client. This problem can happen with production-grade clusters as well as local development tools, such as Minikube or Minishift. In such cases, you might get the following errors from your client:&lt;/p&gt; &lt;pre&gt;[2019-04-22 21:04:11,976] WARN [Consumer clientId=consumer-1, groupId=console-consumer-42133] Connection to node 1 (/10.0.2.15:31301) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) &lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt;[2019-04-22 21:11:37,295] WARN [Producer clientId=console-producer] Connection to node -1 (/10.0.2.15:31488) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) &lt;/pre&gt; &lt;p&gt;When you see one of these errors, you can compare the following addresses:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The address under which you expect your nodes to be reachable by your Kafka clients (with Minikube, that should be the IP address returned by the &lt;code&gt;minikube ip&lt;/code&gt; command).&lt;/li&gt; &lt;li&gt;The address of the nodes where the Kafka pods are running&lt;/li&gt; &lt;/ul&gt; &lt;pre class="brush: bash; light: true; title: ; notranslate"&gt; kubectl get node &amp;#60;node-name&amp;#62; -o=jsonpath='{range .status.addresses[*]}{.type}{&amp;#34;\t&amp;#34;}{.address}{&amp;#34;\n&amp;#34;}' &lt;/pre&gt; &lt;ul&gt; &lt;li&gt;The address advertised by the Kafka broker&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;kubectl exec my-cluster-kafka-0 -c kafka -it -- cat /tmp/strimzi.properties | grep advertised &lt;/pre&gt; &lt;p&gt;If these addresses are different, they are probably causing the problem, but Strimzi can help. You can use the override options to change the addresses advertised by the Kafka pods. Instead of reading the address from the Kubernetes API from the node resources, you can configure them in the &lt;code&gt;Kafka&lt;/code&gt; custom resource. For example:&lt;/p&gt; &lt;pre&gt;# ... listeners: external: type: nodeport tls: false overrides: brokers: - broker: 0 advertisedHost: XXX.XXX.XXX.XXX - broker: 1 advertisedHost: XXX.XXX.XXX.XXX - broker: 2 advertisedHost: XXX.XXX.XXX.XXX # ... &lt;/pre&gt; &lt;p&gt;The overrides can specify either DNS names or IP addresses. This solution is not ideal, because you need to maintain the addresses in the custom resource and remember to update them every time you are, for example, scaling up your Kafka cluster. In many cases, however, you might not be able to change the addresses reported by the Kubernetes APIs. So, this approach at least gives you a way to get it working.&lt;/p&gt; &lt;p&gt;Another thing which might make using node ports complicated is the presence of a firewall. If your client cannot connect, you should check whether Kubernetes node and the port are reachable using simple tools such as &lt;code&gt;telnet&lt;/code&gt; or &lt;code&gt;ping&lt;/code&gt;. In public clouds, such as Amazon AWS, you will also need to enable access to the nodes/node ports in the security groups.&lt;/p&gt; &lt;h2&gt;TLS support&lt;/h2&gt; &lt;p&gt;Strimzi supports TLS when exposing Kafka using node ports. For historical reasons, TLS encryption is enabled by default, but you can disable it if you want.&lt;/p&gt; &lt;pre&gt;# ... listeners: external: type: nodeport tls: false # ... &lt;/pre&gt; &lt;p&gt;When exposing Kafka using node ports with TLS, Strimzi currently doesn&amp;#8217;t support TLS hostname verification. The main reason is that, with node ports, it is hard to pin down the addresses that will be used and add them to the TLS certificates. This is mainly because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The node where the broker runs may change every time the pod or node is restarted.&lt;/li&gt; &lt;li&gt;The nodes in the cluster might sometimes change frequently, and we would need to refresh the TLS certificates every time nodes are added or removed and addresses change.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Customizations&lt;/h2&gt; &lt;p&gt;Strimzi aims to make node ports work out of the box, but there are several options you can use to customize the Kafka cluster and its node port services.&lt;/p&gt; &lt;h3&gt;Preconfigured node port numbers&lt;/h3&gt; &lt;p&gt;By default, the node port numbers are generated/assigned by the Kubernetes controllers. That means that every time you delete your Kafka cluster and deploy a new one, a new set of node ports will be assigned to the Kubernetes services created by Strimzi. So, after every redeployment, you have to reconfigure all your applications using the node ports with the new node port of the bootstrap service.&lt;/p&gt; &lt;p&gt;Strimzi allows you to customize the node ports in the &lt;code&gt;Kafka&lt;/code&gt; custom resource:&lt;/p&gt; &lt;pre&gt;# ... listeners: external: type: nodeport tls: true authentication: type: tls overrides: bootstrap: nodePort: 32100 brokers: - broker: 0 nodePort: 32000 - broker: 1 nodePort: 32001 - broker: 2 nodePort: 32002 # ... &lt;/pre&gt; &lt;p&gt;The example above requests node port &lt;code&gt;32100&lt;/code&gt; for the bootstrap service and ports &lt;code&gt;32000&lt;/code&gt;, &lt;code&gt;32001&lt;/code&gt;, and &lt;code&gt;32002&lt;/code&gt; for the per-broker services. This allows you to redeploy your cluster without changing the node ports numbers in all your applications.&lt;/p&gt; &lt;p&gt;Note that Strimzi doesn&amp;#8217;t do any validation of the requested port numbers, so you have to make sure that they are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;within the range assigned for node ports in the configuration of your Kubernetes cluster, and&lt;/li&gt; &lt;li&gt;not used by any other service.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You do not have to configure all the node ports. You can decide to configure only some of them, for example, only the one for the external bootstrap service.&lt;/p&gt; &lt;h3&gt;Configuring advertised hosts and ports&lt;/h3&gt; &lt;p&gt;Strimzi also allows you to customize the advertised hostname and port that will be used in the configuration of the Kafka pods:&lt;/p&gt; &lt;pre&gt;# ... listeners: external: type: nodeport authentication: type: tls overrides: brokers: - broker: 0 advertisedHost: example.hostname.0 advertisedPort: 12340 - broker: 1 advertisedHost: example.hostname.1 advertisedPort: 12341 - broker: 2 advertisedHost: example.hostname.2 advertisedPort: 12342 # ... &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;advertisedHost&lt;/code&gt; field can contain either DNS name or an IP address. You can, of course, also decide to customize just one of these.&lt;/p&gt; &lt;p&gt;Changing the advertised port will only change the advertised port in the Kafka broker configuration. It will have no impact on the node port that is assigned by Kubernetes. To configure the node port numbers used by the Kubernetes services, use the &lt;code&gt;nodePort&lt;/code&gt; option described above.&lt;/p&gt; &lt;p&gt;Overriding the advertised hosts is something we used in the troubleshooting section above when the node address provided by the Kubernetes API was not the correct one. It can be useful in other situations as well, such as when your network does some network address translation:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-604197 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Customized-advertized-host.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Customized-advertized-host.png" alt="Strimzi Customized advertised host" width="609" height="502" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Customized-advertized-host.png 609w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Customized-advertized-host-300x247.png 300w" sizes="(max-width: 609px) 100vw, 609px" /&gt;&lt;/p&gt; &lt;p&gt;Another example might be when you don&amp;#8217;t want the clients to connect directly to the nodes where the Kafka pods are running. You can have only selected Kubernetes nodes exposed to the clients and use the &lt;code&gt;advertisedHost&lt;/code&gt; option to configure the Kafka brokers to use these nodes.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-604207 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Infra-nodes.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Infra-nodes.png" alt="Strimzi Infra nodes" width="478" height="516" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Infra-nodes.png 478w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/2-Infra-nodes-278x300.png 278w" sizes="(max-width: 478px) 100vw, 478px" /&gt;&lt;/p&gt; &lt;h2&gt;Pros and cons&lt;/h2&gt; &lt;p&gt;Exposing your Kafka cluster to the outside using node ports can give you a lot of flexibility. It can also deliver very good performance. Compared to other solutions, such as load balancers, routes, or ingress, there is no middleman to be a bottleneck or add latency. Your client connections will go to your Kafka broker in the most direct way possible; however, there is also a price to pay for this. Node ports are a very low-level solution. Often, you will run into problems with the detection of the advertised addresses as described in the sections above. Additionally, node ports may expect you to expose your Kubernetes nodes to the clients, which is often seen as security risk by the administrators.&lt;/p&gt; &lt;h3&gt;Read more&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/?p=601077"&gt;Accessing Apache Kafka in Strimzi: Part 1 – Introduction &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F07%2Faccessing-apache-kafka-in-strimzi-part-2-node-ports%2F&amp;#038;title=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%202%20%E2%80%93%20Node%20ports" data-a2a-url="https://developers.redhat.com/blog/2019/06/07/accessing-apache-kafka-in-strimzi-part-2-node-ports/" data-a2a-title="Accessing Apache Kafka in Strimzi: Part 2 – Node ports"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/07/accessing-apache-kafka-in-strimzi-part-2-node-ports/"&gt;Accessing Apache Kafka in Strimzi: Part 2 – Node ports&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bAFtnczHL0k" height="1" width="1" alt=""/&gt;</content><summary>This article series explains how Apache Kafka and its clients work and how Strimzi makes it accessible for clients running outside of Kubernetes. In the first article, we provided an introduction to the topic, and here we will look at exposing an Apache Kafka cluster managed by Strimzi using node ports. Specifically, in this article, we’ll look at how node ports work and how they can be used with ...</summary><dc:creator>Jakub Scholz</dc:creator><dc:date>2019-06-07T07:00:20Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/07/accessing-apache-kafka-in-strimzi-part-2-node-ports/</feedburner:origLink></entry><entry><title>Accessing Apache Kafka in Strimzi: Part 1 – Introduction</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/90oBTF3Cj64/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="jboss a-mq" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Jakub Scholz</name></author><id>searchisko:content:id:jbossorg_blog-accessing_apache_kafka_in_strimzi_part_1_introduction</id><updated>2019-06-06T07:00:30Z</updated><published>2019-06-06T07:00:30Z</published><content type="html">&lt;p&gt;&lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt; is an open source project that provides container images and operators for running &lt;a href="https://developers.redhat.com/videos/youtube/CZhOJ_ysIiI/"&gt;Apache Kafka&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Scalability is one of the flagship features of Apache Kafka. It is achieved by partitioning the data and distributing them across multiple brokers. Such data sharding has also a big impact on how Kafka clients connect to the brokers. This is especially visible when Kafka is running within a platform like Kubernetes but is accessed from outside of that platform.&lt;/p&gt; &lt;p&gt;This article series will explain how Kafka and its clients work and how Strimzi makes it accessible for clients running outside of Kubernetes.&lt;span id="more-601077"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Productized and supported versions of the Strimzi and &lt;a href="http://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; projects are available as part of the &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/amq"&gt;Red Hat AMQ&lt;/a&gt; product.&lt;/p&gt; &lt;p&gt;It would, of course, be insufficient just to shard the data into partitions. The ingress and egress data traffic also needs to be properly handled. The clients writing to or reading from a given partition have to connect directly to the leader broker which is hosting it. Thanks to the clients connecting directly to the individual brokers, the brokers don&amp;#8217;t need to do any forwarding of data between the clients and other brokers. That helps to significantly reduce the amount of work the brokers have to do and the amount of traffic flowing around within the cluster.&lt;/p&gt; &lt;p&gt;The only data traffic between the different brokers is due to replication, when the follower brokers are fetching data from the lead broker for a given partition. That makes the data shards independent of each other, which also makes Kafka scale so well.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-603927 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connecting-to-leaders.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connecting-to-leaders.png" alt="Clients connecting to partitions" width="810" height="334" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connecting-to-leaders.png 810w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connecting-to-leaders-300x124.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connecting-to-leaders-768x317.png 768w" sizes="(max-width: 810px) 100vw, 810px" /&gt;&lt;/p&gt; &lt;p&gt;How do the clients know where to connect?&lt;/p&gt; &lt;h2&gt;Kafka&amp;#8217;s discovery protocol&lt;/h2&gt; &lt;p&gt;Kafka has its own discovery protocol. When a Kafka client is connecting to the Kafka cluster, it first connects to any broker that is a member of the cluster and asks it for &lt;em&gt;metadata&lt;/em&gt; for one or more topics. The &lt;em&gt;metadata&lt;/em&gt; contains the information about the topics, its partitions, and the brokers that host these partitions. All brokers should have the data for the whole cluster because they are all synced through Zookeeper. Therefore, it doesn&amp;#8217;t matter to which broker the client is connected as first—all of them will give it the same response.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-603937 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connection-flow.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connection-flow.png" alt="Connection flow" width="956" height="222" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connection-flow.png 956w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connection-flow-300x70.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Connection-flow-768x178.png 768w" sizes="(max-width: 956px) 100vw, 956px" /&gt;&lt;/p&gt; &lt;p&gt;Once the client gets the &lt;em&gt;metadata&lt;/em&gt;, it will use that data to figure out where to connect when it wants to write to or read from a given partition. The broker addresses used in the &lt;em&gt;metadata&lt;/em&gt; will be either created by the broker itself based on the hostname of the machine where the broker runs, or it can be configured by the user using the &lt;code&gt;advertised.listeners&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;The client will use the address from the &lt;em&gt;metadata&lt;/em&gt; to open one or more new connections to the addresses of the brokers that host the particular partitions it is interested in. Even when the &lt;em&gt;metadata&lt;/em&gt; points to the same broker where the client already connected and received &lt;em&gt;metadata&lt;/em&gt; from, it would still open a second connection. And, these connections will be used to produce or consume data.&lt;/p&gt; &lt;p&gt;Note that this description of the Kafka protocol is intentionally simplified for the purposes of this article.&lt;/p&gt; &lt;h2&gt;What does this mean for Kafka on Kubernetes?&lt;/h2&gt; &lt;p&gt;So, what does all this mean for running Kafka on Kubernetes? If you are familiar with Kubernetes, you probably know that the most common way to expose some application is using a Kubernetes &lt;code&gt;Service&lt;/code&gt;. Kubernetes services work as layer 4 load balancers. They provide a stable DNS address, where the clients can connect, and they forward the connections to one of the pods that is backing the service.&lt;/p&gt; &lt;p&gt;This approach works reasonably well with most stateless applications, which just want to connect randomly to one of the back ends behind the service. But, it can get a lot trickier if your application requires some kind of stickiness because of some state associated with a particular pod. This can be session stickiness, for example, where a client needs to connect to the same pod as last time because of some session information that the pod already has. Or it can be a data stickiness, where a client needs to connect to a particular pod because it contains some particular data.&lt;/p&gt; &lt;p&gt;This is also the case with Kafka. A Kubernetes service can be used for the initial connection only—it will take the client to &lt;em&gt;one&lt;/em&gt; of the brokers within the cluster where it can get the metadata. However, the subsequent connections cannot be done through that service because it would route the connection &lt;em&gt;randomly&lt;/em&gt; to one of the brokers in the cluster instead of leading it to one particular broker.&lt;/p&gt; &lt;p&gt;How does Strimzi deal with this? There are two general ways to solve this problem:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write your own proxy/load balancer, which would do more intelligent routing on the application layer (layer 7). Such a proxy could, for example, abstract the architecture of the Kafka cluster from the client and pretend that the cluster has just one big broker running everything and just route the traffic to the different brokers in the background. Kubernetes already does this for the HTTP traffic using the Ingress resource.&lt;/li&gt; &lt;li&gt;Make sure you use the &lt;code&gt;advertised.listeners&lt;/code&gt; option in the broker configuration in a way that allows the clients to connect directly to the broker.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In Strimzi, we currently support the second option.&lt;/p&gt; &lt;h3&gt;Connecting from inside the same Kubernetes cluster&lt;/h3&gt; &lt;p&gt;Doing this for clients running inside the same Kubernetes cluster as the Kafka cluster is quite simple. Each pod has its own IP address, which other applications can use to connect directly to it. This is normally not used by regular Kubernetes applications. One reason for this is that Kubernetes doesn&amp;#8217;t offer a nice way to discover these IP addresses. To find out the IP address, you need to use the Kubernetes API, then find the right pod and its IP address. And you would need to have the appropriate rights to do this. Instead, Kubernetes uses the services with their stable DNS names as the main discovery mechanism.&lt;/p&gt; &lt;p&gt;With Kafka, this is not an issue, because it has its own discovery protocol. We do not need the clients to figure out the API address from the Kubernetes API. We just need to configure it and the advertised address, and then the clients will discover it through the Kafka &lt;em&gt;metadata&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;There is an even better option, which is used by Strimzi. For StatefulSets (which Strimzi is using to run the Kafka broker), you can use the Kubernetes headless service to give each of the pods a stable DNS name. Strimzi uses these DNS names as the advertised addresses for the Kafka brokers. So, with Strimzi:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The initial connection is done using a regular Kubernetes service to get the &lt;em&gt;metadata&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;The subsequent connections are opened using the DNS names given to the pods by another headless Kubernetes service. The diagram below shows how it looks with an example Kafka cluster named &lt;code&gt;my-cluster&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-603947 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Inside-Kuebrnetes.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Inside-Kuebrnetes.png" alt="Inside Kubernetes" width="956" height="407" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Inside-Kuebrnetes.png 956w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Inside-Kuebrnetes-300x128.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/1-Inside-Kuebrnetes-768x327.png 768w" sizes="(max-width: 956px) 100vw, 956px" /&gt;&lt;/p&gt; &lt;p&gt;Both approaches have pros and cons. Using the DNS can sometimes cause problems with cached DNS information. When the underlying IP addresses of the pods change (e.g., during rolling updates), the clients connecting to the brokers need to have the latest DNS information. However, we found that using IP addresses causes even worse problems, because sometimes Kubernetes re-uses them very aggressively, and a new pod gets an IP address that was used just a few seconds before by some other Kafka node.&lt;/p&gt; &lt;h3&gt;Connecting from the outside&lt;/h3&gt; &lt;p&gt;Although the access for clients running inside the same Kubernetes cluster is relatively simple, it will get a bit harder from the outside. There are some tools for joining the Kubernetes network with the regular network outside of Kubernetes, but most Kubernetes clusters run on their own network, which is separated from the world outside. That means things like pod IP addresses or DNS names are not resolvable for any clients running outside the cluster. Thanks to that, it is clear that we need to use a separate Kafka listener for access from inside and outside of the cluster, because the advertised addresses will need to be different.&lt;/p&gt; &lt;p&gt;Kubernetes and Red Hat OpenShift have many different ways of exposing applications, such as node ports, load balancers, or routes. Strimzi supports all of these to let users find the way that best suits their use case. We will look at them in more detail in the subsequent articles in this series.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#38;linkname=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F06%2Faccessing-apache-kafka-in-strimzi-part-1-introduction%2F&amp;#038;title=Accessing%20Apache%20Kafka%20in%20Strimzi%3A%20Part%201%20%E2%80%93%20Introduction" data-a2a-url="https://developers.redhat.com/blog/2019/06/06/accessing-apache-kafka-in-strimzi-part-1-introduction/" data-a2a-title="Accessing Apache Kafka in Strimzi: Part 1 – Introduction"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/06/accessing-apache-kafka-in-strimzi-part-1-introduction/"&gt;Accessing Apache Kafka in Strimzi: Part 1 – Introduction&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/90oBTF3Cj64" height="1" width="1" alt=""/&gt;</content><summary>Strimzi is an open source project that provides container images and operators for running Apache Kafka on Kubernetes and Red Hat OpenShift. Scalability is one of the flagship features of Apache Kafka. It is achieved by partitioning the data and distributing them across multiple brokers. Such data sharding has also a big impact on how Kafka clients connect to the brokers. This is especially visibl...</summary><dc:creator>Jakub Scholz</dc:creator><dc:date>2019-06-06T07:00:30Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/06/accessing-apache-kafka-in-strimzi-part-1-introduction/</feedburner:origLink></entry><entry><title>Network sniffing</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Mm9i-OVpKWo/network-sniffing.html" /><category term="feed_group_name_jgroups" scheme="searchisko:content:tags" /><category term="feed_name_belasblog" scheme="searchisko:content:tags" /><category term="wireshark tshark tcpdump network sniffing jgroups" scheme="searchisko:content:tags" /><author><name>Bela Ban</name></author><id>searchisko:content:id:jbossorg_blog-network_sniffing</id><updated>2019-06-05T11:17:25Z</updated><published>2019-06-05T11:12:00Z</published><content type="html">Oftentimes, &lt;a href="http://jgroups.org/"&gt;JGroups&lt;/a&gt;/&lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid"&gt;Datagrid&lt;/a&gt; users capture traffic from the network for analysis. Using tools such as &lt;a href="https://www.wireshark.org/"&gt;wireshark&lt;/a&gt; or &lt;a href="https://www.wireshark.org/docs/man-pages/tshark.html"&gt;tshark&lt;/a&gt;, they can look at UDP packets or TCP streams.&lt;br /&gt;&lt;br /&gt;There used to be a &lt;a href="https://developer.jboss.org/wiki/JGroupsWiresharkPlugin"&gt;wireshark plugin&lt;/a&gt;, written by Richard Achmatowicz, but since it was written in C, every time the wire format changed, the C code had to be changed, too. It is therefore not maintained any longer.&lt;br /&gt;&lt;br /&gt;However, there's a class in JGroups that can be used to read messages from the wire: &lt;a href="https://github.com/belaban/JGroups/blob/master/tests/other/org/jgroups/tests/ParseMessages.java"&gt;ParseMessages&lt;/a&gt;. Since it uses the the same code that's reading messages off the wire, it can always parse messages in the version it's shipped with. It is therefore resistant to wire format changes.&lt;br /&gt;&lt;br /&gt;In 4.1.0, I changed ParseMessages to be more useful:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Reading of TCP streams is now supported&lt;/li&gt;&lt;li&gt;It can read packets from stdin (ideal for piping from tshark)&lt;/li&gt;&lt;li&gt;Handling of binary data (e.g. from a PCAP capture) is supported&lt;/li&gt;&lt;li&gt;Views are parsed and displayed (e.g. in VIEW or JOIN response messages)&lt;/li&gt;&lt;li&gt;Logical names can be displayed: instead of &lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;{node-2543, node-2543}&lt;/span&gt; instead of &lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;{3673e687-fafb-63e0-2ff1-67c0a8a6f8eb,312aa7da-f3d5-5999-1f5c-227f6e43728e}&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&amp;nbsp;To demonstrate how to use this, I made 4 short videos:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=vSD2QZNSUuE"&gt;Capture UDP IPv4 traffic with tshark&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=zEyTDEetj48"&gt;Capture TCP IPv6 traffic with tshark&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=mlhj9EPJ31c"&gt;Capture with tcpdump and wireshark&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;The documentation is &lt;a href="http://www.jgroups.org/manual4/index.html#_analyzing_wire_format_packets"&gt;here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Happy network sniffing!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Mm9i-OVpKWo" height="1" width="1" alt=""/&gt;</content><summary>Oftentimes, JGroups/Datagrid users capture traffic from the network for analysis. Using tools such as wireshark or tshark, they can look at UDP packets or TCP streams. There used to be a wireshark plugin, written by Richard Achmatowicz, but since it was written in C, every time the wire format changed, the C code had to be changed, too. It is therefore not maintained any longer. However, there's a...</summary><dc:creator>Bela Ban</dc:creator><dc:date>2019-06-05T11:12:00Z</dc:date><feedburner:origLink>http://belaban.blogspot.com/2019/06/network-sniffing.html</feedburner:origLink></entry><entry><title>Announcing Red Hat CodeReady Workspaces 1.2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XnJ0pYXUmRY/" /><category term="Announcement" scheme="searchisko:content:tags" /><category term="CodeReady Workspaces" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><author><name>Stevan LeMeur</name></author><id>searchisko:content:id:jbossorg_blog-announcing_red_hat_codeready_workspaces_1_2</id><updated>2019-06-05T07:05:59Z</updated><published>2019-06-05T07:05:59Z</published><content type="html">&lt;p&gt;We are pleased to introduce Red Hat CodeReady Workspaces version 1.2, which provides a cloud developer workspace server and browser-based IDE built for teams and organizations. &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;CodeReady Workspaces&lt;/a&gt; includes ready-to-use developer stacks for most of the popular programming languages, frameworks, and Red Hat technologies.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Release overview&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Red Hat CodeReady Workspaces 1.2 introduces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compatibility with &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift 4.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;New UBI 8-based developer stacks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-603037"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Red Hat CodeReady Workspaces 1.2 is available today in the &lt;a href="https://access.redhat.com/containers/"&gt;Red Hat Container Catalog&lt;/a&gt;. Install it on &lt;a href="https://developers.redhat.com/openshift/"&gt;OpenShift Container Platform&lt;/a&gt; or &lt;a href="https://www.openshift.com/products/dedicated/"&gt;OpenShift Dedicated&lt;/a&gt;, starting at version 3.11, by following the instructions in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/1.2/html/administration_guide/installing_codeready-workspaces"&gt;Administration Guide.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For OpenShift 4.0 or 4.1, CodeReady Workspaces 1.2 is available today from the OperatorHub. Based on a new operator that leverages the Operator Lifecycle Manager, the installation flow is getting simpler and can be handled without leaving the OpenShift Console. If you already have Red Hat OpenShift 4.0 or 4.1, get CodeReady Workspaces from the OperatorHub and follow the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/1.2/html/administration_guide/installing-codeready-workspaces-from-operator-hub"&gt;dedicated documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Notable enhancements&lt;/b&gt;&lt;/h2&gt; &lt;h3&gt;&lt;b&gt;Compatibility with OpenShift 4.x&lt;/b&gt;&lt;/h3&gt; &lt;p&gt;CodeReady Workspaces is now compatible with OpenShift 4.0 and OpenShift 4.1 and gets a dedicated operator leveraging the Operator Lifecycle Manager. Now available in Developer Preview, CodeReady Workspaces can be installed from the OpenShift 4.0 Operator Hub.&lt;/p&gt; &lt;h3&gt;&lt;b&gt;New UBI 8 based developer stacks&lt;/b&gt;&lt;/h3&gt; &lt;p&gt;CodeReady Workspaces is now providing default developer stacks for different technologies based on Red Hat Universal Base Image (UBI). All existing stacks have been updated to UBI 8 so you can take advantage of the latest and greatest official Red Hat container images.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-603047 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/06/CodeReadyWorkspaces-Stacks-1024x487.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/06/CodeReadyWorkspaces-Stacks-1024x487.png" alt="CodeReadyWorkspaces stacks" width="640" height="304" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/06/CodeReadyWorkspaces-Stacks-1024x487.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/CodeReadyWorkspaces-Stacks-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/06/CodeReadyWorkspaces-Stacks-768x366.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;In addition to those changes, we added a new developer stack for Node.js 10. The other default stacks have been updated with the latest versions of the different embedded technologies.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Supported environments&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Red Hat CodeReady Workspaces for OpenShift can be installed on Openshift Container Platform or OpenShift Dedicated starting at version 3.11.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#38;linkname=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Fannouncing-red-hat-codeready-workspaces-1-2%2F&amp;#038;title=Announcing%20Red%20Hat%20CodeReady%20Workspaces%201.2" data-a2a-url="https://developers.redhat.com/blog/2019/06/05/announcing-red-hat-codeready-workspaces-1-2/" data-a2a-title="Announcing Red Hat CodeReady Workspaces 1.2"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/05/announcing-red-hat-codeready-workspaces-1-2/"&gt;Announcing Red Hat CodeReady Workspaces 1.2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XnJ0pYXUmRY" height="1" width="1" alt=""/&gt;</content><summary>We are pleased to introduce Red Hat CodeReady Workspaces version 1.2, which provides a cloud developer workspace server and browser-based IDE built for teams and organizations. CodeReady Workspaces includes ready-to-use developer stacks for most of the popular programming languages, frameworks, and Red Hat technologies. Release overview Red Hat CodeReady Workspaces 1.2 introduces: Compatibility wi...</summary><dc:creator>Stevan LeMeur</dc:creator><dc:date>2019-06-05T07:05:59Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/05/announcing-red-hat-codeready-workspaces-1-2/</feedburner:origLink></entry><entry><title>EventFlow: Event-driven microservices on Red Hat OpenShift (Part 2)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-Xwz1zdNM_0/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><author><name>Hugo Hiden</name></author><id>searchisko:content:id:jbossorg_blog-eventflow_event_driven_microservices_on_red_hat_openshift_part_2</id><updated>2019-06-05T07:00:47Z</updated><published>2019-06-05T07:00:47Z</published><content type="html">&lt;p&gt;In &lt;a href="https://developers.redhat.com/blog/2018/10/15/eventflow-event-driven-microservices-on-openshift-part-1/"&gt;part 1&lt;/a&gt;, I introduced the EventFlow platform for developing, deploying, and managing event-driven &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; using Red Hat AMQ Streams. This post will demonstrate how to deploy the EventFlow platform on &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;, install a set of sample processors, and build a flow.&lt;span id="more-583717"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Deploying the EventFlow platform&lt;/h2&gt; &lt;p&gt;The EventFlow platform requires Red Hat OpenShift and these instructions have been tested on OpenShift 3.11 running on &lt;a href="https://www.okd.io/minishift/"&gt;Minishift&lt;/a&gt; 1.2.5. This post will use the upstream &lt;a href="http://www.strimzi.io"&gt;Strimzi&lt;/a&gt; project in place of &lt;a href="https://access.redhat.com/products/red-hat-amq#streams"&gt;Red Hat AMQ Streams&lt;/a&gt;, but the instructions should be identical except the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/using_amq_streams_on_openshift_container_platform/getting-started-str#downloads-str"&gt;installation of the Cluster Operator&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prerequisites — Minishift and Strimzi&lt;/h3&gt; &lt;p&gt;Once you&amp;#8217;ve installed Minishift, the first step is to start Minishift with slightly altered default configuration—increasing the amount of CPU and memory available:&lt;/p&gt; &lt;pre&gt;minishift config set openshift-version v3.11.0 minishift config set memory 8GB minishift config set cpus 4 minishift config set disk-size 50g minishift config set image-caching true minishift addons enable admin-user minishift addons enable anyuid minishift start &lt;/pre&gt; &lt;p&gt;This may take a few minutes to start, but once it has, you can continue with the configuration:&lt;/p&gt; &lt;pre&gt;eval $(minishift oc-env) eval $(minishift docker-env) &lt;/pre&gt; &lt;p&gt;This sets up the correct environment variables in your session. Because EventFlow uses Apache Kafka for its communication, the next step is to install a Strimzi Kafka cluster:&lt;/p&gt; &lt;pre&gt;oc login -u system:admin oc apply -f https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.11.3/strimzi-cluster-operator-0.11.3.yaml -n myproject &lt;/pre&gt; &lt;p&gt;This step sets up an Operator that can create clusters on demand, so the next step is to do just that and create a small Kafka cluster with a single broker instance:&lt;/p&gt; &lt;pre&gt;oc apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.11.3/examples/kafka/kafka-persistent-single.yaml -n myproject &lt;/pre&gt; &lt;p&gt;While this is being brought up, you can monitor the progress using:&lt;/p&gt; &lt;pre&gt;oc get pods -w &lt;/pre&gt; &lt;p&gt;or open the console using:&lt;/p&gt; &lt;pre&gt;minishift console &lt;/pre&gt; &lt;p&gt;&lt;img class=" aligncenter size-large wp-image-598087 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console-1024x557.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console-1024x557.png" alt="" width="640" height="348" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console-1024x557.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console-768x418.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Console.png 1190w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;which should show a Strimzi installation running.&lt;/p&gt; &lt;h3&gt;Installing EventFlow components&lt;/h3&gt; &lt;p&gt;EventFlow comprises a number of components, which must be installed next. The first item is Custom Resource Definitions (CRDs), which are used to represent the flow structure, the individual processor elements and the representation of the targeted deployment clouds. This is done by installing the following three yaml definitions:&lt;/p&gt; &lt;pre&gt;oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow/master/ui/src/main/resources/processor-crd.yml oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow/master/ui/src/main/resources/flow-crd.yml oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow/master/ui/src/main/resources/cloud-crd.yml &lt;/pre&gt; &lt;p&gt;Once this is done, the next step in the installation is to give the EventFlow Operator permission to monitor Custom Resources:&lt;/p&gt; &lt;pre&gt;oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:myproject:default &lt;/pre&gt; &lt;p&gt;Finally, the EventFlow Operator, User Interface, and the Environment Variable Operator need to be deployed and configured:&lt;/p&gt; &lt;pre&gt;https://raw.githubusercontent.com/rh-event-flow/eventflow/master/yaml/00-deploy-components.yaml.txt &lt;/pre&gt; &lt;p&gt;&lt;img class=" aligncenter size-large wp-image-598137 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed-1024x585.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed-1024x585.png" alt="" width="640" height="366" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed-1024x585.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed-300x171.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed-768x439.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/EventFlowPost2Deployed.png 1194w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;h3&gt;Deploying the sample microservices&lt;/h3&gt; &lt;p&gt;Several sample processors are available for use in this proof-of-concept; these are available as containers in DockerHub and they can be registered by running the following commands:&lt;/p&gt; &lt;pre&gt;oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow-processor-samples/master/data-source/src/main/resources/source-cr.yml oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow-processor-samples/master/data-processor/src/main/resources/processor-cr.yml oc create -f https://raw.githubusercontent.com/rh-event-flow/eventflow-processor-samples/master/data-sink/src/main/resources/sink-cr.yml &lt;/pre&gt; &lt;p&gt;This will register a number of Custom Resources in Red Hat OpenShift describing the flow processors. The &lt;code&gt;Processor&lt;/code&gt; Custom Resource is used to describe the interface that the service offers the flow. User editable parameters can be defined as well as the number of inputs and outputs the processor has. There are three types of processors (producers, consumers, and processors) that a flow can include, and the examples include one of each. The source of the samples can be downloaded from: &lt;a href="https://github.com/project-streamzi/eventflow-processor-samples"&gt;https://github.com/rh-event-flow/eventflow-processor-samples&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Composing microservices&lt;/h3&gt; &lt;p&gt;So far, we have shown how to install the necessary components and register a set of example microservices. With that complete, the EventFlow UI can be used to integrate the microservices into a flow.&lt;/p&gt; &lt;p&gt;The install script for the EventFlow will have exposed an OpenShift route for the manager UI. To determine the URL of the Flow Manager, check the &lt;code&gt;HOST/PORT&lt;/code&gt; section of the OpenShift route:&lt;/p&gt; &lt;pre&gt;oc get route eventflow-ui &lt;/pre&gt; &lt;p&gt;From the flow manager UI, flows can be created and edited. The sample processors will include a &lt;code&gt;source&lt;/code&gt;, which will generate random &lt;code&gt;double&lt;/code&gt;s and a &lt;code&gt;sink&lt;/code&gt; that log the data to the console. Right-click on the canvas, add these two processors, and connect them together. Then, give the flow a unique name and click the &lt;em&gt;Submit&lt;/em&gt; button. This will deploy your flow.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-large wp-image-595447 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/SimpleEventFlow.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/SimpleEventFlow.png" alt="" width="812" height="335" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/SimpleEventFlow.png 812w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/SimpleEventFlow-300x124.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/SimpleEventFlow-768x317.png 768w" sizes="(max-width: 812px) 100vw, 812px" /&gt;&lt;/p&gt; &lt;p&gt;Revisit the Red Hat OpenShift console, and you will see the containers running. The &lt;code&gt;flow&lt;/code&gt; doesn&amp;#8217;t really exist but a set of microservices has been deployed, Apache Kafka topics have been created, and the services will be configured to communicate over those topics. The logs of the &lt;code&gt;-logdata-XXX&lt;/code&gt; container will show the messages being received.&lt;/p&gt; &lt;p&gt;One of the EventFlow platform components—the FlowOperator—monitors flows and reacts when they are changed. If the flow is reconfigured (e.g., a filter processor is added to the flow between the data source and log data microservices), the FlowOperator will detect this change and respond accordingly. In this case, a new microservice for the filter will be instantiated, which will consume messages from the data source. The log-data microservice will be reconfigured to consume the output of the filter rather than the original data source. The FlowOperator only reconfigures the parts of the flow that have changed; therefore, the random-data microservice will remain unchanged.&lt;/p&gt; &lt;p&gt;Dynamic reconfiguration of event-driven microservices is a powerful tool. Not only can new microservices be introduced to a running flow but application settings can be edited (e.g., change the threshold on the filter and only this container will be redeployed), and non-functional properties can be modified (e.g., the number of replicas of each container).&lt;/p&gt; &lt;p&gt;Over time, more runtime settings will be exposed to the flow interface, including auto-scaling properties, monitoring, and metrics, and, because the communication is done via Apache Kafka, whether new processors should begin consuming from offset (the &amp;#8216;start&amp;#8217; of the topic). This last point allows a flow to be reconfigured and then behave (once it has caught up) as if the new version had been deployed originally.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-large wp-image-595477 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow-1024x347.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow-1024x347.png" alt="" width="640" height="217" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow-1024x347.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow-300x102.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow-768x260.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/ReconfiguredEventFlow.png 1090w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;Because flows are represented as Kubernetes Custom Resources, they are integrated into the Red Hat OpenShift API. They can be created, listed, and deleted from the command line—to tear down the flow run &lt;code&gt;$ oc delete flow demo&lt;/code&gt;. Processors can also be interacted with in this way, for example, &lt;code&gt;$ oc get processors&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;One advantage in creating an abstract representation of a flow and making use of an operator to deploy it within a container platform is the fact that it is possible to support the same flow structure on different middleware technologies. Part 3 of this series will explore exactly that and demonstrate a reimplementation of the EventFlow concept on top of Knative, a recently announced cloud-native serving and eventing project designed to work natively in Kubernetes and Red Hat Openshift.&lt;/p&gt; &lt;h3&gt;Learn more&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/using_amq_streams_on_openshift_container_platform/overview-str"&gt;Red Hat AMQ Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/rh-event-flow/eventflow"&gt;EventFlow&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/"&gt;Strimzi &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#38;linkname=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F05%2Feventflow-event-driven-microservices-on-red-hat-openshift-part-2%2F&amp;#038;title=EventFlow%3A%20Event-driven%20microservices%20on%20Red%20Hat%20OpenShift%20%28Part%202%29" data-a2a-url="https://developers.redhat.com/blog/2019/06/05/eventflow-event-driven-microservices-on-red-hat-openshift-part-2/" data-a2a-title="EventFlow: Event-driven microservices on Red Hat OpenShift (Part 2)"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/05/eventflow-event-driven-microservices-on-red-hat-openshift-part-2/"&gt;EventFlow: Event-driven microservices on Red Hat OpenShift (Part 2)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-Xwz1zdNM_0" height="1" width="1" alt=""/&gt;</content><summary>In part 1, I introduced the EventFlow platform for developing, deploying, and managing event-driven microservices using Red Hat AMQ Streams. This post will demonstrate how to deploy the EventFlow platform on Red Hat OpenShift, install a set of sample processors, and build a flow. Deploying the EventFlow platform The EventFlow platform requires Red Hat OpenShift and these instructions have been tes...</summary><dc:creator>Hugo Hiden</dc:creator><dc:date>2019-06-05T07:00:47Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/05/eventflow-event-driven-microservices-on-red-hat-openshift-part-2/</feedburner:origLink></entry><entry><title>Announcing Thorntail 2.4 general availability</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zMTZ9vi8cx4/" /><category term="Announcement" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Application Runtimes" scheme="searchisko:content:tags" /><category term="thorntail" scheme="searchisko:content:tags" /><author><name>James Falkner</name></author><id>searchisko:content:id:jbossorg_blog-announcing_thorntail_2_4_general_availability</id><updated>2019-06-04T07:01:07Z</updated><published>2019-06-04T07:01:07Z</published><content type="html">&lt;div style="float: right;"&gt;&lt;/div&gt; &lt;p&gt;At this year&amp;#8217;s &lt;a href="https://www.redhat.com/en/summit/2019"&gt;Red Hat Summit&lt;/a&gt;, Red Hat announced Thorntail 2.4 general availability for Red Hat customers through a subscription to &lt;a href="https://www.redhat.com/en/products/application-runtimes"&gt;Red Hat Application Runtimes&lt;/a&gt;. Red Hat Application Runtimes provides application developers with a variety of application runtimes running on the &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Introduction to Thorntail&lt;/h2&gt; &lt;p&gt;&lt;a href="https://thorntail.io/"&gt;Thorntail&lt;/a&gt; is the new name for WildFly Swarm, and it bundles everything you need to develop and run &lt;a href="https://developers.redhat.com/blog/2018/08/23/eclipse-microprofile-and-red-hat-update-thorntail-and-smallrye/"&gt;Thorntail&lt;/a&gt; and &lt;a href="https://microprofile.io/"&gt;MicroProfile&lt;/a&gt; applications by packaging server runtime libraries with your application code and running it with &lt;code&gt;java -jar&lt;/code&gt;. It speeds up the transition from monoliths to &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; and takes advantage of your existing industry standard &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java EE technology experience&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-601787"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s New in Thorntail?&lt;/h2&gt; &lt;p&gt;This release is an incremental release from Thorntail 2.2 and adds support for &lt;a href="https://openjdk.java.net/projects/jdk/11/"&gt;Java 11&lt;/a&gt; and &lt;a href="https://github.com/eclipse/microprofile/releases/tag/2.2"&gt;MicroProfile 2.2&lt;/a&gt; (the latest release at the time of writing), a feature-rich collection of APIs for developing enterprise &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;. Incremental updates in MicroProfile 2.2 (and therefore in Thorntail 2.4) include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://microprofile.io/project/eclipse/microprofile-fault-tolerance"&gt;&lt;strong&gt;Fault Tolerance 2.0&lt;/strong&gt;&lt;/a&gt;: Implements a collection of programming patterns like Bulkheads, Timeouts, Circuit Breakers, and Fallbacks to monitor and gracefully react to potential failure conditions. Utilizing these patterns can eliminate the potential for cascading failures in a microservices architecture.&lt;/li&gt; &lt;li&gt;&lt;a href="https://microprofile.io/project/eclipse/microprofile-opentracing"&gt;&lt;strong&gt;OpenTracing 1.3&lt;/strong&gt;&lt;/a&gt;: Enables tracing the flow of a request as it traverses multiple services within a microservices architecture. When Thorntail is used with Jaeger (a distributed tracing service), organizations can quickly track down performance bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://microprofile.io/project/eclipse/microprofile-open-api"&gt;&lt;strong&gt;Open API 1.1&lt;/strong&gt;&lt;/a&gt;: A Java implementation of the Open API specification that exposes machine-readable format of custom-developed RESTful endpoints.&lt;/li&gt; &lt;li&gt;&lt;a href="https://microprofile.io/project/eclipse/microprofile-rest-client"&gt;&lt;strong&gt;Rest Client 1.2.0&lt;/strong&gt;&lt;/a&gt;: A type-safe API for invoking RESTful services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thorntail also includes a number of features that make it easy to deploy and manage Thorntail projects, such as integrating data sources, support for &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; and &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat SSO&lt;/a&gt;, and more. Consult the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_application_runtimes/1/html/red_hat_openshift_application_runtimes_release_notes/"&gt;release notes&lt;/a&gt; for a complete list.&lt;/p&gt; &lt;h2&gt;A redesigned launcher for Red Hat OpenShift&lt;/h2&gt; &lt;div id="attachment_601797" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="http://developers.redhat.com/launch"&gt;&lt;img aria-describedby="caption-attachment-601797" class="wp-image-601797 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-29-at-4.11.22-PM-1024x705.png" alt="" width="640" height="441" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-29-at-4.11.22-PM-1024x705.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-29-at-4.11.22-PM-300x207.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-29-at-4.11.22-PM-768x529.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-29-at-4.11.22-PM.png 1053w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-601797" class="wp-caption-text"&gt;Redesigned launcher experience.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Using &lt;a href="https://developers.redhat.com/launch"&gt;developers.redhat.com/launch&lt;/a&gt;, you can immediately create and deploy a Thorntail application directly to &lt;a href="http://openshift.com/"&gt;OpenShift Online&lt;/a&gt; or to your own local OpenShift cluster. It provides a hassle-free way of creating applications from scratch, starting with example applications, or importing your own, as well as an easy way to build and deploy those applications to Red Hat OpenShift.&lt;/p&gt; &lt;p&gt;Examples are available to showcase how developers can use Thorntail to build fundamental building blocks of cloud-native applications and services, such as creating secured RESTful APIs, implementing health checks, externalizing configuration, or integrating with the OpenShift Service Mesh based on the &lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;Istio&lt;/a&gt; project.&lt;/p&gt; &lt;h2&gt;Test driving a sample app using Thorntail&lt;/h2&gt; &lt;p&gt;Thorntail is a Java framework and, as such, it can be run using &lt;a href="https://developers.redhat.com/products/openjdk/overview/"&gt;OpenJDK&lt;/a&gt;. Let&amp;#8217;s test drive one of the Thorntail boosters on OpenShift (here I am using the &lt;a href="https://developers.redhat.com/products/cdk/overview/"&gt;Red Hat CDK&lt;/a&gt;, but any OpenShift cluster will do). The following is one set of commands you could use to pull the OpenJDK image to your local system for use with Thorntail:&lt;/p&gt; &lt;pre&gt;oc new-project thorntail oc import-image java:8 --from=registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift --confirm&lt;/pre&gt; &lt;p&gt;Then, the following commands could be used to build and deploy the Thorntail application to Red Hat OpenShift:&lt;/p&gt; &lt;pre&gt;oc new-app --name rest-example 'java:8~https://github.com/thorntail-examples/rest-http-redhat#2.4.0-redhat-1' oc expose svc/rest-example&lt;/pre&gt; &lt;p&gt;You can watch the build take place:&lt;/p&gt; &lt;pre&gt;oc logs -f bc/rest-example&lt;/pre&gt; &lt;p&gt;Once the build completes, wait for the deployment to finish:&lt;/p&gt; &lt;pre&gt;oc rollout status -w dc/rest-example&lt;/pre&gt; &lt;p&gt;And then access the sample app&amp;#8217;s UI:&lt;/p&gt; &lt;pre&gt;open http://$(oc get route rest-example -o jsonpath='{.spec.host}{"\n"}')&lt;/pre&gt; &lt;p&gt;Red Hat customers using the OpenJDK distribution with Thorntail will be able to keep current with the latest updates, security advisories, knowing when and why containers are updated, and remaining up to date on the latest available tagged image.&lt;/p&gt; &lt;h2&gt;Documentation&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/products/rhoar/overview/"&gt;Red Hat OpenShift Application Runtimes (RHOAR)&lt;/a&gt; team has been continuously adding and improving on the official documentation for Thorntail. This includes updates in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_application_runtimes/1/html/red_hat_openshift_application_runtimes_release_notes/"&gt;Release Notes&lt;/a&gt;, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_application_runtimes/1/html/getting_started_with_red_hat_openshift_application_runtimes/"&gt;Getting Started Guide,&lt;/a&gt; and the new &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_application_runtimes/1/html/thorntail_runtime_guide/"&gt;Thorntail Runtime Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Developer interactive learning scenarios&lt;/h2&gt; &lt;p&gt;These &lt;a href="https://learn.openshift.com/middleware/rhoar-getting-started-thorntail/"&gt;self-paced scenarios&lt;/a&gt; provide you with a preconfigured Red Hat OpenShift instance, accessible from your browser without any downloads or configuration. Use it to &lt;a href="https://learn.openshift.com/middleware/rhoar-getting-started-thorntail/"&gt;experiment with Thorntail&lt;/a&gt; or learn about other technologies within RHOAR and see how it helps solve real-world problems.&lt;/p&gt; &lt;div id="attachment_528797" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://learn.openshift.com/middleware/rhoar-getting-started-thorntail/"&gt;&lt;img aria-describedby="caption-attachment-528797" class="wp-image-528797 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-2.25.40-PM-1024x732.png" alt="Interactive Learning Scenario for Thorntail" width="640" height="458" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-2.25.40-PM-1024x732.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-2.25.40-PM-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-2.25.40-PM-768x549.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-2.25.40-PM.png 1271w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-528797" class="wp-caption-text"&gt;Interactive Learning Scenario for Thorntail&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Getting support for Thorntail&lt;/h2&gt; &lt;p&gt;Support for Thorntail is available to Red Hat customers through a subscription to Red Hat OpenShift Application Runtimes. Contact your local Red Hat representative or &lt;a href="https://www.redhat.com/en/about/contact/sales"&gt;Red Hat Sales&lt;/a&gt; for details on how you can enjoy world-class support offered from Red Hat and its worldwide partner network.&lt;/p&gt; &lt;p&gt;Moving forward, customers can expect support for Thorntail and other RHOAR runtimes according to the &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes/"&gt;Red Hat Product Update and Support Lifecycle&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What’s next for Thorntail?&lt;/h2&gt; &lt;p&gt;The Thorntail team is continually taking &lt;a href="https://issues.jboss.org/projects/THORN/issues?filter=allopenissues"&gt;feedback&lt;/a&gt; from customers and the wider community of open source developers, as well as tracking the &lt;a href="https://thorntail.io/downloads/"&gt;upstream Thorntail releases.&lt;/a&gt; They are working to make updates to the RHOAR runtimes based on that feedback, as well as considering support for additional modules from Red Hat and the very large Java community. The Thorntail community is also continuing to track the evolution of and contribute to &lt;a href="https://developers.redhat.com/blog/2018/04/24/jakarta-ee-is-officially-out/"&gt;Jakarta EE&lt;/a&gt; as well as the advances in the &lt;a href="https://microprofile.io"&gt;MicroProfile project&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Red Hat customers enjoy &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes"&gt;world-class support&lt;/a&gt; for Thorntail 2.x (and all other runtimes in &lt;a href="https://www.redhat.com/en/products/application-runtimes"&gt;Red Hat Application Runtimes&lt;/a&gt;). Longer term, the Thorntail team will look to the &lt;a href="https://developers.redhat.com/blog/2019/03/07/quarkus-next-generation-kubernetes-native-java-framework/"&gt;recently announced Quarkus project&lt;/a&gt; to deliver even more impressive resource consumption and performance numbers, and a fantastic development experience utilizing &lt;a href="https://smallrye.io/"&gt;SmallRye&lt;/a&gt; to implement the &lt;a href="https://microprofile.io/"&gt;Eclipse MicroProfile&lt;/a&gt; specifications. For more information about Thorntail and Quarkus, read &lt;a href="https://thorntail.io/posts/thorntail-community-announcement-on-quarkus/"&gt;this blog post&lt;/a&gt; outlining the community direction.&lt;/p&gt; &lt;h2&gt;The people behind Thorntail&lt;/h2&gt; &lt;p&gt;This release was produced by the Red Hat&amp;#8217;s RHOAR product team, and it involved many hours of development, testing, writing documentation, testing some more, and working with the wider Red Hat community of customers, partners, and Thorntail developers to incorporate contributions, both big and small. We are glad you have chosen to use it and hope that it meets or exceeds your expectations!&lt;/p&gt; &lt;h2&gt;Thorntail resources&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/rhoar/overview/"&gt;Red Hat OpenShift Application Runtimes Developer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2018/08/23/eclipse-microprofile-and-red-hat-update-thorntail-and-smallrye/"&gt;Eclipse MicroProfile and Red Hat Update: Thorntail and SmallRye&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://thorntail.io/archive/"&gt;Thorntail Blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_application_runtimes/1/html/thorntail_runtime_guide/"&gt;Thorntail Runtime Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/thorntail"&gt;Thorntail Discussion Group&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://twitter.com/thorntail_io"&gt;Thorntail on Twitter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://webchat.freenode.net/?channels=thorntail"&gt;Thorntail on IRC&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://issues.jboss.org/projects/THORN/issues?filter=allopenissues"&gt;Thorntail Issue Tracker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://microprofile.io"&gt;MicroProfile&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#38;linkname=Announcing%20Thorntail%202.4%20general%20availability" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fannouncing-thorntail-2-4-general-availability%2F&amp;#038;title=Announcing%20Thorntail%202.4%20general%20availability" data-a2a-url="https://developers.redhat.com/blog/2019/06/04/announcing-thorntail-2-4-general-availability/" data-a2a-title="Announcing Thorntail 2.4 general availability"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/04/announcing-thorntail-2-4-general-availability/"&gt;Announcing Thorntail 2.4 general availability&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zMTZ9vi8cx4" height="1" width="1" alt=""/&gt;</content><summary>At this year’s Red Hat Summit, Red Hat announced Thorntail 2.4 general availability for Red Hat customers through a subscription to Red Hat Application Runtimes. Red Hat Application Runtimes provides application developers with a variety of application runtimes running on the Red Hat OpenShift Container Platform. Introduction to Thorntail Thorntail is the new name for WildFly Swarm, and it bundles...</summary><dc:creator>James Falkner</dc:creator><dc:date>2019-06-04T07:01:07Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/04/announcing-thorntail-2-4-general-availability/</feedburner:origLink></entry><entry><title>Container-related content you might have missed at Red Hat Summit</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jHpJWWNTdoI/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat Enterprise Linux" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="RHEL8" scheme="searchisko:content:tags" /><author><name>Doug Tidwell</name></author><id>searchisko:content:id:jbossorg_blog-container_related_content_you_might_have_missed_at_red_hat_summit</id><updated>2019-06-04T07:00:34Z</updated><published>2019-06-04T07:00:34Z</published><content type="html">&lt;p&gt;If you weren&amp;#8217;t lucky enough to attend the recent &lt;a href="https://www.redhat.com/en/summit/2019"&gt;Red Hat Summit&lt;/a&gt; &lt;em&gt;or&lt;/em&gt; you went but couldn&amp;#8217;t make it to all the container-related sessions, worry not. We teamed up with Scott McCarty, Principal Technology Product Manager–Containers at Red Hat, to bring you an overview of what you missed.&lt;/p&gt; &lt;h2&gt;Choosing the right container base image for your applications&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image"&gt;Red Hat Universal Base Image (UBI)&lt;/a&gt; gives you three options for building &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt; with the full power of &lt;a href="https://developers.redhat.com/rhel8"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) underneath. The goal is to create the smallest possible image that fully supports your application. You select a base image depending on the application you&amp;#8217;re packaging in a container. For example, if you have a Golang or .NET application, all of that application&amp;#8217;s dependencies are built in. That means you can use the minimal image (&lt;code&gt;ubi-minimal&lt;/code&gt;), which contains &lt;code&gt;microdnf&lt;/code&gt;, a package manager that only supports install, update, and remove functions. It also includes, well, a minimal set of tools.&lt;/p&gt; &lt;p&gt;&lt;span id="more-599577"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The base image (&lt;code&gt;ubi&lt;/code&gt;) lets you run any application that runs on Red Hat Enterprise Linux. It contains the full-featured &lt;code&gt;yum&lt;/code&gt; package manager along with basic operating system tools, such as &lt;code&gt;tar&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, and &lt;code&gt;vi&lt;/code&gt;. (&lt;code&gt;vi&lt;/code&gt; haters, please keep your discussions civil in the comments section below.) If you need to run multiple services in a single container, &lt;code&gt;ubi-init&lt;/code&gt; runs &lt;code&gt;systemd&lt;/code&gt; at startup. To use this, enable your services at build time, and you&amp;#8217;re ready to go.&lt;/p&gt; &lt;p&gt;Scott also covered support options for various images and hosting combinations. For example, if you have a certified application (see &lt;a href="http://crunchtools.com/files/2019/05/Choosing-the-right-container-base-image-for-your-application.pdf"&gt;Scott&amp;#8217;s slides&lt;/a&gt; for application certification info) running in a container built on a UBI with everything hosted on a Red Hat platform, you&amp;#8217;re entitled to the highest level of support. Other combinations, of course, may have lower levels of support.&lt;/p&gt; &lt;p&gt;UBIs are a great addition to your container toolbox. For more information, &lt;a href="http://crunchtools.com/files/2019/05/Choosing-the-right-container-base-image-for-your-application.pdf"&gt;the slides&lt;/a&gt; are available online, and &lt;a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image"&gt;Scott&amp;#8217;s article on the Red Hat Universal Base Image&lt;/a&gt; is a great resource as well.&lt;/p&gt; &lt;h2&gt;Building production-ready containers&lt;/h2&gt; &lt;p&gt;One great topic in this presentation by Scott McCarty and Ben Breard was the Five Commandments of building containers:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Standardize&lt;/strong&gt;: Make sure everyone is using the same base images wherever possible.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimize&lt;/strong&gt;: Limit the content in the images to what actually serves the workload.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delegate&lt;/strong&gt;: The responsibilities for maintaining the layers of the image should lie with the people who have the expertise for that technology. For example, your middleware experts should be in charge of the &lt;code&gt;Dockerfile&lt;/code&gt; that defines the middleware layer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: Put processes in place to automate builds via Helm charts, Ansible playbooks, and operators wherever possible.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterate&lt;/strong&gt;: As you find mistakes, capture that hard-earned knowledge in code.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="http://crunchtools.com/files/2019/05/Summit-2019_-Building-Production-Ready-Containers.pdf"&gt;Take a look at the slides&lt;/a&gt; for lots of great information and real-world experiences.&lt;/p&gt; &lt;h2&gt;RHEL 8 container tools&lt;/h2&gt; &lt;p&gt;Scott and Dan Walsh covered open source projects from the &lt;a href="https://www.opencontainers.org"&gt;Open Container Initiative&lt;/a&gt;: &lt;a href="https://podman.io"&gt;podman&lt;/a&gt;, &lt;a href="https://github.com/containers/skopeo/blob/master/README.md"&gt;skopeo&lt;/a&gt;, and &lt;a href="https://buildah.io"&gt;buildah&lt;/a&gt;. &lt;a href="http://crunchtools.com/files/2019/05/Red-Hat-Enterprise-Linux-8-Container-Tools.pdf"&gt;Dan and Scott&amp;#8217;s slides&lt;/a&gt; are available and, as a bonus, if you visit &lt;a href="http://stats.eventcore.com/wf/click?upn=x0IXmEraw-2BSAZWl0mHPX9iDyKwSVIjiK-2FaDAYDiMIfeBgTYKkvJOtt0rgrK0P3XN06O6KQabIRHUH-2BqK-2BgZJI-2FoIoB20OAAC6H0NpWHX6KI-3D_Pj5ETFMBU1yXtgiSsKbRxZjbVfYeUF5sPcVH0t2gHPpJT4a-2BK-2B-2FRwfUNNnOHT-2FH4wDZGGzzUWOGZWBuEK9oPeB0oNzFM4luz-2FndLLXAeX09Q-2FhemZ39BAFDmoC4Nde9NhARC75IqsFlT0pFW3-2FxViNLZ12OKly9ifAhQHQjfHtLzb5UQnn2I9KItRwJzJoAL7B0a51fYIc603jA-2FyZ4qvh3vKir5thLkaH87bWOlOAU0uC9vQz4Qw457Fya8Fpei3U3sNcsgV-2FVGhRP1xOwIqCiTVWSuv-2F4I-2BSMYQGlRET98lzbBrUl5lm2NsswioOWM"&gt;the Red Hat Summit virtual event&lt;/a&gt;, you can find a video of this session in the &amp;#8220;Road to Red Hat Enterprise Linux 8&amp;#8221; track. We also have great resources on &lt;a href="https://developers.redhat.com/topics/containers/"&gt;our containers page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you haven&amp;#8217;t seen Dan explain the benefits of podman, put your life on hold and go do that now.&lt;/p&gt; &lt;h2&gt;Linux container internals 2.0&lt;/h2&gt; &lt;p&gt;This comprehensive session included a section on registries, pointing out the features of &lt;a href="https://quay.io"&gt;Quay&lt;/a&gt; and the &lt;a href="https://access.redhat.com/containers/"&gt;Red Hat Container Catalog&lt;/a&gt;, including the container health index calculated for each image in the registry. This extremely useful feature lets image consumers know whether a given image has security vulnerabilities. Although this is a great feature, it does put the responsibility on the part of the image owner to continue rebuilding and updating the image as vulnerabilities are found and fixes roll out. (As an example, yr author just discovered he needs to rebuild &lt;a href="https://quay.io/repository/dougtidwell/2048-stack?tab=info"&gt;the 2048 image&lt;/a&gt; created for the &lt;a href="https://developers.redhat.com/che-custom-stacks/"&gt;&amp;#8220;Creating custom stacks in Eclipse Che&amp;#8221; video&lt;/a&gt;.)&lt;/p&gt; &lt;p&gt;Scott covered many other crucial topics, including container orchestration, container standards, and architecture. As you&amp;#8217;d expect, &lt;a href="http://crunchtools.com/files/2019/05/Linux-Container-Internals-2.0.pdf"&gt;the slides&lt;/a&gt; are available online. You can also take &lt;a href="https://learn.openshift.com/subsystems/"&gt;an interactive, hands-on Katacoda lab&lt;/a&gt; for a quick start or check out &lt;a href="https://github.com/fatherlinux/learn-katacoda"&gt;Scott&amp;#8217;s sample code&lt;/a&gt; for an in-depth look.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;We&amp;#8217;ve provided just a taste of the great container-related content Scott and others presented at Red Hat Summit. Again, check out &lt;a href="https://developers.redhat.com/topics/containers/"&gt;our containers page&lt;/a&gt; for more resources to help you get started. And if you have ideas about what you&amp;#8217;d like to see next, let us know in the comments below.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#38;linkname=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F04%2Fcontainer-related-information-you-might-have-missed-at-red-hat-summit%2F&amp;#038;title=Container-related%20content%20you%20might%20have%20missed%20at%20Red%20Hat%20Summit" data-a2a-url="https://developers.redhat.com/blog/2019/06/04/container-related-information-you-might-have-missed-at-red-hat-summit/" data-a2a-title="Container-related content you might have missed at Red Hat Summit"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/04/container-related-information-you-might-have-missed-at-red-hat-summit/"&gt;Container-related content you might have missed at Red Hat Summit&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jHpJWWNTdoI" height="1" width="1" alt=""/&gt;</content><summary>If you weren’t lucky enough to attend the recent Red Hat Summit or you went but couldn’t make it to all the container-related sessions, worry not. We teamed up with Scott McCarty, Principal Technology Product Manager–Containers at Red Hat, to bring you an overview of what you missed. Choosing the right container base image for your applications The Red Hat Universal Base Image (UBI) gives you thre...</summary><dc:creator>Doug Tidwell</dc:creator><dc:date>2019-06-04T07:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/04/container-related-information-you-might-have-missed-at-red-hat-summit/</feedburner:origLink></entry><entry><title>How to build a Raspberry Pi photo booth</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hVQ_57aylVI/" /><category term="coderland" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Internet of Things" scheme="searchisko:content:tags" /><category term="Python" scheme="searchisko:content:tags" /><category term="raspberry pi" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><author><name>Doug Tidwell</name></author><id>searchisko:content:id:jbossorg_blog-how_to_build_a_raspberry_pi_photo_booth</id><updated>2019-06-03T07:00:42Z</updated><published>2019-06-03T07:00:42Z</published><content type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/coderland/"&gt;Coderland&lt;/a&gt; booth at the recent &lt;a href="https://www.redhat.com/en/summit/2019"&gt;Red Hat Summit&lt;/a&gt; was all about &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless computing&lt;/a&gt; as implemented in &lt;a href="https://developers.redhat.com/coderland/serverless/"&gt;the Compile Driver&lt;/a&gt;. If you haven&amp;#8217;t gone through that example (you really should), that code creates a souvenir photo by superimposing the Coderland logo, a date stamp, and a message on top of an image from a webcam. We thought it would be fun to build a &lt;a href="https://opensource.com/tags/raspberry-pi"&gt;Raspberry Pi&lt;/a&gt; version for the booth so we could offer attendees a free souvenir. Here&amp;#8217;s a look at the finished product:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599337 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/photo_booth_front-1024x566.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/photo_booth_front-1024x566.jpg" alt="Front of the photo booth" width="640" height="354" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/photo_booth_front-1024x566.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/photo_booth_front-300x166.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/photo_booth_front-768x425.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;span id="more-598717"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The case shown here incorporates the Raspberry Pi, a touchscreen, and a camera. As you can see, this setup produces results similar to the &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; code inside the Compile Driver.&lt;/p&gt; &lt;h2&gt;Parts list&lt;/h2&gt; &lt;p&gt;Here&amp;#8217;s what I used to build everything:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/"&gt;A Raspberry Pi 3B+&lt;/a&gt; (the latest model)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.raspberrypi.org/products/raspberry-pi-touch-display/"&gt;The official Raspberry Pi touch screen&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.raspberrypi.org/products/camera-module-v2/"&gt;The official Raspberry Pi camera module&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://smarticase.com/collections/all/products/smartipi-touch?variant=11864926209"&gt;A SmartiPi Touch case to hold the three things above&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://tzumi.com/collections/power-banks-1/products/pocketjuice-endurance-ac-8-000-mah-portable-charger-black"&gt;A Tzumi PocketJuice 8,000 mAh portable battery pack to power everything&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.usa.canon.com/internet/portal/us/home/products/details/printers/mobile-compact-printer/cp1300-bkn/selphy-cp1300"&gt;A Canon Selphy CP1300 printer&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With these things combined, we had a handheld photo booth to take pictures of attendees in front of a backdrop that made it look like they were riding the Compile Driver. A nice ending to a conversation about serverless computing. We also had people stop by the booth to ask &amp;#8220;what is that thing?&amp;#8221; Whatever you do with it, people love the Raspberry Pi.&lt;/p&gt; &lt;h2&gt;Assembling the hardware&lt;/h2&gt; &lt;p&gt;The first task was assembling the case. Fortunately, the folks at &lt;a href="https://smarticase.com/"&gt;Smarticase&lt;/a&gt; provide &lt;a href="https://www.youtube.com/watch?v=XKVd5638T_8"&gt;a YouTube video that covers everything&lt;/a&gt;. You bolt the touch screen into the case, then attach the Raspberry Pi. An extra-long camera cable is included to connect the two. The Pi can be fastened directly to the case or held in place with a bracket. I went with the bracket approach because that makes it easy to take the Pi out and change its SD card. Finally, the case includes a small bracket to hold the camera module. This is attached to the case with Lego bricks:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599347 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_attached_camera-784x1024.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_attached_camera-784x1024.jpg" alt="Camera attached with Legos" width="640" height="836" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_attached_camera-784x1024.jpg 784w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_attached_camera-230x300.jpg 230w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_attached_camera-768x1003.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;A minor Lego hack: I needed to reverse the direction of the studs on the blocks so that the camera faced away from the screen. (Turns out the nubby things on top of a brick are called studs and the holes on the bottom are called tubes. You learn something every day.) Thanks to &lt;a href="http://thebrickblogger.com/2014/11/reversing-studs-on-lego-bricks-plates/"&gt;this post from The Brick Blogger&lt;/a&gt;, I found a straightforward way to do it. I took some Lego wheels, removed the rubber tire portions, then used the wheels to connect the two bricks. To make sure this stayed in place, I superglued everything together:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599357 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_hack-1024x684.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_hack-1024x684.jpg" alt="Lego hack" width="640" height="428" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_hack-1024x684.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_hack-300x200.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/lego_hack-768x513.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;h2&gt;Software&lt;/h2&gt; &lt;p&gt;With the hardware built, I installed the latest &lt;a href="https://www.raspberrypi.org/downloads/"&gt;Raspbian&lt;/a&gt; to a new SD card. The card I used was 32GB, although an 8GB card (if you can find one that small) would have plenty of space. Next, I cloned the repo &lt;a href="https://github.com/laurentalacoque/TouchSelfie-extended"&gt;TouchSelfie-extended&lt;/a&gt;, a fork of another photo booth project. There are quite a few RPi photo booths out there, but this one was the best match for what we needed. Speaking of which, I should probably mention our requirements. We needed the photo booth to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take pictures and overlay various things on those images.&lt;/li&gt; &lt;li&gt;Let people email their photos to themselves.&lt;/li&gt; &lt;li&gt;Display a privacy notice to let people know how we use their information (spoiler: we don&amp;#8217;t).&lt;/li&gt; &lt;li&gt;Use the portable photo printer to create souvenir pictures.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As you&amp;#8217;d expect for Raspberry Pi software, the TouchSelfie-extended package is written in Python. Once you&amp;#8217;ve cloned the repo, run the &lt;code&gt;setup.py&lt;/code&gt; script to enable the options you want, such as emailing, printing, or uploading the photos you take.&lt;/p&gt; &lt;h3&gt;Setting up the overlay image&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;setup.py&lt;/code&gt; script automates some of the configuration, but there are other values you need to set in the file &lt;code&gt;configuration.json&lt;/code&gt;. To overlay an image on the photos you take, specify a value for &lt;code&gt;logo_file&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;"logo_file": "/home/pi/Documents/TouchSelfie-extended/scripts/Summit Wednesday Overlay.png",&lt;/pre&gt; &lt;p&gt;With this value set, the package superimposes the file &lt;code&gt;Summit Wednesday Overlay.png&lt;/code&gt; on top of the photo.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Confession&lt;/em&gt;: The overlay image included the greeting, a hardcoded date, and the Coderland and Red Hat Developer program logos. The original Java version generated those things separately, but your author doesn&amp;#8217;t have the Python skills to work such (Image)Magick.&lt;/p&gt; &lt;p&gt;It took a while to figure out the constraints, but it turns out you can&amp;#8217;t store any EXIF data in the PNG file you want to use as the overlay. Here are the GIMP settings that work:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599497 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/GIMP-options.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/GIMP-options.png" alt="GIMP options that work" width="290" height="458" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/GIMP-options.png 290w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/GIMP-options-190x300.png 190w" sizes="(max-width: 290px) 100vw, 290px" /&gt;&lt;/p&gt; &lt;h3&gt;Enabling email&lt;/h3&gt; &lt;p&gt;To enable email, I had to jump through some Google/OAuth hoops to get the credentials that allowed the code to send images via my Gmail account. Those are stored in a file named &lt;code&gt;google_credentials.dat&lt;/code&gt;. The rest of the email setup involved straightforward changes to the configuration file:&lt;/p&gt; &lt;pre&gt;"email_body": "We're glad you stopped by the booth at Red Hat Summit 2019. Here's your souvenir photo - Enjoy!\n", "email_subject": "Thanks for visiting Coderland!", "enable_email": true, "gmail_user": "xxxxxxxxxxxxxxxxxx@gmail.com",&lt;/pre&gt; &lt;h3&gt;Creating a privacy notice&lt;/h3&gt; &lt;p&gt;With email set up, we needed a privacy notice that let attendees know that their data would only be used to send the photo. I added a Tk Label to the existing Tk Checkbox widget. I also changed the logic so that the photo was not sent unless the user clicked the checkbox. The onscreen keyboard looked like this:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599507 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.56.44-PM-1024x616.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.56.44-PM-1024x616.png" alt="Onscreen keyboard with privacy notice" width="640" height="385" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.56.44-PM-1024x616.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.56.44-PM-300x180.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.56.44-PM-768x462.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;If an attendee typed their email address with the onscreen keyboard, clicked the checkbox, and clicked send, the photo made its way to their inbox. (No doubt shared with all their friends and loved ones upon arrival.)&lt;/p&gt; &lt;h3&gt;Configuring the printer&lt;/h3&gt; &lt;p&gt;Getting the printer working required setting up CUPS on the Raspberry Pi. Once set up, the CUPS admin console found the printer on our wireless network:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599147 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.58.57-PM-1024x616.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.58.57-PM-1024x616.png" alt="CUPS console on the RPi" width="640" height="385" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.58.57-PM-1024x616.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.58.57-PM-300x180.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-20-at-4.58.57-PM-768x462.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;I couldn&amp;#8217;t get the print command built in to the original code to work, so I just replaced it with a system call to &lt;code&gt;lp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;printCommand = 'lp -o raw -o media=postcard -d Canon_SELPHY_CP1300 ' + self.last_picture_filename; os.popen(printCommand);&lt;/pre&gt; &lt;p&gt;The hassle here was that specifying the paper size with &lt;code&gt;media=postcard&lt;/code&gt; is completely undocumented. The values &lt;code&gt;a4&lt;/code&gt;, &lt;code&gt;letter&lt;/code&gt;, and &lt;code&gt;legal&lt;/code&gt; are the only ones mentioned in the &lt;code&gt;lp&lt;/code&gt; man page. The images from the Raspberry Pi wouldn&amp;#8217;t print correctly until I found the &lt;code&gt;postcard&lt;/code&gt; setting in the source code of another printer driver. Just one of those unexpected productivity killers that pop up in every project.&lt;/p&gt; &lt;h2&gt;The finished product&lt;/h2&gt; &lt;p&gt;The final photo booth opens with this interface:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-large wp-image-599517 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-22-at-3.12.10-PM.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-22-at-3.12.10-PM.png" alt="The basic photo booth interface" width="806" height="479" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-22-at-3.12.10-PM.png 806w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-22-at-3.12.10-PM-300x178.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/05/Screen-Shot-2019-05-22-at-3.12.10-PM-768x456.png 768w" sizes="(max-width: 806px) 100vw, 806px" /&gt;&lt;/p&gt; &lt;p&gt;The middle button on the bottom takes the photo. The printer and envelope icons print and email the modified picture, as you would expect. The first image in this post shows you what the interface looks like once you&amp;#8217;ve taken a picture. (FWIW, the buttons in the lower corners allow you to take a collage of photos or generate an animated GIF, but neither of those are printable. A more ambitious programmer would have removed them.)&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;As with any Raspberry Pi project, there&amp;#8217;s an element of astonishment that such small, inexpensive hardware can do something so cool. Total cost for the entire rig, including the printer and power brick, was around US $265 before taxes and shipping. It was fun to have this in the booth at Red Hat Summit, but you could have lots of fun with this at more personal events like parties and weddings. If you build one, let us know how it goes. And send us a picture!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#38;linkname=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F06%2F03%2Fhow-to-build-a-raspberry-pi-photo-booth%2F&amp;#038;title=How%20to%20build%20a%20Raspberry%20Pi%20photo%20booth" data-a2a-url="https://developers.redhat.com/blog/2019/06/03/how-to-build-a-raspberry-pi-photo-booth/" data-a2a-title="How to build a Raspberry Pi photo booth"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/06/03/how-to-build-a-raspberry-pi-photo-booth/"&gt;How to build a Raspberry Pi photo booth&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hVQ_57aylVI" height="1" width="1" alt=""/&gt;</content><summary>The Coderland booth at the recent Red Hat Summit was all about serverless computing as implemented in the Compile Driver. If you haven’t gone through that example (you really should), that code creates a souvenir photo by superimposing the Coderland logo, a date stamp, and a message on top of an image from a webcam. We thought it would be fun to build a Raspberry Pi version for the booth so we cou...</summary><dc:creator>Doug Tidwell</dc:creator><dc:date>2019-06-03T07:00:42Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/06/03/how-to-build-a-raspberry-pi-photo-booth/</feedburner:origLink></entry><entry><title>Beginners Guide: Multicloud Portability for Dummies</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Z7p9OiJyW3Y/beginners-guide-multicloud-portability-for-dummies.html" /><category term="best practices" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Publishing" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-beginners_guide_multicloud_portability_for_dummies</id><updated>2019-06-03T05:00:11Z</updated><published>2019-06-03T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div style="text-align: left;"&gt;&lt;a href="http://bit.ly/multicloud-for-dummies" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" target="_blank"&gt;&lt;img alt="multicloud portability for dummies" border="0" data-original-height="1259" data-original-width="813" height="200" src="https://1.bp.blogspot.com/-pk54HLLINxE/XN7BGZZpugI/AAAAAAAAtrs/85eqSDtXSOUtpP94Fe_FrJ5IBC3onqJWQCLcBGAs/s200/multicloud-portability-for-dummies.png" title="" width="128" /&gt;&lt;/a&gt;&lt;/div&gt;This last year was a good year for talking about cloud, hybrid cloud, and hybrid multicloud.&lt;br /&gt;&lt;br /&gt;We saw many organziations struggling with some of the same issues time and again.&amp;nbsp;Their frustrations being voiced were enough that together with &lt;a href="https://www.linkedin.com/in/roelhodzelmans/" target="_blank"&gt;Roel Hodzelmans &lt;/a&gt;we pulled together an introductory session for &lt;a href="http://www.schabell.org/2018/05/redhat-summit-2018-3-pitfalls-hybrid-multicloud-slides.html" target="_blank"&gt;Red Hat Summit 2018&lt;/a&gt; entitled, &lt;a href="http://www.schabell.org/2018/05/redhat-summit-2018-3-pitfalls-video.html" target="_blank"&gt;3 Pitfalls Everyone Should Avoid with Hybrid Multicloud&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Based on reactions to this talk we decided to expand the reach with a &lt;a href="http://www.schabell.org/2018/03/3-pitfalls-everyone-should-avoid-with-hybrid-multicloud-part-1.html" target="_blank"&gt;series of four articles&lt;/a&gt; covering all aspects of the material.&amp;nbsp; The series was picked up through &lt;a href="https://dzone.com/articles/3-pitfalls-everyone-should-avoid-with-hybrid-multi" target="_blank"&gt;on DZone.com&lt;/a&gt;, generating so much interest that there's clearly a need for these multicloud insights.&lt;br /&gt;&lt;br /&gt;Together with the 'for dummies' series, we came up with an outline based on our content, feedback from attendees, and the published articles. This become a &lt;a href="http://bit.ly/multicloud-for-dummies" target="_blank"&gt;free e-book you can download&lt;/a&gt;, so let's look at the outline.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;Here's a brief overview of the contents.&lt;br /&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;Chapter 1 - Surveying your cloud landscape&lt;/h3&gt;&lt;div&gt;Touching on the reasons cloud can be compelling for your organization, you'll look at cloud benefits, workloads, and how to evaluate hybrid, multicloud, and hybrid multicloud combinations.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Chapter 2 - Strategizing your best cloud solutions&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Looking at the core pitfalls to your organizations cloud options in this chapter, the focus is on providing you with the foundation to make effective choices with your cloud strategy.&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Chapter 3 - Building a cloud to fit your needs&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;When building your cloud to fit your needs, the strategy is of paramount importance to have an overview of applications, migration plans, the business case, and so much more.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Chapter 4 - Making your hybrid multicloud happen&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Planning only goes so far, now it's time to execute on the plan. From pilot to production you'll take a look at how to make this happen with the best chances of success.&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Chapter 5 - Ten hybrid multicloud secrets&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;These would be secrets, so &lt;a href="http://bit.ly/multicloud-for-dummies" target="_blank"&gt;download the e-book&lt;/a&gt; and find out what they are!&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=PAQGqI_kcn0:nBC5AcVamEs:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=PAQGqI_kcn0:nBC5AcVamEs:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=PAQGqI_kcn0:nBC5AcVamEs:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=PAQGqI_kcn0:nBC5AcVamEs:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=PAQGqI_kcn0:nBC5AcVamEs:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/PAQGqI_kcn0" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Z7p9OiJyW3Y" height="1" width="1" alt=""/&gt;</content><summary>This last year was a good year for talking about cloud, hybrid cloud, and hybrid multicloud. We saw many organziations struggling with some of the same issues time and again. Their frustrations being voiced were enough that together with Roel Hodzelmans we pulled together an introductory session for Red Hat Summit 2018 entitled, 3 Pitfalls Everyone Should Avoid with Hybrid Multicloud. Based on rea...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2019-06-03T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/PAQGqI_kcn0/beginners-guide-multicloud-portability-for-dummies.html</feedburner:origLink></entry><entry><title>This week in JBoss (June 1st 2019) - The Quiet One</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Hx_ayPthgm4/this-week-in-jboss-june-1st-2019-the-quiet-one" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><author><name>Mark Little</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_june_1st_2019_the_quiet_one</id><updated>2019-06-01T13:07:48Z</updated><published>2019-06-01T13:07:48Z</published><content type="html">&lt;!-- [DocumentBodyStart:37af30b1-e78d-4556-998f-4186eba0defd] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;Well it's been a quiet week or so since we last posted, at least for blogs. But remember that you can always check the &lt;a class="jive-link-external-small" href="https://developers.redhat.com/" rel="nofollow"&gt;Red Hat Developers site&lt;/a&gt; for more activity. In the meantime, let's take a look at what else has been going on.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The Hibernate team have been busy with updates to the project(s), including &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_orm_5_4_3_final_released" rel="nofollow"&gt;ORM 5.4.3.Final&lt;/a&gt; and &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_search_6_0_0_alpha6_released" rel="nofollow"&gt;Search 6.0.0.Alpha6&lt;/a&gt;. We've also had Andrew Dinn blogging about Byteman 4.0.7 being made available. Bela &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/4_1_0_released" rel="nofollow"&gt;announced&lt;/a&gt; the release of JGroups 4.1.0, which now includes support for GraalVM and Quarkus! Speaking of Quarkus, Dimitris found time to &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/getting_quarked" rel="nofollow"&gt;write up his thoughts&lt;/a&gt; on the Quarkus announcement and what it meant for him personally but also for developers; it's a good read. And Bilgin touches on Quarkus in his article "&lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/camel_rebirth_with_subsecond_experiences" rel="nofollow"&gt;Camel Rebirth with Subsecond Experiences&lt;/a&gt;", though there's a lot more in the article than just Quarkus so even if that's not an area of interest for you yet but you are a Camel user then definitely check it out. Finally, Eric has managed to publish a couple of articles on CodeReady Studio 12. One on setting up &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/how_to_setup_data_virtualization_tooling_for_codeready_studio_12" rel="nofollow"&gt;Data Virtualization Tooling&lt;/a&gt; and another on &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/how_to_setup_process_automation_tooling_for_codeready_studio_12" rel="nofollow"&gt;Process Automation Tooling.&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;OK, as I said at the start, it's been a bit quiet but no less interesting so hopefully you find something of interest in the above and maybe get the chance to give feedback to the authors and their respective projects! See you next time!&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:37af30b1-e78d-4556-998f-4186eba0defd] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Hx_ayPthgm4" height="1" width="1" alt=""/&gt;</content><summary>Well it's been a quiet week or so since we last posted, at least for blogs. But remember that you can always check the Red Hat Developers site for more activity. In the meantime, let's take a look at what else has been going on.   The Hibernate team have been busy with updates to the project(s), including ORM 5.4.3.Final and Search 6.0.0.Alpha6. We've also had Andrew Dinn blogging about Byteman 4....</summary><dc:creator>Mark Little</dc:creator><dc:date>2019-06-01T13:07:48Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2019/06/01/this-week-in-jboss-june-1st-2019-the-quiet-one</feedburner:origLink></entry></feed>
